global:
  # After an alert is resolved, keep the alert on the Alerts status page.  This
  # is longer than the default, to help track down false positives or flaky
  # alerts.
  resolve_timeout: 60m

# The directory from which notification templates are read.
# TODO(soltesz): Add directory and actual templates.
templates:
- '/etc/alertmanager/template/*.tmpl'

# All incoming alerts are "routed" to a "receiver" based on various criteria.
# As well, alerts are aggregated in various ways to reduce alert volume.
route:
  # Group incoming alerts together with these labels. For example:
  #   For example: multiple individual alerts for instance=A and
  #   alertname=LatencyHigh would be batched into a single group.
  group_by: ['alertname']

  # By default, wait at least 'group_wait' before sending a notification to get
  # a set of alerts to 'group_by'.
  group_wait: 30s

  # By default, after the first notification is sent for a group, wait
  # 'group_interval' before sending another notification for subsequent alerts
  # in the same group.
  group_interval: 5m

  # By default, wait 'repeat_interval' before resending an alert.
  repeat_interval: 3h

  # When no other routes match, use the default receiver.
  receiver: slack-alerts-ticket

  # The above attributes are inherited by all child routes. Children can
  # overwrite any value.

  # The child route trees.
  routes:
  # This route performs matches on alert labels to catch alerts that are
  # related to a specific service.
  - match:
      service: federation

    # If the service matches, continue down the child routes. If the severity
    # of the alert is 'page', then send the notification to an alternative
    # receiver.
    routes:
    - match:
        severity: page
      receiver: slack-alerts-page


# When two alerts are firing at the same time, we can "inhibit" one based on
# the other. For example, a host is offline and a service on that host is not
# running. The second alert is redundant because a service cannot work if the
# host is offline.
#
# Note: Sometimes the ALERT definition can take multiple services into account.
# But, sometimes it makes more sense to do the suppression here.
#
# TODO(soltesz): add hints about when each condition is more appropriate.

inhibit_rules:
- source_match:
    severity: 'page'
  target_match:
    severity: 'ticket'
  # Apply inhibition if the group is the same.
  equal: ['instance', 'service']

# Various other checks rely on the script-exporter being up. If the
# script-exporter is not up, then don't alert on those dependencies.
# The empty `equal` parameter tells AM to not attempt a match on any other
# labels i.e., `source_match` and `target_match_re` are sufficient.
- source_match:
    alertname: 'ScriptExporterDownOrMissing'
  target_match_re:
    alertname: 'ScriptExporterMissingMetrics|TooManyNdtServersDown'
  equal: []

# Don't alert on missing SNMP metrics or SNMP scraping being broken if the
# snmp-exporter is down. The empty `equal` parameter tells AM to not attempt a
# match on any other labels i.e., `source_match` and `target_match_re` are
# sufficient.
- source_match:
    alertname: 'SnmpExporterDownOrMissing'
  target_match_re:
    alertname: 'SnmpExporterMissingMetrics|SnmpScrapingDownAtSite'
  equal: []

receivers:
# For M-Lab Slack, visit:
#   https://measurementlab.slack.com/apps/manage/custom-integrations
# Follow the "Incoming WebHooks link", and either add a new configuration or
# edit one of the existing configurations. Copy the "Webhook URL" from the
# slack configuration to the `api_url` values below.

- name: 'slack-alerts-page'
  slack_configs:
  - send_resolved: true
    api_url: {{SLACK_CHANNEL_URL}}
    channel: alerts-{{SHORT_PROJECT}}
    username: alert-page
    text: |
	  Open Issues: {{GITHUB_ISSUE_QUERY}}
	  Summary    : {{ .CommonAnnotations.summary }}
	  Description: {{if .CommonAnnotations.description}}{{.CommonAnnotations.description}}{{else}}Please add an alert description!{{end}}
	  Dashboard  : {{if .CommonAnnotations.dashboard}}{{.CommonAnnotations.dashboard}}{{else}}Please add an alert dashboard!{{end}}

  webhook_configs:
  - url: '{{GITHUB_RECEIVER_URL}}'

- name: 'slack-alerts-ticket'
  slack_configs:
  - send_resolved: true
    api_url: {{SLACK_CHANNEL_URL}}
    channel: alerts-{{SHORT_PROJECT}}
    username: alertmanager
    text: |
	  Open Issues: {{GITHUB_ISSUE_QUERY}}
	  Summary    : {{ .CommonAnnotations.summary }}
	  Description: {{if .CommonAnnotations.description}}{{.CommonAnnotations.description}}{{else}}Please add an alert description!{{end}}
	  Dashboard  : {{if .CommonAnnotations.dashboard}}{{.CommonAnnotations.dashboard}}{{else}}Please add an alert dashboard!{{end}}
  webhook_configs:
  - url: '{{GITHUB_RECEIVER_URL}}'
