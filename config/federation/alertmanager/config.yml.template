global:
  # After an alert is resolved, keep the alert on the Alerts status page.  This
  # is longer than the default, to help track down false positives or flaky
  # alerts.
  resolve_timeout: 60m

# The directory from which notification templates are read.
# TODO(soltesz): Add directory and actual templates.
templates:
- '/etc/alertmanager/template/*.tmpl'

# All incoming alerts are "routed" to a "receiver" based on various criteria.
# As well, alerts are aggregated in various ways to reduce alert volume.
route:
  # Group incoming alerts together with these labels. For example:
  #   For example: multiple individual alerts for instance=A and
  #   alertname=LatencyHigh would be batched into a single group.
  group_by: ['alertname']

  # By default, wait at least 'group_wait' before sending a notification to get
  # a set of alerts to 'group_by'.
  group_wait: 30s

  # By default, after the first notification is sent for a group, wait
  # 'group_interval' before sending another notification for subsequent alerts
  # in the same group.
  group_interval: 5m

  # By default, wait 'repeat_interval' before resending a notification.
  repeat_interval: 12h

  # When no other routes match, use the default receiver.
  receiver: slack-alerts-ticket

  routes:
  - receiver: 'pagerduty-pages'
    continue: true
    match:
      severity: page
      page_project: mlab-{{SHORT_PROJECT}}
  - receiver: 'slack-alerts-ticket'

# When two alerts are firing at the same time, we can "inhibit" one based on
# the other. For example, a host is offline and a service on that host is not
# running. The second alert is redundant because a service cannot work if the
# host is offline.
#
# Note: Sometimes the ALERT definition can take multiple services into account.
# But, sometimes it makes more sense to do the suppression here.
#
# TODO(soltesz): add hints about when each condition is more appropriate.

inhibit_rules:
- source_match:
    severity: 'page'
  target_match:
    severity: 'ticket'
  # Apply inhibition if the group is the same.
  equal: ['instance', 'service']

# Various other checks rely on the script-exporter being up. If the
# script-exporter is not up, then don't alert on those dependencies.
# The empty `equal` parameter tells AM to not attempt a match on any other
# labels i.e., `source_match` and `target_match_re` are sufficient.
- source_match:
    alertname: 'ScriptExporterDownOrMissing'
  target_match_re:
    alertname: 'ScriptExporterMissingMetrics|TooManyNdtServersDown'
  equal: []

# Don't alert on missing SNMP metrics or SNMP scraping being broken if the
# snmp-exporter is down. The empty `equal` parameter tells AM to not attempt a
# match on any other labels i.e., `source_match` and `target_match_re` are
# sufficient.
- source_match:
    alertname: 'SnmpExporterDownOrMissing'
  target_match_re:
    alertname: 'SnmpExporterMissingMetrics|SnmpScrapingDownAtSite'
  equal: []

# Don't alert about missing platform cluster DaemonSet metrics if the entire
# platform cluster scrape job is down.
- source_match:
    alertname: 'PlatformCluster_FederationScrapeJobFailing'
  target_match_re:
    alertname: 'PlatformCluster_.+Missing|MachineRunningWithoutK8sNode'
  equal: ['cluster']

# Don't alert on BMC connection failures if the switch is down.
- source_match:
    alertname: 'SwitchDownAtSite'
  target_match_re:
    alertname: 'BMC_ConnectionFailed'
  equal: []

receivers:
# For M-Lab Slack, visit:
#   https://measurementlab.slack.com/apps/manage/custom-integrations
# Follow the "Incoming WebHooks link", and either add a new configuration or
# edit one of the existing configurations. Copy the "Webhook URL" from the
# slack configuration to the `api_url` values below.

- name: 'slack-alerts-ticket'
  slack_configs:
  - send_resolved: true
    api_url: {{SLACK_CHANNEL_URL}}
    channel: alerts-{{SHORT_PROJECT}}
    username: alertmanager
    title: >
      [{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}]
      {{.GroupLabels.alertname}}
    text: |
      {{- if eq .Status "firing" -}}
      {{.CommonAnnotations.summary}}
      > {{if .CommonAnnotations.description}}{{.CommonAnnotations.description}}
      {{- else}}*Please add an alert description!*{{end -}}
      <https://github.com/m-lab/prometheus-support/edit/master/config/federation/prometheus/alerts.yml|(edit...)>
      {{- end }}
      {{ range $key, $value := .CommonLabels }} {{ $key }}=*{{ $value }}*, {{ end }}
    actions:
    # NOTE: An empty url is invalid config. An invalid URL will not display.
    - type: button
      text: Github
      url: '{{GITHUB_ISSUE_QUERY}}%20'
    # NOTE: Add a dashboard.
    - type: button
      text: '{{if .CommonAnnotations.dashboard}}Dashboard{{else}}Create Dashboard{{end}}'
      url: >
        {{- if .CommonAnnotations.dashboard }}{{ .CommonAnnotations.dashboard }}
        {{- else }}https://grafana.mlab-sandbox.measurementlab.net/dashboard/new?orgId=1
        {{- end -}}
    # NOTE: this constructs a query string for the alertmanager to create
    # a silence. The format of the query is a url encoded label set: e.g.
    # {foo="a",bar="b"}
    - type: button
      text: Add Silence
      url: >
        {{- if eq .Status "firing" -}}
          {{- .ExternalURL }}/#/silences/new?filter=
          {{- printf "{" | urlquery }}
          {{- range $index, $pair := .CommonLabels.SortedPairs }}
            {{- if $index }}{{ printf "," | urlquery }}{{end}}
            {{- printf "%s=\"%s\"" $pair.Name $pair.Value | urlquery }}
          {{- end -}}
          {{- printf "}" | urlquery }}
        {{- end -}}

  # All alerts sent to slack are also sent to the github webhook receiver.
  webhook_configs:
  - url: '{{GITHUB_RECEIVER_URL}}'

- name: 'pagerduty-pages'
  pagerduty_configs:
  - send_resolved: true
    service_key: {{PAGERDUTY_SERVICE_KEY}}
