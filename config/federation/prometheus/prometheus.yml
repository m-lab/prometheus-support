# M-Lab Prometheus configuration.

global:
  scrape_interval:     60s  # Set the scrape interval to every 60 seconds.
  evaluation_interval: 60s  # Evaluate rules every 60 seconds.
  # scrape_timeout is set to the global default (10s).

  # These labels are attached to any time series or alert sent to external
  # systems (federation, remote storage, Alertmanager).
  # TODO(soltesz): use this when M-Lab adds federation or alertmanager.
  # external_labels:
  #   monitor: 'mlab1'


# Load rules once and periodically evaluate them according to the global
# 'evaluation_interval'.
rule_files:
  - /etc/prometheus/rules.yml
  - /etc/prometheus/alerts.yml

# Scrape configurations.
#
# Each job name defines monitoring targets (or a method for discovering
# targets).
#
# The M-Lab Prometheus configuration uses three config types:
#  * automatically discovered services via kubernetes (kubernetes_sd_config)
#  * automatically discovered services via file (file_sd_config)
#  * static targets (static_config)
#
# Kubernetes targets are discovered automatically by querying the kubernetes
# master API. The configuration for this is simplest when Prometheus runs in
# the same cluster as the kubernetes master being monitored. In particular,
# the master CA certificates and an authentication token are mounted
# automatically in every container's filesystem for easy access.
#
# Discovery of legacy targets occurs by reading a configuration file. This
# configuration file can be updated out of band after start and Prometheus will
# periodically re-read the contents, adding new targets or removing old ones.
#
# Static targets cannot change after Prometheus starts. They are the least
# flexible. Because of this, only well known, or long lived targets, or
# singleton targets that need special relabeling rules should be static.
scrape_configs:

  # Kubernetes configurations were inspired by:
  # https://github.com/prometheus/prometheus/blob/master/documentation/examples
  #
  # The four kubernetes scrape configs correspond to specific cluster
  # components.
  #  * master API
  #  * cluster nodes
  #  * pods
  #  * service endpoints
  #
  # The separation allows each component to use different authentication
  # configs, or apply different relabeling rules.

  # Scrape config for kubernetes master API server.
  #
  # The kubernetes API is exposed as an "endpoint". Since kubernetes may have
  # many endpoints, this configuration restricts the targets monitored to the
  # default/kubernetes service. The relabeling rules ignore other endpoints.
  - job_name: 'kubernetes-apiservers'
    kubernetes_sd_configs:
      - role: endpoints

    # The kubernetes API requires authentication and uses a privately signed
    # certificate. The tls_config specifies the private CA cert and an
    # auth token. Kubernetes automatically mounts these files in the container
    # filesystem.
    scheme: https
    tls_config:
      ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
    bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

    # The source_labels are concatenated with ';'. The regex matches a single
    # value for the default kubernetes service endpoint. If there are
    # multiple API servers, all will match this pattern.
    relabel_configs:
      - source_labels: [__meta_kubernetes_namespace,
                        __meta_kubernetes_service_name,
                        __meta_kubernetes_endpoint_port_name]
        action: keep
        regex: default;kubernetes;https


  # Scrape config for kubernetes nodes.
  #
  # A kubernetes cluster consists of one or more nodes. Each reports metrics
  # related to the whole machine.
  - job_name: 'kubernetes-nodes'
    kubernetes_sd_configs:
      - role: node

    scheme: https
    tls_config:
      ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt

      # Nodes are discovered and scrapped using the kubernetes internal network
      # IP. Unfortunately, the certificates do not validate on requests:
      #
      #   "x509: cannot validate certificate for 10.0.4.126 because it doesn't
      #    contain any IP SANs"
      #
      # This is a known issue without a likely solution for private APIs:
      #    https://github.com/prometheus/prometheus/issues/1822
      #
      # Since these IPs are internal to the kubernetes virtual network, it
      # should be safe to skip certificate verification.
      insecure_skip_verify: true
    # TODO(soltesz): if we skip_verify, do we still need the bearer token?
    bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

    # Copy node labels from kubernetes to labels on the Prometheus metrics.
    # TODO(soltesz): There are many labels. Some look unnecessary. Restrict
    # pattern to match helpful labels.
    relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      # Node /metrics in v1.6+ are accessible via a proxy through the
      # kubernetes api server. So, we must update the target and metric path.
      - target_label: __address__
        replacement: kubernetes.default.svc:443
      - source_labels: [__meta_kubernetes_node_name]
        regex: (.+)
        target_label: __metrics_path__
        replacement: /api/v1/nodes/${1}/proxy/metrics


  # Scrape config for kubernetes pods.
  #
  # Kubernetes pods are scraped when they have an annotation:
  #   `prometheus.io/scrape=true`.
  #
  # Only container that include an explicit containerPort declaration are
  # scraped. For example:
  #
  #      ports:
  #        - containerPort: 9090
  #
  # Configuration expects the default HTTP protocol scheme.
  # Configuration expects the default path of /metrics on targets.
  - job_name: 'kubernetes-pods'
    kubernetes_sd_configs:
      - role: pod

    relabel_configs:
      # For inventory, record whether a pod is ready. This helps distinguish
      # between: missing from inventory, not ready and failing, ready but
      # failing, ready and working.
      # and working.
      - source_labels: [__meta_kubernetes_pod_ready]
        action: replace
        target_label: ready

      # Check for the prometheus.io/scrape=true annotation.
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true

      # Only keep containers that have a declared container port.
      - source_labels: [__meta_kubernetes_pod_container_port_number]
        action: keep
        regex: (\d+)

      # Copy all pod labels from kubernetes to the Prometheus metrics.
      - action: labelmap
        regex: __meta_kubernetes_pod_label_(.+)

      # Add the kubernetes namespace as a Prometheus label.
      - source_labels: [__meta_kubernetes_namespace]
        action: replace
        target_label: namespace

      # Extract the "<cluster>-<node-pool>" name from the GKE node name.
      - source_labels: [__meta_kubernetes_pod_node_name]
        action: replace
        regex: gke-(.*)(-[^-]+){2}
        replacement: $1
        target_label: cluster

      # Identify the deployment name of a pod or daemon set. This requires two
      # steps. Two steps are necessary to handle both daemon set pod names and
      # deployment pod names correctly. And, because, evidently, a combined
      # regex doesn't work as expected.
      #
      # Step 1: unconditionally remove the last field of a pod or daemonset.
      #   e.g. prometheus-server-3165440997-ppf9w
      #   e.g. node-exporter-ltxgz
      - source_labels: [__meta_kubernetes_pod_name]
        action: replace
        regex: (.*)(-[^-]{5})
        replacement: $1
        target_label: deployment

      # Step 2: Remove the trailing pod_template_hash if present.
      # In the case of a daemon set that does not have the trailing hash, the
      # regex will not match and deployment remains unchanged.
      #   e.g. prometheus-server-3165440997
      - source_labels: [deployment]
        action: replace
        regex: (.*)(-[0-9]+)
        replacement: $1
        target_label: deployment

      # Add the kubernetes pod name.
      - source_labels: [__meta_kubernetes_pod_name]
        action: replace
        target_label: pod

      # Add the kubernetes pod container name.
      - source_labels: [__meta_kubernetes_pod_container_name]
        action: replace
        target_label: container


  # Scrape config for kubernetes service endpoints.
  #
  # Service endpoints are scraped when they have an annotation:
  #   `prometheus.io/scrape=true`.
  #
  # Port 80 is sraped by default. To use a different port, use the annotation:
  #   `prometheus.io/port=9090`.
  #
  # Configuration expects the default HTTP protocol scheme.
  # Configuration expects the default path of /metrics on targets.
  - job_name: 'kubernetes-service-endpoints'
    kubernetes_sd_configs:
      - role: endpoints

    relabel_configs:
      # Check for the prometheus.io/scrape=true annotation.
      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      # Check for the prometheus.io/port=<port> annotation.
      - source_labels: [__address__,
                        __meta_kubernetes_service_annotation_prometheus_io_port]
        action: replace
        target_label: __address__
        # A google/re2 regex, matching addresses with or without default ports.
        # NB: this will not work with IPv6 addresses. But, atm, kubernetes uses
        # IPv4 addresses for internal network and GCE doesn not support IPv6.
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
      # Copy all service labels from kubernetes to the Prometheus metrics.
      - action: labelmap
        regex: __meta_kubernetes_service_label_(.+)
      # Add the kubernetes namespace as a Prometheus label.
      - source_labels: [__meta_kubernetes_namespace]
        action: replace
        target_label: kubernetes_namespace
      # Add the kubernetes service name as a Prometheus label.
      - source_labels: [__meta_kubernetes_service_name]
        action: replace
        target_label: kubernetes_name


  # Scrape config for legacy targets.
  #
  # Using an out-of-band process, generated configs can be copied into the
  # Prometheus container. Prometheus will periodically re-read these files to
  # load any new targets or remove missing ones.
  #
  # The file format is described here:
  #   https://prometheus.io/docs/operating/configuration/#file_sd_config
  - job_name: 'legacy-targets'
    file_sd_configs:
      - files:
          # NOTE: The path /legacy-targets does not exist in the default
          # image. Mount this path as a volume with the docker or kubernetes
          # configuration.
          - /legacy-targets/*.json
          - /legacy-targets/*.yaml
        # Attempt to re-read files every five minutes.
        refresh_interval: 5m


  # Scrape config for the nagios exporter.
  #
  # The nagios exporter generates its own labels.
  #
  # TODO(soltesz): the "job=nagios.measurementlab.net:5000" label may be
  # unnecessary; consider removing or replacing the job label.
  - job_name: 'nagios-exporter'
    static_configs:
      - targets: ['nagios.measurementlab.net:5000']

  # Scrape config for federation.
  #
  # The '/federate' target allows retrieving a set of timeseries from other
  # prometheus servers.
  #
  # Learn more at:
  #   https://prometheus.io/docs/operating/federation/#configuring-federation
  #
  - job_name: 'federation-targets'
    # Normally, the prometheus server updates target labels at collection time,
    # but if we're aggregating from other prometheus servers, we want to take
    # them as-is.
    honor_labels: true
    metrics_path: '/federate'
    # Allow more time, since the number of metrics may be large.
    scrape_timeout: 50s
    params:
      'match[]':
        # Note: For now collect only the metrics necessary for alerts. There
        # must be at least one match parameter.Prometheus selects the union of
        # all match parameters.
        - 'up{application="scraper"}'
        - 'up{application="scraper-sync"}'
        - '{deployment="downloader"}'
        - 'scraper_maxrawfiletimearchived'
        - 'scraper_lastcollectionattempt'

    file_sd_configs:
      - files:
          # NOTE: The path /federation-targets does not exist in the default
          # image. Mount this path as a volume with the docker or kubernetes
          # configuration.
          - /federation-targets/*.json
        # Attempt to re-read files every five minutes.
        refresh_interval: 5m

  # Blackbox configurations.
  #
  # Each blackbox configuration uses a different probe (tcp, icmp, http, etc).
  - job_name: 'blackbox-targets'
    metrics_path: /probe

    file_sd_configs:
      - files:
          - /blackbox-targets/*.json
        # Attempt to re-read files every five minutes.
        refresh_interval: 5m

    # This relabel config is necessary. The relabel config redefines the address
    # to scrape and sets the correct parameters to pass to the scrape target.
    #
    # While not as direct as other configs, this approach allows us to specify a
    # dynamic list of targets for a static blackbox exporter. This is also the
    # supported configuration: https://github.com/prometheus/blackbox_exporter
    relabel_configs:

      # The default __address__ value is a target host from the config file.
      # Here, we set (i.e. "replace") a request parameter "target" equal to the
      # host value.
      - source_labels: [__address__]
        regex: (.*)(:80)?
        target_label: __param_target
        replacement: ${1}

      # Use the "module" label defined in the input file as the module name for
      # the blackbox exporter request.
      - source_labels: [module]
        regex: (.*)
        target_label: __param_module
        replacement: ${1}

      # Use the target parameter defined above and use it to define the
      # "instance" label.
      - source_labels: [__param_target]
        regex: (.*)
        target_label: instance
        replacement: ${1}

      # Since __address__ is the target that prometheus will contact,
      # unconditionally reset the __address__ label to the blackbox exporter
      # address.
      - source_labels: []
        regex: .*
        target_label: __address__
        replacement: blackbox-public-service.default.svc.cluster.local:9115


  # Scrape config for pushgateway.
  #
  # Push gateways allow services to publish metrics for later collection by
  # prometheus. Push gateways are helpful for processes that cannot run an
  # exporter, whose exporter is not publically accessible, or processes that
  # are short-lived.
  #
  # Like federation, labels set on the pushgatway should be honored by
  # prometheus.
  #
  # Learn more at:
  #   https://github.com/prometheus/pushgateway#prometheus-pushgateway
  - job_name: 'pushgateway-targets'
    # Normally, the prometheus server updates target labels at collection time,
    # but here we want to take them as-is.
    honor_labels: true

    static_configs:
      - targets:
        - pushgateway-public-service.default.svc.cluster.local:9091

  # Scrape config for App Engine Flex VMs.
  #
  # In order to scrape Prometheus metrics directly from Flex VMs, the app.yaml
  # configuration must include a rule to forward the container port to the host
  # VM. The measurementlab/mlab-service-discovery detects these VMs and
  # generates a targets file.
  #
  # Learn more at:
  #   TODO(soltesz): add canonical link to the service discovery repo when it
  #   exists.
  - job_name: 'aeflex-targets'
    file_sd_configs:
      - files:
          - /aeflex-targets/*.json
        # Attempt to re-read files every five minutes.
        refresh_interval: 5m

    relabel_configs:
      # Copy all labels from the aeflex service discovery.
      - action: labelmap
        regex: __aef_(.+)
  # TODO(JM) make this work with all instances
  # Scrape config for the annotation service
  - job_name: 'annotation-service'
    static_configs:
      - targets: ['annotator-dot-mlab-sandbox.appspot.com']
