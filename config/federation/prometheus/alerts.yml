# M-Lab alert configuration.
#
# ALERT <alert name>
#   IF <expression>
#   [ FOR <duration> ]
#   [ LABELS <label set> ]
#   [ ANNOTATIONS <label set> ]
#
## Notes about delay.
#
# There are inherent delays in the monitoring and alert pipeline. When
# designing alerts, remember that each step includes some delay:
#
#  * the time between reality changing and monitoring observing the change.
#  * the time between the observed change and the delay defined by the alert.
#  * the time between a firing alert and the "group_wait" time in alertmanager.
#  * the time between sending a notification and the notification arriving.
#
# For example, for a 60s collection period, a 2m alert delay, a 30s group_wait,
# and a 15s-3m SMS delivery delay, the time between reality changing and a
# human knowing could be over six minutes.
#
# Your job is to balance false-positives with responsiveness when necessary.

# ClusterDown: when any of the federated prometheus k8s clusters is down for
# three sample periods, then raise an alert.
#
# All scraper metrics come from federated targets, so this is critical.
ALERT ClusterDown
  IF up{job="federation-targets"} == 0
  FOR 10m
  LABELS {
    severity = "page"
  }
  ANNOTATIONS {
    summary = "Instance {{ $labels.instance }} down",
    description = "{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 2 minutes.",
  }

##
## SLOs
##

# MachineSLO
#
# SidestreamIsNotRunning: an M-Lab server is online, but the sidestream exporter
# is not. Since sidestream is a core service, this must be fixed.
ALERT SidestreamIsNotRunning
  IF sum_over_time(up{service="sidestream"}[10m]) == 0
        AND ON(machine)
     sum_over_time(probe_success{service="ssh806"}[20m]) / 20 >= 0.90
        UNLESS ON(machine)
     lame_duck_node == 1
  FOR 10m
  LABELS {
    severity = "page"
  }
  ANNOTATIONS {
    summary = "",
    description = "",
  }


# ScraperSLO
#
# ScraperMostRecentArchivedFileTimeIsTooOld: scraper uploads archives for a
# machine once a day. If the machine is online (for at least 30 hours), but
# scraper has not uploaded an archive for that machine for more than two days
# plus 8 hours, there is a problem.
#
# Note: we can wait two days because we expect that either a) few machines are
# affected by this at once, or b) many machines are affected and the
# ParserDailyVolumeTooLow will trigger first.
#
# Note: the delay threshold is set to 2h to prevent false positives. For
# example, if a machine remains running while it is not network accessible,
# then the machine will need time for scraper to catch up once it is network
# accessible again.
#
# TODO(soltesz): remove the != 0 check when legacy records are removed.
ALERT ScraperMostRecentArchivedFileTimeIsTooOld
  IF (time() - (scraper_maxrawfiletimearchived{container="scraper-sync"} != 0)) > (56 * 60 * 60)
        AND ON(machine)
     (time() - process_start_time_seconds{service="sidestream"}) > (30 * 60 * 60)
        UNLESS ON(machine)
     lame_duck_node == 1
  FOR 2h
  LABELS {
    severity = "page"
  }
  ANNOTATIONS {
    summary = "Scraper max file mtime is too old {{ $labels.rsync_url }}",
    description = "Max file mtime for {{ $labels.rsync_url }} is older than 56 hours.",
  }

# Scraper internal consistency.
#
# Verify that for every running scraper there is a corresponding metric from
# scraper-sync indicating that a collection was attempted. These should always
# be in sync with one another.
#
# We use scraper_lastcollectionattempt because scraper_maxrawfiletimearchived
# is not updated until the first successful upload. This is not possible before
# a machine comes online.
ALERT ScraperSyncPresentWithoutScraperCollector
  IF (scraper_lastcollectionattempt{container="scraper-sync"}
        UNLESS ON(machine, experiment, rsync_module)
           up{container="scraper"})
  FOR 3h
  LABELS {
    severity = "page"
  }
  ANNOTATIONS {
    summary = "",
    description = "",
  }

ALERT ScraperCollectorMissingFromScraperSync
  IF (up{container="scraper"}
        UNLESS ON(machine, experiment, rsync_module)
           scraper_lastcollectionattempt{container="scraper-sync"})
  FOR 3h
  LABELS {
    severity = "page"
  }
  ANNOTATIONS {
    summary = "",
    description = "",
  }

##
## Inventory.
##

ALERT InventoryConfigurationIsMissing
  IF absent(up{service="ssh806"}) OR absent(up{service="rsyncd"})
  FOR 30m
  LABELS {
    severity = "ticket"
  }
  ANNOTATIONS {
    summary = "Inventory configuration {{ $labels.service }} is missing.",
    description = "Machine or rsyncd service configuration has been missing for too long.",
    hints = "Check the behavior of the m-lab/operator/.travis.yml deployment, the GCS buckets, and the gcp-service-discovery component of prometheus-support.",
  }

ALERT InventoryMachinesWithoutRsyncd
  IF up{service="ssh806"} UNLESS ON(machine) up{service="rsyncd"}
  FOR 30m
  LABELS {
    severity = "ticket"
  }
  ANNOTATIONS {
    summary = "Rsyncd configuration is missing from some machines.",
    hints = "",
  }

ALERT InventoryRsyncdWithoutMachines
  IF up{service="rsyncd"} UNLESS ON(machine) up{service="ssh806"}
  FOR 30m
  LABELS {
    severity = "ticket"
  }
  ANNOTATIONS {
    summary = "Machine configuration is missing for some rsyncd services.",
    hints = "",
  }


##
## Services.
##

ALERT SidestreamServicesAreMissing
  IF absent(up{service="sidestream"})
  FOR 30m
  LABELS {
    severity = "ticket"
  }
  ANNOTATIONS {
    summary = "",
    hints = "",
  }

ALERT SidestreamRunningWithoutMachine
  IF up{service="sidestream"} UNLESS ON(machine) up{service="ssh806"}
  FOR 30m
  LABELS {
    severity = "ticket"
  }
  ANNOTATIONS {
    summary = "",
    hints = "",
  }

ALERT MachineWithoutSidestreamRunning
  IF up{service="ssh806"} UNLESS ON(machine) up{service="sidestream"}
  FOR 30m
  LABELS {
    severity = "ticket"
  }
  ANNOTATIONS {
    summary = "",
    hints = "",
  }

# Scrapers are configured on machine "c", but machine "c" is not in the rsyncd inventory.
ALERT ScraperRunningWithoutRsyncd
  IF up{container="scraper"} UNLESS ON(machine, experiment) up{service="rsyncd"}
  FOR 30m
  LABELS {
    severity = "ticket"
  }
  ANNOTATIONS {
    summary = "",
    hints = "",
  }

# Rsync inventory includes machine "b", but machine "b" does not have a configured scraper.
ALERT RsyncRunningWithoutScraper
  IF up{service="rsyncd"} UNLESS ON(machine, experiment) up{container="scraper"}
  FOR 30m
  LABELS {
    severity = "ticket"
  }
  ANNOTATIONS {
    summary = "",
    hints = "",
  }


# DownloaderIsFailingToUpdate: The downloader hasn't successfully retrieved the files in at least 21 hours, meaning that at least the last two download attempts have failed.
ALERT DownloaderIsFailingToUpdate
  IF time()-downloader_last_success_time_seconds > (21 * 60 * 60)
  FOR 5m
  LABELS {
    severity = "page"
  }
  ANNOTATIONS {
    summary = "Neither of the last two attempts to download the maxmind/routeviews feeds were successful.",
    hints = "Check for errors with the downloader service on grafana with the downloader_Error_Count metric, or check the stackdriver logs for the downloader cluster."
  }

# DownloaderNotRunning: The downloader cluster crashed and not running at all.
ALERT DownloaderDownOrMissing
  IF up{container="downloader"} == 0 OR absent(up{container="downloader"})
  FOR 10m
  LABELS {
    severity = "ticket"
  }
  ANNOTATIONS {
    summary = "The downloader for maxmind/routeviews feeds is down on {{ $labels.instance }}.",
    hints = "Check the status of Kubernetes clusters on each M-Lab GCP project. Look at the travis deployment history for m-lab/downloader."
  }

# Prometheus is unable to get data from the snmp_exporter service.
ALERT SnmpExporterDownOrMissing
  IF up{job="snmp-exporter"} == 0 OR absent(up{job="snmp-exporter"})
  FOR 10m
  LABELS {
    severity = "ticket"
  }
  ANNOTATIONS {
    summary = "The snmp_exporter service is down on {{ $labels.instance }}.",
    hints = "The snmp_exporter service runs in a Docker container on a GCE VM named 'snmp-exporter' in each M-Lab GCP project. Look at the Travis-CI builds/deploys for m-lab/prometheus-snmp-exporter, or SSH to the VM and poke around."
  }

# Some SNMP metrics are missing from Prometheus. These should always be present.
# The wait period shouuld be longer than that for the SnmpExporterDownOrMissing
# alert.
ALERT SnmpExporterMissingMetrics
  IF absent(ifHCOutOctets)
  FOR 30m
  LABELS {
    severity = "page"
  }
  ANNOTATIONS {
    summary = "Expected SNMP metrics are missing from Prometheus!",
    hints = "If the snmp_exporter service is running, then there may be a target configuration error. Check the target definitions in GCS and the target status in Prometheus.",
    prometheus_targets = "http://status.mlab-oti.measurementlab.net:9090/targets",
    gcsbucket = "https://console.cloud.google.com/storage/browser/operator-mlab-oti/prometheus/snmp-targets",
    dashboard = "http://status.mlab-oti.measurementlab.net:3000/dashboard/db/switch-metrics"
  }

# Prometheus is unable to get data from the script_exporter service.
ALERT ScriptExporterDownOrMissing
  IF up{job="script-exporter"} == 0 OR absent(up{job="script-exporter"})
  FOR 10m
  LABELS {
    severity = "ticket"
  }
  ANNOTATIONS {
    summary = "The script_exporter service is down on {{ $labels.instance }}.",
    hints = "The script_exporter service runs in a Docker container on a GCE VM named 'script-exporter' in each M-Lab GCP project. For deployment details and troubleshooting, you can usually figure out the issue by looking through the Travis-CI build logs: https://travis-ci.org/m-lab/prometheus-script-exporter. You can also look for hints in the dashboard for the GCE instance, or by SSHing to the instance itself."
  }

# Some script_exporter metrics are missing from Prometheus. These should always
# be present. The wait period should be longer than that for the
# ScriptExporterDownOrMissing alert.
ALERT ScriptExporterMissingMetrics
  IF absent(script_success{service="ndt_e2e"})
        OR absent(script_success{service="ndt_queue"})
  FOR 30m
  LABELS {
    severity = "page"
  }
  ANNOTATIONS {
    summary = "Expected script_exporter metrics are missing from Prometheus!",
    hints = "If the script_exporter service is running, then there may be a target configuration error. Check the target definitions in GCS and the target status in Prometheus.",
    prometheus_targets = "http://status.mlab-oti.measurementlab.net:9090/targets",
    gcsbucket = "https://console.cloud.google.com/storage/browser/operator-mlab-oti/prometheus/script-targets",
    dashboard = "http://status.mlab-oti.measurementlab.net:3000/dashboard/file/Ops_SwitchOverview.json"
  }

# Prometheus is unable to get data from the blackbox_exporter service for IPv4
# probes.
ALERT BlackboxExporterIpv4DownOrMissing
  IF up{job="blackbox-targets"} == 0 OR absent(up{job="blackbox-targets"})
  FOR 10m
  LABELS {
    severity = "ticket"
  }
  ANNOTATIONS {
    summary = "The blackbox_exporter service is down for IPv4 probes.",
    hints = "Check the status of the blackbox-server pod in the prometheus-federation cluster on each M-Lab GCP project."
  }

# Prometheus is unable to get data from the blackbox_exporter service for IPv6
# probes.
ALERT BlackboxExporterIpv6DownOrMissing
  IF up{job="blackbox-targets-ipv6"} == 0
        OR absent(up{job="blackbox-targets-ipv6"})
  FOR 10m
  LABELS {
    severity = "ticket"
  }
  ANNOTATIONS {
    summary = "The blackbox_exporter service is down for IPv6 probes.",
    hints = "The blackbox_exporter for IPv6 checks runs in a Linode VM. Make sure the VM is up and running. If it is, check the status of the BBE container running on it. Domains for VM are like blackbox-exporter-ipv6.<project>.measurementlab.net."
  }

# More than a certain percentage of NDT servers meet the criteria for being
# down.
ALERT TooManyNdtServersDown
  IF count_scalar(
    probe_success{service="ndt_raw"} AND ON(machine)
      up{service="nodeexporter"} == 1 UNLESS ON(machine)
      lame_duck_node{} == 1
    UNLESS ON(machine) (
      probe_success{service="ndt_raw"} == 1 AND ON(machine)
      probe_success{service="ndt_ssl"} == 1 AND ON(machine)
      script_success{service="ndt_e2e"} == 1 AND ON(machine)
      vdlimit_used{experiment="ndt.iupui"} /
        vdlimit_total{experiment="ndt.iupui"} < 0.95
    )
  )
  /
  count(
    probe_success{service="ndt_raw"} AND ON(machine)
    up{service="nodeexporter"} == 1 UNLESS ON(machine)
    lame_duck_node{} == 1
  ) > 0.25
  FOR 30m
  LABELS {
    severity = "page"
  }
  ANNOTATIONS {
    summary = "Too large a percentage of NDT servers are down.",
    hints = "Make sure that the blackbox_exporter, script_exporter and node_exporters are all working as expected. Was any update to the platform just released?",
    dashboard = "https://grafana.mlab-oti.measurementlab.net/dashboard/file/Ops_PlatformOverview.json"
  }

# One or more NDT-specific metrics is missing. These are the NDT metrics that
# mlab-ns relies on to determine whether NDT is up and running, so we need to
# make sure that the metrics are always present. NOTE: mlab-ns additionally
# relies on the script_exporter metric 'script_success{service="ndt_e2e"}', but
# alerting for that metric is already handled by the
# ScriptExporterMissingMetrics alert.
ALERT NdtMetricsMissing
  IF absent(probe_success{service="ndt_raw"})
        OR absent(probe_success{service="ndt_raw_ipv6"})
        OR absent(probe_success{service="ndt_ssl"})
        OR absent(probe_success{service="ndt_ssl_ipv6"})
        OR absent(vdlimit_used{experiment="ndt.iupui"})
        OR absent(vdlimit_total{experiment="ndt.iupui"})
  FOR 30m
  LABELS {
    severity = "page"
  }
  ANNOTATIONS {
    summary = "A metric for an NDT service is missing.",
    hints = "If the blackbox_exporter service is running, then there may be a target configuration error. Check the target definitions in GCS and the target status in Prometheus. vdlimit_* metrics are provided by node_exporter on each node.",
    prometheus_targets = "https://prometheus.mlab-oti.measurementlab.net/targets#job-blackbox-targets"
  }

# One or more Neubot-specific metrics is missing. These are the Neubot metrics that
# mlab-ns relies on to determine whether Neubot is up and running, so we need to
# make sure that the metrics are always present.
ALERT NeubotMetricsMissing
  IF absent(probe_success{service="neubot"})
        OR absent(probe_success{service="neubot_ipv6"})
  FOR 30m
  LABELS {
    severity = "ticket"
  }
  ANNOTATIONS {
    summary = "A metric for a Neubot service is missing.",
    hints = "If the blackbox_exporter service is running, then there may be a target configuration error. Check the target definitions in GCS and the target status in Prometheus.",
    prometheus_targets = "https://prometheus.mlab-oti.measurementlab.net/targets#job-blackbox-targets"
  }

# One or more Mobiperf-specific metrics is missing. These are the Mobiperf
# metrics that mlab-ns relies on to determine whether Mobiperf is up and
# running, so we need to make sure that the metrics are always present.
ALERT MobiperfMetricsMissing
  IF absent(probe_success{service="mobiperf"})
        OR absent(probe_success{service="mobiperf_ipv6"})
  FOR 30m
  LABELS {
    severity = "ticket"
  }
  ANNOTATIONS {
    summary = "A metric for a Mobiperf service is missing.",
    hints = "If the blackbox_exporter service is running, then there may be a target configuration error. Check the target definitions in GCS and the target status in Prometheus.",
    prometheus_targets = "https://prometheus.mlab-oti.measurementlab.net/targets#job-blackbox-targets"
  }

# Some number of nodes don't have a lame-duck status.
ALERT LameDuckMetricMissingForNode
  IF count(
    up{service="nodeexporter"} == 1 UNLESS ON(machine) lame_duck_node{}
  ) > 0
  FOR 30m
  LABELS {
    severity = "ticket"
  }
  ANNOTATIONS {
    summary = "Some number of nodes are missing lame-duck status metrics.",
    hints = "Check /var/spool/node_exporter/ on the node to see if the file lame_duck.prom is missing. If it is, use the mlabops Ansible lame-duck.yaml playbook to restore it."
  }

# TODO:
#   Replace this with two other alerts:
#    1.  Alert if hourly test volume on servers drops relative to same hour on recent days.
#    2.  E2E alert that compares rows in tables to test volume on servers.
#
# ParserDailyVolumeTooLow: 24 hour test volume has dropped over 30% compared to
# the average of the 2 smallest test volumes of 4 days out of the last week.  Two vectors
# of conservative constant value avoid false alarms when there is little valid history.
# On occasion, processing may fall behind a bit.  The "FOR 2h" waits 2h before triggering
# an actual alert, so the pipeline may fall behind for up to 2 hours without alerting.
# However, if the pipeline falls several hours behind, and stays behind for more than 
# 2 hours, the alert will fire.
#
# In normal operation, we expect the 50th quantile to split mid-way between the two smallest
# volume days of the 4 sample days.  The 4 sample always include one weekend day, so one of
# the two smallest days will generally be a weekend day.
# For example, for a Tuesday, the prior data might be ordered (decreasing):
# M, F, W, Sa, C1, C2,
# and the 50th quantile will be midway between previous Sat and previous Wed.
#
# The alert condition ignores batch processing.
#
# Implementation notes:
# This alert uses label_replace to merge multiple vectors.  We tried simpler queries using
# AND, OR or +, but these do not do what we need.  We use label_replace to add a new "delay"
# label, which then allows us to compute quantile across multiple vectors.  (Or we could compute
# sums, averages, topk, min, etc.)
# The constant vectors require adding two labels, "service" which associates with a pipeline, and
# "delay" which differentiates from the actual delayed metrics for 1d, 3d, etc.
# For each pipeline service, the quantile computation then aggregates across the 6 vectors in
# the delay dimension.
ALERT ParserDailyVolumeTooLow
  IF sum by(service) (increase(etl_test_count{service!~".*batch.*", status="ok"}[24h]))
      < (0.7 * quantile by(service)(0.50,
         label_replace(sum by(service) (increase(etl_test_count{service!~".*batch.*", status="ok"}[24h] offset 1d)),"delay","1d","",".*" ) OR
         label_replace(sum by(service) (increase(etl_test_count{service!~".*batch.*", status="ok"}[24h] offset 3d)),"delay","3d","",".*" ) OR
         label_replace(sum by(service) (increase(etl_test_count{service!~".*batch.*", status="ok"}[24h] offset 5d)),"delay","5d","",".*" ) OR
         label_replace(sum by(service) (increase(etl_test_count{service!~".*batch.*", status="ok"}[24h] offset 7d)),"delay","7d","",".*" ) OR
         label_replace(label_replace(vector(0),"delay","c1","",".*" ), "service", "etl-disco-parser",      "", ".*") OR
         label_replace(label_replace(vector(0),"delay","c2","",".*" ), "service", "etl-disco-parser",      "", ".*") OR
         label_replace(label_replace(vector(0),"delay","c1","",".*" ), "service", "etl-ndt-parser",        "", ".*") OR
         label_replace(label_replace(vector(0),"delay","c2","",".*" ), "service", "etl-ndt-parser",        "", ".*") OR
         label_replace(label_replace(vector(0),"delay","c1","",".*" ), "service", "etl-sidestream-parser", "", ".*") OR
         label_replace(label_replace(vector(0),"delay","c2","",".*" ), "service", "etl-sidestream-parser", "", ".*") OR
         label_replace(label_replace(vector(0),"delay","c1","",".*" ), "service", "etl-traceroute-parser", "", ".*") OR
         label_replace(label_replace(vector(0),"delay","c2","",".*" ), "service", "etl-traceroute-parser", "", ".*")
         ))
  FOR 2h
  LABELS {
    severity = "page"
  }
  ANNOTATIONS {
    summary = "Today's test volume is less than 70% of nominal daily volume.",
    hints = "Are machines online? Is data being collected? Is the parser working?",
    url = "http://status.mlab-oti.measurementlab.net:3000/dashboard/file/Alert_ParserDailyVolumeTooLow.json"
  }

# Nagios exporter cannot be scrapped (i.e. is down according to prometheus).
ALERT NagiosExporterDown
  IF up{job="nagios-oam-exporter"} == 0 OR up{job="nagios-exporter"} == 0
  FOR 10m
  LABELS {
    severity = "ticket"
  }
  ANNOTATIONS {
    summary = "The Nagios exporter instance is down on {{ $labels.instance }}.",
    hints = "The Nagios exporter instance runs in a Linode VM."
  }

# Nagios exporter is running but not fully functional.
ALERT NagiosExporterUnavailable
  IF nagios_livestatus_available{job="nagios-oam-exporter"} == 0 OR nagios_livestatus_available{job="nagios-exporter"} == 0
  FOR 10m
  LABELS {
    severity = "ticket"
  }
  ANNOTATIONS {
    summary = "The Nagios exporter instance is unavailable on {{ $labels.instance }}.",
    hints = "The Nagios exporter instance runs in a Linode VM."
  }

# Nagios exporter is not scraped at all.
ALERT NagiosExporterMissing
  IF absent(up{job="nagios-oam-exporter"}) OR absent(up{job="nagios-exporter"})
  FOR 10m
  LABELS {
    severity = "ticket"
  }
  ANNOTATIONS {
    summary = "The Nagios exporter instance is not being monitored.",
    hints = "The Nagios exporter instance should run in a Linode VM."
  }

# The node_exporter running on eb.measurementlab.net is down.
ALERT NodeExporterOnEbDownorMissing
  IF up{job="eb-node-exporter"} == 0 OR absent(up{job="eb-node-exporter"})
  FOR 10m
  LABELS {
    severity = "ticket"
  }
  ANNOTATIONS {
    summary = "The node_exporter instance running on eb.measurementlab.net is down.",
    hints = "Login to EB to see if it is in fact crashed. If so, look through the logs."
  }


# GardenerDownOrMissing fires when the etl-gardener is down or absent.
ALERT GardenerDownOrMissing
  # TODO: enable annotations to ignore some container ports, and simplify this query.
  # https://github.com/m-lab/prometheus-support/issues/48
  IF up{container="etl-gardener", instance=~".*:9090"} == 0 OR absent(up{container="etl-gardener", instance=~".*:9090"})
  FOR 10m
  LABELS {
    severity = "ticket"
  }
  ANNOTATIONS {
    summary = "The ETL Gardener instance is down on {{ $labels.instance }}",
    hints = "The Gardener runs in the data-processing-cluster"
  }
