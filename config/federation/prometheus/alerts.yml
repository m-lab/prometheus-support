# M-Lab alert configuration.
#
# See https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules
# for more information on the alerting rules syntax.
#
## Notes about delay.
#
# There are inherent delays in the monitoring and alert pipeline. When
# designing alerts, remember that each step includes some delay:
#
#  * the time between reality changing and monitoring observing the change.
#  * the time between the observed change and the delay defined by the alert.
#  * the time between a firing alert and the "group_wait" time in alertmanager.
#  * the time between sending a notification and the notification arriving.
#
# For example, for a 60s collection period, a 2m alert delay, a 30s group_wait,
# and a 15s-3m SMS delivery delay, the time between reality changing and a
# human knowing could be over six minutes.
#
# Your job is to balance false-positives with responsiveness when necessary.

groups:
- name: alerts.yml
  rules:
# ClusterDown: when any of the federated prometheus k8s clusters is down for
# three sample periods, then raise an alert.
  - alert: ClusterDown
    expr: up{job="federation-targets"} == 0
    for: 10m
    labels:
      repo: dev-tracker
      severity: page
      cluster: prometheus-federation
      page_project: mlab-oti
    annotations:
      summary: Instance {{ $labels.instance }} down
      description: '{{ $labels.instance }} of job {{ $labels.job }} has been down
        for more than 10 minutes.'

##
## SLOs
##
#
# SwitchSLO
#
# A switch at a site has been unpingable for too long and we need to contact
# the site host or transit provider to investigate.
  - alert: SwitchUnpingableAtSite
    expr: |
      probe_success{instance=~"s1.*",module="icmp"} == 0
        unless on(site) gmx_site_maintenance == 1
    for: 1d
    labels:
      repo: ops-tracker
      severity: ticket
      cluster: prometheus-federation
    annotations:
      summary: The switch at {{ $labels.site }} has been unpingable for too long.
      description: >
        The switch has been unpingable for too long. The problem could be with
        the switch itself, or with the transit provider.
      dashboard: 'https://grafana.mlab-oti.measurementlab.net/d/SuqnZ6Hiz/?orgId=1&var-site_name={{$labels.site}}'

# DownloaderIsFailingToUpdate: The downloader hasn't successfully retrieved the files in
# at least 21 hours, meaning that at least the last two download attempts have failed.
  - alert: DownloaderIsFailingToUpdate
    expr: time() - downloader_last_success_time_seconds > (21 * 60 * 60)
    for: 1h
    labels:
      repo: dev-tracker
      severity: ticket
      cluster: prometheus-federation
    annotations:
      summary: Neither of the last two attempts to download the maxmind or
        routeviews feeds were successful.
      description: Check for errors with the downloader service on grafana with
        the downloader_error_count metric, or check the stackdriver logs for
        the downloader cluster.
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/ZGuYht1mk/

# DownloaderNotRunning: The downloader cluster crashed and not running at all.
  - alert: DownloaderDownOrMissing
    expr: up{container="downloader"} == 0 or absent(up{container="downloader"})
    for: 10m
    labels:
      repo: dev-tracker
      severity: ticket
      cluster: prometheus-federation
    annotations:
      summary: The downloader for maxmind/routeviews feeds is down or missing.
      description: Check the status of Kubernetes clusters on each M-Lab GCP
        project. Look at the travis deployment history for m-lab/downloader.

# Prometheus is unable to get data from the script_exporter deployment.
  - alert: ScriptExporterDownOrMissing
    expr: up{deployment="script-exporter"} == 0 or absent(up{deployment="script-exporter"})
    for: 10m
    labels:
      repo: ops-tracker
      severity: ticket
      cluster: prometheus-federation
    annotations:
      summary: The script_exporter service is down on missing.
      description: >
        The script_exporter service runs in the prometheus-federation GKE
        cluster in each M-Lab GCP project. For deployment details and
        troubleshooting, you can usually figure out the issue by looking
        through the [Travis-CI build logs][1]. You can also look for hints
        in the Kubernetes logs for the pod, or by kexec'ing into the pod
        itself.
        [1]: https://travis-ci.org/m-lab/prometheus-support

# Some script_exporter metrics are missing from Prometheus. These should always
# be present. The wait period should be longer than that for the
# ScriptExporterDownOrMissing alert.
  - alert: ScriptExporterMissingMetrics
    expr: |
      absent(script_success{service="ndt5_client"})
    for: 30m
    labels:
      repo: ops-tracker
      severity: ticket
      cluster: prometheus-federation
    annotations:
      summary: Expected script_exporter metrics are missing from Prometheus!
      description: >
        If the script_exporter service is running, then there may be a target
        configuration error. Check the [target definitions in GCS][1] and the [target
        status in Prometheus][2].
        [1]: https://console.cloud.google.com/storage/browser/operator-mlab-oti/prometheus/script-targets
        [2]: https://prometheus.mlab-oti.measurementlab.net/targets
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/coR-U9gMx

# Prometheus is unable to get data from the blackbox_exporter service for IPv4
# probes. The service is down, or the metric is missing.
  - alert: BlackboxExporterIpv4DownOrMissing
    expr: |
      up{job="blackbox-exporter-ipv4"} == 0
        or absent(up{job="blackbox-exporter-ipv4"})
    for: 10m
    labels:
      repo: ops-tracker
      severity: ticket
      cluster: prometheus-federation
    annotations:
      summary: The blackbox_exporter service is down for IPv4 probes.
      description: Check the status of the blackbox-server pod in the
        prometheus-federation cluster on each M-Lab GCP project.

# Prometheus is unable to get data from the blackbox_exporter service for IPv6
# probes. The service is down, or the metric is missing.
  - alert: BlackboxExporterIpv6DownOrMissing
    expr: up{job="blackbox-exporter-ipv6"} == 0 or absent(up{job="blackbox-exporter-ipv6"})
    for: 10m
    labels:
      repo: ops-tracker
      severity: ticket
      cluster: prometheus-federation
    annotations:
      summary: The blackbox_exporter service is down or missing for IPv6 probes.
      description: The blackbox_exporter for IPv6 checks runs in a Linode VM.
        Make sure the VM is up and running. If it is, check the status of the
        BBE container running in the VM. Domains for VMs are like
        blackbox-exporter-ipv6.<project>.measurementlab.net.

# Unable to scrape the Github Maintenance exporter or the job is missing.
  - alert: GithubMaintenanceExporterDownOrMissing
    expr: |
      up{job="nginx-proxied-services", service="gmx"} == 0
        or absent(up{job="nginx-proxied-services", service="gmx"})
    for: 10m
    labels:
      repo: ops-tracker
      severity: ticket
      cluster: prometheus-federation
    annotations:
      summary: Scraping of the Github Maintenance Exporter is failing or missing.
      description: >
        Scraping of the Github Maintenance Exporter is failing or missing.
        Check that the gmx-server deployment is healthy and that a pod for it
        exists. Check the status of the pod for errors. Check the logs of the
        pod for errors.  Check the [reason that the scrape failed][1].
        [1]: https://prometheus.mlab-oti.measurementlab.net/targets#job-nginx-proxied-services

  # Blackbox exporter probes to a machine are succeeding to a node that
  # kubernetes does not know about (not joined to the k8s cluster).
  - alert: MachineRunningWithoutK8sNode
    expr: probe_success{service="ssh", module="ssh_v4_online"} == 1
            unless on(machine) kube_node_status_condition
            unless on(machine) gmx_machine_maintenance == 1
    for: 1h
    labels:
      repo: ops-tracker
      severity: ticket
      cluster: prometheus-federation
    annotations:
      summary: A machine is running that is not part of the k8s cluster.
      description: >
        Blackbox exporter probes (for ssh) are succeeding to a node that
        kubernetes does not know about. This can happen when a machine gets
        segmented from the network, then gets manually deleted from kubernetes,
        then at some point the machine has its network connectivity restored.
        In this situation all of the containers on the machine are still
        running, but the node is no longer known to kubernetes. It becomes like
        a zombie node, possibly continuing to upload data to GCS and serve
        experiment tests. The simple fix is to reboot the zombie node.

  # A TLS certificate in use by an experiment is about to expire.
  - alert: TlsCertificateAboutToExpire
    expr: min by (experiment) ((probe_ssl_earliest_cert_expiry{experiment!=""} - time()) / 86400) < 5
    for: 1h
    labels:
      repo: ops-tracker
      severity: ticket
      cluster: prometheus-federation
    annotations:
      summary: A TLS certificate in use by an experiment will expire in less
        than 5 days. Did cert-manager update the certificate in the cluster?
        Check with `kubectl describe cert mlab-oti-measurement-lab-org`. If the
        certificate in the cluster looks good, check to be sure that the
        experiment container picked up the updated certificate from its mount
        of the certificate Secret.

## mlab-ns queries.
#
# The following alerts are based on the exact queries that mlab-ns runs to
# determine the state of NDT services.
# https://github.com/m-lab/mlab-ns/blob/master/server/mlabns/util/prometheus_status.py
#
# The expression in the denominator of the queries can be read as:
# count `probe_success`es _unless_ the node is in GMX _and_ the node does not
# exist in the cluster. We want nodes in GMX in the denominator, but not for
# sites that don't even exist yet.
#
# TODO(kinkade): The rewrite of mlab-ns should export these types of metrics
# such that we don't have to duplicate the queries here in the alerts.

  # "ndt" mlab-ns query
  - alert: PlatformCluster_TooManyNDT5IPv4ServersDown
    expr: |
      (
        sum(
          min by (experiment, machine) (
            probe_success{service="ndt_raw"} OR
            script_success{service="ndt5_client"} OR
            label_replace(((node_filesystem_size_bytes{cluster="platform", mountpoint="/cache/data"} -
              node_filesystem_avail_bytes{cluster="platform", mountpoint="/cache/data"}) /
                node_filesystem_size_bytes{cluster="platform", mountpoint="/cache/data"}),
                "experiment", "ndt.iupui", "", "") < bool 0.95 OR
            label_replace(kube_node_spec_taint{cluster="platform", key="lame-duck"},
              "experiment", "ndt.iupui", "", "") != bool 1 OR
            label_replace(gmx_machine_maintenance, "experiment", "ndt.iupui", "", "") != bool 1
          )
        ) /
        count(
          probe_success{service="ndt_raw"} unless on(machine)
            (gmx_machine_maintenance == 1 unless on(machine) kube_node_status_condition)
        )
      ) < 0.90
    for: 10m
    labels:
      repo: ops-tracker
      severity: ticket
      cluster: prometheus-federation
    annotations:
      summary: Less than 90% of ndt experiments are online according to mlab-ns.
      description: https://github.com/m-lab/ops-tracker/wiki/Alerts-&-Troubleshooting#PlatformCluster_TooManyServersDown
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/T-t8rWwGz/mlab-ns-prometheus-queries

  # "ndt_ipv6" mlab-ns query
  - alert: PlatformCluster_TooManyNDT5IPv6ServersDown
    expr: |
      (
        sum(
          min by (experiment, machine) (
            probe_success{service="ndt_raw_ipv6"} OR
            script_success{service="ndt5_client"} OR
            label_replace(((node_filesystem_size_bytes{cluster="platform", mountpoint="/cache/data"} -
              node_filesystem_avail_bytes{cluster="platform", mountpoint="/cache/data"}) /
                node_filesystem_size_bytes{cluster="platform", mountpoint="/cache/data"}),
                "experiment", "ndt.iupui", "", "") < bool 0.95 OR
            label_replace(kube_node_spec_taint{cluster="platform", key="lame-duck"},
              "experiment", "ndt.iupui", "", "") != bool 1 OR
            label_replace(gmx_machine_maintenance, "experiment", "ndt.iupui", "", "") != bool 1
          )
        ) /
        count(
          probe_success{service="ndt_raw_ipv6"} unless on(machine)
            (gmx_machine_maintenance == 1 unless on(machine) kube_node_status_condition)
        )
      ) < 0.75
    for: 10m
    labels:
      repo: ops-tracker
      severity: ticket
      cluster: prometheus-federation
    annotations:
      summary: Less than 75% of ndt_ipv6 experiments are online according to mlab-ns.
      description: https://github.com/m-lab/ops-tracker/wiki/Alerts-&-Troubleshooting#PlatformCluster_TooManyServersDown
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/T-t8rWwGz/mlab-ns-prometheus-queries

  # "ndt_ssl" mlab-ns query
  - alert: PlatformCluster_TooManyNDT5TLSIPv4ServersDown
    expr: |
      (
        sum(
          min by (experiment, machine) (
            probe_success{service="ndt_ssl"} OR
            script_success{service="ndt5_client"} OR
            label_replace(((node_filesystem_size_bytes{cluster="platform", mountpoint="/cache/data"} -
              node_filesystem_avail_bytes{cluster="platform", mountpoint="/cache/data"}) /
                node_filesystem_size_bytes{cluster="platform", mountpoint="/cache/data"}),
                "experiment", "ndt.iupui", "", "") < bool 0.95 OR
            label_replace(kube_node_spec_taint{cluster="platform", key="lame-duck"},
              "experiment", "ndt.iupui", "", "") != bool 1 OR
            label_replace(gmx_machine_maintenance, "experiment", "ndt.iupui", "", "") != bool 1
          )
        ) /
        count(
          probe_success{service="ndt_ssl"} unless on(machine)
            (gmx_machine_maintenance == 1 unless on(machine) kube_node_status_condition)
        )
      ) < 0.90
    for: 10m
    labels:
      repo: ops-tracker
      severity: page
      cluster: prometheus-federation
      page_project: mlab-oti
    annotations:
      summary: Less than 90% of ndt_ssl experiments are online according to mlab-ns.
      description: https://github.com/m-lab/ops-tracker/wiki/Alerts-&-Troubleshooting#PlatformCluster_TooManyServersDown
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/T-t8rWwGz/mlab-ns-prometheus-queries

  # "ndt_ssl_ipv6" mlab-ns query
  - alert: PlatformCluster_TooManyNDT5TLSIPv6ServersDown
    expr: |
      (
        sum(
          min by (experiment, machine) (
            probe_success{service="ndt_ssl_ipv6"} OR
            script_success{service="ndt5_client"} OR
            label_replace(((node_filesystem_size_bytes{cluster="platform", mountpoint="/cache/data"} -
              node_filesystem_avail_bytes{cluster="platform", mountpoint="/cache/data"}) /
                node_filesystem_size_bytes{cluster="platform", mountpoint="/cache/data"}),
                "experiment", "ndt.iupui", "", "") < bool 0.95 OR
            label_replace(kube_node_spec_taint{cluster="platform", key="lame-duck"},
              "experiment", "ndt.iupui", "", "") != bool 1 OR
            label_replace(gmx_machine_maintenance, "experiment", "ndt.iupui", "", "") != bool 1
          )
        ) /
        count(
          probe_success{service="ndt_ssl_ipv6"} unless on(machine)
            (gmx_machine_maintenance == 1 unless on(machine) kube_node_status_condition)
        )
      ) < 0.75
    for: 10m
    labels:
      repo: ops-tracker
      severity: ticket
      cluster: prometheus-federation
    annotations:
      summary: Less than 75% of ndt_ssl_ipv6 experiments are online according to mlab-ns.
      description: https://github.com/m-lab/ops-tracker/wiki/Alerts-&-Troubleshooting#PlatformCluster_TooManyServersDown
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/T-t8rWwGz/mlab-ns-prometheus-queries

  # "ndt7" mlab-ns query
  - alert: PlatformCluster_TooManyNDT7IPv4ServersDown
    expr: |
      (
        sum(
          min by (experiment, machine) (
            probe_success{service="ndt7"} OR
            label_replace(((node_filesystem_size_bytes{cluster="platform", mountpoint="/cache/data"} -
              node_filesystem_avail_bytes{cluster="platform", mountpoint="/cache/data"}) /
                node_filesystem_size_bytes{cluster="platform", mountpoint="/cache/data"}),
                "experiment", "ndt.iupui", "", "") < bool 0.95 OR
            label_replace(kube_node_spec_taint{cluster="platform", key="lame-duck"},
              "experiment", "ndt.iupui", "", "") != bool 1 OR
            lame_duck_experiment{cluster="platform"} != bool 1 OR
            label_replace(gmx_machine_maintenance, "experiment", "ndt.iupui", "", "") != bool 1
        )
        ) /
        count(
          probe_success{service="ndt7"} unless on(machine)
            (gmx_machine_maintenance == 1 unless on(machine) kube_node_status_condition)
        )
      ) < 0.90
    for: 10m
    labels:
      repo: ops-tracker
      severity: ticket
      cluster: prometheus-federation
    annotations:
      summary: Less than 90% of ndt7 experiments are online according to mlab-ns.
      description: https://github.com/m-lab/ops-tracker/wiki/Alerts-&-Troubleshooting#PlatformCluster_TooManyServersDown
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/T-t8rWwGz/mlab-ns-prometheus-queries

  # "ndt7_ipv6" mlab-ns query
  - alert: PlatformCluster_TooManyNDT7IPv6ServersDown
    expr: |
      (
        sum(
          min by (experiment, machine) (
            probe_success{service="ndt7_ipv6"} OR
            label_replace(((node_filesystem_size_bytes{cluster="platform", mountpoint="/cache/data"} -
              node_filesystem_avail_bytes{cluster="platform", mountpoint="/cache/data"}) /
                node_filesystem_size_bytes{cluster="platform", mountpoint="/cache/data"}),
                "experiment", "ndt.iupui", "", "") < bool 0.95 OR
            label_replace(kube_node_spec_taint{cluster="platform", key="lame-duck"},
              "experiment", "ndt.iupui", "", "") != bool 1 OR
            lame_duck_experiment{cluster="platform"} != bool 1 OR
            label_replace(gmx_machine_maintenance, "experiment", "ndt.iupui", "", "") != bool 1
        )
        ) /
        count(
          probe_success{service="ndt7_ipv6"} unless on(machine)
            (gmx_machine_maintenance == 1 unless on(machine) kube_node_status_condition)
        )
      ) < 0.75
    for: 10m
    labels:
      repo: ops-tracker
      severity: ticket
      cluster: prometheus-federation
    annotations:
      summary: Less than 75% of ndt7_ipv6 experiments are online according to mlab-ns.
      description: https://github.com/m-lab/ops-tracker/wiki/Alerts-&-Troubleshooting#PlatformCluster_TooManyServersDown
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/T-t8rWwGz/mlab-ns-prometheus-queries

  # "neubot" mlab-ns query
  - alert: PlatformCluster_TooManyNeubotIPv4ServersDown
    expr: |
      (
        sum(
          min by (experiment, machine) (
            probe_success{service="neubot"} OR
            probe_success{service="neubot_tls"} OR
            label_replace(kube_node_spec_taint{cluster="platform", key="lame-duck"},
              "experiment", "neubot.mlab", "", "") != bool 1 OR
            label_replace(gmx_machine_maintenance, "experiment", "neubot.mlab", "", "") != bool 1
          )
        ) /
        count(
          probe_success{service="neubot"} unless on(machine)
            (gmx_machine_maintenance == 1 unless on(machine) kube_node_status_condition)
        )
      ) < 0.90
    for: 10m
    labels:
      repo: ops-tracker
      severity: ticket
      cluster: prometheus-federation
    annotations:
      summary: Less than 90% of neubot experiments are online according to mlab-ns.
      description: https://github.com/m-lab/ops-tracker/wiki/Alerts-&-Troubleshooting#PlatformCluster_TooManyServersDown
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/T-t8rWwGz/mlab-ns-prometheus-queries

  # "neubot_ipv6" mlab-ns query
  - alert: PlatformCluster_TooManyNeubotIPv6ServersDown
    expr: |
      (
        sum(
          min by (experiment, machine) (
            probe_success{service="neubot_ipv6"} OR
            probe_success{service="neubot_tls_ipv6"} OR
            label_replace(kube_node_spec_taint{cluster="platform", key="lame-duck"},
              "experiment", "neubot.mlab", "", "") != bool 1 OR
            label_replace(gmx_machine_maintenance, "experiment", "neubot.mlab", "", "") != bool 1
          )
        ) /
        count(
          probe_success{service="neubot_ipv6"} unless on(machine)
            (gmx_machine_maintenance == 1 unless on(machine) kube_node_status_condition)
        )
      ) < 0.75
    for: 10m
    labels:
      repo: ops-tracker
      severity: ticket
      cluster: prometheus-federation
    annotations:
      summary: Less than 75% of neubot_ipv6 experiments are online according to mlab-ns.
      description: https://github.com/m-lab/ops-tracker/wiki/Alerts-&-Troubleshooting#PlatformCluster_TooManyServersDown
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/T-t8rWwGz/mlab-ns-prometheus-queries


# Alert when the status of IPv6 for any or all machines at a site is down for
# too long, but *only* when the status of IPv4 is up for the same machine. If
# IPv4 is up, then IPv6 should be too.
  - alert: PlatformCluster_IPv6AtSiteDownForTooLong
    expr: |
      count by (site) (
        probe_success{service="ndt7_ipv6", ipv6="present"} == 0
          unless on(machine) (
            probe_success{service="ndt7"} == 0
            or gmx_machine_maintenance == 1
            or kube_node_spec_taint{cluster="platform", key="lame-duck"}
          )
      ) > 0
    for: 24h
    labels:
      repo: ops-tracker
      severity: ticket
      cluster: prometheus-federation
    annotations:
      summary: IPv6 at a site has been down for too long.
      description: https://github.com/m-lab/ops-tracker/wiki/Alerts-&-Troubleshooting#PlatformCluster_IPv6AtSiteDownForTooLong

# Check 5xx errors for the rate-limiter deployment, too.
  - alert: RateLimiterTooManyServerSideErrors
    expr: |
      sum_over_time(stackdriver_gae_app_appengine_googleapis_com_http_server_response_count{
        deployment="mlabns-stackdriver", module_id="rate-limiter", loading="false",
        response_code=~"5.."}[24h]) > 5
    for: 1m
    labels:
      repo: dev-tracker
      severity: ticket
      cluster: prometheus-federation
    annotations:
      summary: Server-side errors rate for rate-limiter is not zero.
      description: Stackdriver reports some of the HTTP requests to
        rate-limiter are returning a 5xx status code. Please check the
        rate-limiter logs to determine the cause.

# HTTP per-status response count is exported by stackdriver. If any of the
# services in the mlab-ns GAE project return > 1% 5xx errors, an alert
# should fire immediately.
  - alert: MlabNS_TooManyServiceErrors
    expr: |
      sum(stackdriver_gae_app_appengine_googleapis_com_http_server_response_count{
        deployment="mlabns-stackdriver", loading="false", response_code=~"5.."}) /
      sum(stackdriver_gae_app_appengine_googleapis_com_http_server_response_count{
        deployment="mlabns-stackdriver", loading="false"}) > 0.01
    for: 2m
    labels:
      repo: dev-tracker
      severity: page
      cluster: prometheus-federation
      page_project: mlab-oti
    annotations:
      summary: Stackdriver reports >1% errors from {{ $labels.module_id }}
        in mlab-ns project.
      description: Stackdriver reports more than 1% of the HTTP requests
        to {{ $labels.module_id }} are returning a 5xx status code. This is
        likely generated by GAE and means there is a persistent failure in
        the underlying service. Please check the GAE dashboard and logs to
        determine the cause.

# If container logs for a node are missing in Stackdriver for too long, then
# fire an alert, unless the node is in maintenance mode. This somewhat awkward
# query discovers cases where the stackdriver metric has been absent for too
# long, indicating a likely issue with the Vector pod on the node, causing it
# to fail to upload container logs to Stackdriver.
  - alert: PlatformCluster_ContainerLogsMissingInStackdriver
    expr: |
      kube_node_status_condition{condition="Ready", status="true", node=~"mlab[1-4].*"} == 1
        unless on(machine) sum_over_time(
          stackdriver_generic_node_logging_googleapis_com_user_platform_cluster_container_logs
        [1h])
        unless on(machine) gmx_machine_maintenance == 1
    for: 24h
    labels:
      repo: ops-tracker
      severity: ticket
      cluster: platform
    annotations:
      summary: Container logs for a node are missing in Stackdriver for more than 24h.
      description: https://github.com/m-lab/ops-tracker/wiki/Alerts-&-Troubleshooting#platformcluster_containerlogsmissinginstackdriver
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/rJ7z2Suik/k8s-site-overview

# One or more generic (non-experiment specific) mlab-ns metrics is missing.
# These are metrics that mlab-ns relies on to determine whether an experiment
# should receive production traffic, so we need to make sure that the metrics
# are always present.
  - alert: MlabNS_GenericMetricsMissing
    expr: |
      absent(node_filesystem_size_bytes{cluster="platform", mountpoint="/cache/data"})
        or absent(node_filesystem_avail_bytes{cluster="platform", mountpoint="/cache/data"})
        or absent(lame_duck_experiment{cluster="platform"})
    for: 30m
    labels:
      repo: ops-tracker
      severity: ticket
      cluster: prometheus-federation
    annotations:
      summary: A generic (non-experiment specific) mlab-ns metric is missing.
      description: >
        One of the metrics that mlab-ns relies on to determine if an experiment
        should receive production traffic is missing. Most of these metrics are
        scraped from the platform cluster. Be sure that the fedrated [scrape job
        for the platform-cluster is healthy][1]. If the GMX metric is missing,
        then make sure that gmx-server deployment healthy in the
        prometheus-federation cluster.
        [1]: https://prometheus.mlab-oti.measurementlab.net/targets#job-platform-cluster

# One or more NDT-specific metrics is missing. These are the NDT metrics that
# mlab-ns relies on to determine whether NDT is up and running, so we need to
# make sure that the metrics are always present. NOTE: mlab-ns additionally
# relies on the script_exporter metric 'script_success{service="ndt5_client"}', but
# alerting for that metric is already handled by the
# ScriptExporterMissingMetrics alert.
  - alert: MlabNS_NdtMetricsMissing
    expr: |
      absent(probe_success{service="ndt_raw"})
        or absent(probe_success{service="ndt_raw_ipv6"})
        or absent(probe_success{service="ndt_ssl"})
        or absent(probe_success{service="ndt_ssl_ipv6"})
    for: 30m
    labels:
      repo: ops-tracker
      severity: ticket
      cluster: prometheus-federation
    annotations:
      summary: A metric for an NDT service is missing.
      description: >
        If the blackbox_exporter service is running, then there may be a target
        configuration error. Check the target definitions in GCS and the [target
        status in Prometheus][1].
        [1]: https://prometheus.mlab-oti.measurementlab.net/targets#job-blackbox-targets

# One or more Neubot-specific metrics is missing. These are the Neubot metrics that
# mlab-ns relies on to determine whether Neubot is up and running, so we need to
# make sure that the metrics are always present.
  - alert: MlabNS_NeubotMetricsMissing
    expr: |
      absent(probe_success{service="neubot"})
        or absent(probe_success{service="neubot_ipv6"})
        or absent(probe_success{service="neubot_tls"})
        or absent(probe_success{service="neubot_tls_ipv6"})
    for: 30m
    labels:
      repo: ops-tracker
      severity: ticket
      cluster: prometheus-federation
    annotations:
      summary: A metric for a Neubot service is missing.
      description: >
        If the blackbox_exporter service is running, then there may be a target
        configuration error. Check the target definitions in GCS and the [target
        status in Prometheus][1].
        [1]: https://prometheus.mlab-oti.measurementlab.net/targets#job-blackbox-targets

# One or more of the backend services handled by the nginx proxy is down.
  - alert: Prometheus_NginxProxiedServiceDown
    expr: probe_success{job="nginx-proxied-services"} == 0
    for: 30m
    labels:
      repo: ops-tracker
      severity: ticket
      cluster: prometheus-federation
    annotations:
      summary: Backend services handled by the nginx proxy are down.
      description: Did the nginx k8s deployment or nginx-lb k8s service fail?
        Did the backend service or deployment fail in some way?

# GCS Transfer SLO
#
# We run daily GCS transfers between project buckets and to the public archive.
# See: https://github.com/m-lab/gcp-config/blob/master/daily-archive-transfers.yaml
#
# This alert enforces that daily transfers are working for all datatypes.
# Periodic delays are expected either to data volume or GCS Transfer service
# variance, so the expression must be firing for over 36h.
  - alert: GCSTransfers_ArchiveFilesDoNotMatchOrMissing
    expr: |
      sum(increase(gcs_archive_files_total{bucket="archive-mlab-oti"}[1d]) - ignoring(bucket)
         increase(gcs_archive_files_total{bucket="archive-measurement-lab"}[1d]) != 0)
      OR
      absent(gcs_archive_files_total)
    for: 36h
    labels:
      repo: dev-tracker
      severity: ticket
      cluster: prometheus-federation
    annotations:
      summary: GCS Transfers may not include all files.
      description: https://github.com/m-lab/ops-tracker/wiki/Alerts-&-Troubleshooting#GCSTransfers_ArchiveFilesDoNotMatchOrMissing

# Pipeline: GCS Archives Not Found in BigQuery
#
# This alert enforces that archives found in GCS are successfully parsed and
# found in BigQuery. Typically, we see ~11-12k archives per day. This alert
# fires when more than 1% are missing for 6hrs.
#
# Scheduling:
# * Both metrics depend on the GCS transfers completing successfully.
# * The bq_daily_archive_count metric is run every 3hrs by the bqx.
# * The gcs exporter is offset 14hr to align with the etl daily parsing.
  - alert: Pipeline_GCSArchivesMissingFromBigQuery
    expr: |
      abs(
         sum by(datatype) (bq_daily_archive_count {datatype="ndt7"}) - ignoring(experiment)
         (experiment_datatype:gcs_archive_files:increase24h{experiment="ndt", datatype="ndt7"} offset 14h)
      ) > 110
    for: 6h
    labels:
      repo: dev-tracker
      severity: ticket
      cluster: prometheus-federation
    annotations:
      summary: Not all GCS archives found in BQ tables for ndt7.
      description: https://github.com/m-lab/ops-tracker/wiki/Alerts-&-Troubleshooting#pipeline_gcsarchivesmissingfrombigquery


# Too many NDT S2C tests on a node are being impacted by switch discards.
  - alert: DataQuality_TooManyNdtS2cTestsWithDiscards
    expr: (bq_ndt_s2c_with_discards / bq_ndt_s2c_total) > 0.05
    for: 10m
    labels:
      repo: ops-tracker
      severity: ticket
      cluster: prometheus-federation
    annotations:
      summary: More than 5% of NDT tests in a 24h period were affected by discards.
      description: Too many NDT S2C tests in a 24h period (BQ partition_date)
        were affected by switch discards on {{ $labels.node }}. The switch may
        be overloaded, or TCP pacing may be misconfigured on the node.
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/SuqnZ6Hiz/ops-switch-overview

# The node_exporter running on eb.measurementlab.net is down.
  - alert: NodeExporterOnEbDownOrMissing
    expr: |
      up{job="eb-node-exporter"} == 0
        or absent(up{job="eb-node-exporter"})
    for: 10m
    labels:
      repo: ops-tracker
      severity: ticket
      cluster: prometheus-federation
    annotations:
      summary: The node_exporter running on eb.measurementlab.net is down.
      description: Login to EB to see if it is in fact crashed. If so, look
        through the logs.

# DataProcessing_GardenerDownOrMissing fires when the etl-gardener-universal is down or absent.
# TODO: enable annotations to ignore some container ports, and simplify this query.
# https://github.com/m-lab/prometheus-support/issues/48
  - alert: DataProcessing_GardenerDownOrMissing
    expr: |
      up{cluster="data-processing", container="etl-gardener",instance=~".*:9090"} == 0
        or absent(up{cluster="data-processing", container="etl-gardener",instance=~".*:9090"})
    for: 10m
    labels:
      repo: dev-tracker
      severity: ticket
      cluster: prometheus-federation
    annotations:
      summary: The Universal Gardener instance is down or missing.
      description: Gardener runs in the "data-processing" cluster.
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/eBbUW6oik/pipeline-gardener?var-project=mlab-oti&var-pipelineDatasource=Data%20Processing%20(mlab-oti)

# DataProcessing_ParserDownOrMissing fires when the k8s etl-parser is down or absent.
# TODO: enable annotations to ignore some container ports, and simplify this query.
# https://github.com/m-lab/prometheus-support/issues/48
  - alert: UniversalParserDownOrMissing
    expr: |
      up{cluster="data-processing", container="etl-parser",instance=~".*:9090"} == 0
        or absent(up{cluster="data-processing", container="etl-parser",instance=~".*:9090"})
    for: 10m
    labels:
      repo: dev-tracker
      severity: ticket
      cluster: prometheus-federation
    annotations:
      summary: ETL Parser instance is down or missing {{ $labels.instance }}.
      description: These parsers run in the "data-processing" cluster.
      # TODO #736 - update this when k8s dashboard is deployed to production.
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/eBbUW6oik/pipeline-gardener?var-project=mlab-oti&var-pipelineDatasource=Data%20Processing%20(mlab-oti)

# GardenerDailyParsingIsBehind fires when daily processing has fallen behind
# There should always be 3 or 4 of the past four dates available, most of time.
  - alert: GardenerDailyParsingIsBehind
    expr: |
      bq_gardener_daily_done_last_4_days < 3
    for: 48h
    labels:
      repo: dev-tracker
      severity: ticket
      cluster: prometheus-federation
    annotations:
      summary: Daily processing for {{ $labels.datatype }} has fallen behind.
      description: Gardener runs in the "data-processing" cluster.
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/q4MrNzh7k/pipeline-slis?orgId=1&var-project=mlab-oti&var-PrometheusDS=Prometheus%20(mlab-oti)&var-Gardener2_DS=Data%20Processing%20(mlab-oti)

# GardenerHistoricalThroughputIsStalled fires when historical reprocessing for
# datatypes under processing by the v2 pipeline falls below 1 date / day.
# The bq_gardener_historical_throughput metric is under the bigquery exporter 3h
# deployment, so the timeout for this alert is 4 hours.
  - alert: GardenerHistoricalThroughputIsStalled
    expr: |
      increase(gardener_jobs_total{status="success", daily="false"}[1d]) > 0
        UNLESS ON(datatype) bq_gardener_historical_throughput > 0
    for: 4h
    labels:
      repo: dev-tracker
      severity: ticket
      cluster: prometheus-federation
    annotations:
      summary: Gardener historical throughput below 1 for {{ $labels.datatype }}.
      description: Gardener runs in the "data-processing" cluster.
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/q4MrNzh7k/pipeline-slis?orgId=1&var-project=mlab-oti&var-PrometheusDS=Prometheus%20(mlab-oti)&var-Gardener2_DS=Data%20Processing%20(mlab-oti)

# GardenerConfigDatatypeMissingInGardenerHistoricalThroughputQuery fires when
# a datatype exists in the Gardener config
# but not in the bq_gardener_historical_throughput query.
  - alert: GardenerConfigDatatypeMissingInGardenerHistoricalThroughputQuery
    expr: |
      gardener_config_datatypes UNLESS ON(datatype) bq_gardener_historical_throughput
    for: 4h
    labels:
      repo: dev-tracker
      severity: ticket
      cluster: prometheus-federation
    annotations:
      summary: Datatype {{ $labels.datatype }} in Gardener config missing in
        bq_gardener_historical_throughput query.
      description: https://github.com/m-lab/ops-tracker/wiki/Alerts-&-Troubleshooting#gardenerconfigdatatypemissingingardenerhistoricalthroughputquery
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/q4MrNzh7k/pipeline-slis?orgId=1&var-project=mlab-oti&var-PrometheusDS=Prometheus%20(mlab-oti)&var-Gardener2_DS=Data%20Processing%20(mlab-oti)

# GardenerFailureRateTooHigh fires when the number of failed Gardener jobs
# in the last day rises above 1%.
  - alert: GardenerFailureRateTooHighOrMissing
    expr: (sum(rate(gardener_jobs_total{status!="success"}[1d])) by (datatype) /
      sum(rate(gardener_jobs_total[1d])) by (datatype)) > 0.01
    for: 10m
    labels:
      repo: dev-tracker
      severity: ticket
      cluster: prometheus-federation
    annotations:
      summary: Gardener job failure rate above 1% for {{ $labels.datatype }}.
      description: Gardener runs in the "data-processing" cluster.
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/q4MrNzh7k/pipeline-slis?orgId=1&var-project=mlab-oti&var-PrometheusDS=Prometheus%20(mlab-oti)&var-Gardener2_DS=Data%20Processing%20(mlab-oti)

# GardenerJobsTotalMissing fires when the number of total jobs is not reported.
  - alert: GardenerJobsTotalMissing
    expr: absent(gardener_jobs_total)
    for: 30m
    labels:
      repo: dev-tracker
      severity: ticket
      cluster: prometheus-federation
    annotations:
      summary: gardener_jobs_total metric is missing.
      description: Gardener runs in the "data-processing" cluster. Is the
        gardener running? If it is running has processing slowed below 1 job every
        30min?
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/q4MrNzh7k/pipeline-slis?orgId=1&var-project=mlab-oti&var-PrometheusDS=Prometheus%20(mlab-oti)&var-Gardener2_DS=Data%20Processing%20(mlab-oti)

# ParserFailureRateTooHighOrMissing fires when the number of failed parser tasks
# in the last hour rises above 1% or the number of total tasks is not reported.
  - alert: ParserFailureRateTooHighOrMissing
    expr: (sum(rate(etl_task_total{status!="OK", cluster="data-processing"}[1h])) by (table) /
      sum(rate(etl_task_total{cluster="data-processing"}[1h])) by (table)) > 0.01
      OR absent(etl_task_total{cluster="data-processing"})
    for: 10m
    labels:
      repo: dev-tracker
      severity: ticket
      cluster: prometheus-federation
    annotations:
      summary: Parser task failure rate above 1% or missing for {{ $labels.table }}.
      description: Parsers filtered by cluster="data-processing".
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/q4MrNzh7k/pipeline-slis?orgId=1&var-project=mlab-oti&var-PrometheusDS=Prometheus%20(mlab-oti)&var-Gardener2_DS=Data%20Processing%20(mlab-oti)

# ETL_ParserPanicNonZero fires when an ETL parser panics. The number of panics
# may increase due to a transient issue or a short-lived trigger for a parser
# bug. The alert will fire as long as the rate is above zero for more than 5
# minutes. So, it's possible for false-negatives in response to isolated events,
# but it also allows the alert to stop firing without a redeploy when the event
# is short-lived.
  - alert: ETL_ParserPanicNonZero
    expr: irate(etl_panic_count[4m]) > 0
    for: 5m
    labels:
      repo: dev-tracker
      severity: ticket
      cluster: prometheus-federation
    annotations:
      summary: An ETL parser panicked {{ $labels.instance }}
      description: Bugs cause panics. This bug should be fixed. Parsers run in
        AppEngine. Check logs to see the panic stack trace. Identify the archive
        that led to the panic (logs or TaskQueue tasks with many retries). Fix
        the bug or create a new issue describing the failure and linking to the
        triggering archive.

# NDT_AsnAnnotationRatioTooLowOrMissing fires when the client annotations on NDT
# tests appear to have too many failures or the bq_ndt_annotation_* metrics
# disappear.
  - alert: DataPipeline_AsnAnnotationRatioTooLowOrMissing
    expr: |
      bq_annotation_asn_success / bq_annotation_total < 0.98
        or absent(bq_annotation_asn_success / bq_annotation_total)
    for: 60m
    labels:
      repo: dev-tracker
      severity: ticket
      cluster: prometheus-federation
    annotations:
      summary: Too many NDT tests are missing asn annotations!
      description: The annotator runs in AppEngine. Check logs and recent
        deployments. The daily and batch parsers may also be affected.

# NDT_GeoAnnotationRatioTooLowOrMissing fires when the client annotations on NDT
# tests (or sidecars) appear to have too many failures or the bq_annotation_* metrics
# disappear.
  - alert: DataPipeline_GeoAnnotationRatioTooLowOrMissing
    expr: |
      bq_annotation_geo_success / bq_annotation_total < 0.98
        or absent(bq_annotation_geo_success / bq_annotation_total)
    for: 60m
    labels:
      repo: dev-tracker
      severity: ticket
      cluster: prometheus-federation
    annotations:
      summary: Too many NDT tests are missing geolocation annotations!
      description: The uuid-annotator generates annotations and they are joined with
        tests data by gardener JOINs. Check logs and recent deployments of these
        components.

# Rebot_MissingMetrics fires when no metrics can be collected for rebot for
# the past 10 minutes.
  - alert: Rebot_MissingMetrics
    expr: absent(up{run='rebot'})
    for: 10m
    labels:
      repo: ops-tracker
      severity: ticket
      cluster: prometheus-federation
    annotations:
      summary: The rebot instance running on eb.measurementlab.net is down.
      description: Metrics for rebot cannot be collected. The instance is down
        or not reachable. Check that the rebot daemon on eb.measurementlab.net
        is running and port 9999 is reachable.

# Cloud_TooManyAppEngineVersions fires when there are too many AE versions to deploy
# full ETL deployment successfully.
  - alert: Cloud_TooManyAppEngineVersions
    expr: sum(gcp_aeflex_versions{container="service-discovery"}) / 210 > 0.95
    for: 10m
    labels:
      repo: dev-tracker
      severity: ticket
      cluster: prometheus-federation
    annotations:
      summary: Too many AppEngine versions for the project.
      description: https://github.com/m-lab/ops-tracker/wiki/Alerts-&-Troubleshooting#cloud_toomanyappengineversions
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/Yor77urmk/cloud-appengine

# Cloud_TooManyInactiveInstances checks that all running instances are active
# rather than wasting resources.
  - alert: Cloud_TooManyInactiveInstances
    expr: |
      sum (gcp_aeflex_instances{container="service-discovery", active="false"})
        / sum (gcp_aeflex_instances{container="service-discovery"}) > 0.25
    for: 30m
    labels:
      repo: dev-tracker
      severity: ticket
      cluster: prometheus-federation
    annotations:
      summary: Too many AppEngine instances are running but not serving.
      description: Failed or interrupted deployments do not shutdown instances
        completely. Over time, a project accumulates inactive instances that
        waste resources. Please delete them or create a tool that does it
        automatically.
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/Yor77urmk/cloud-appengine

# Boot_MachineFailedToBoot a machine has failed to boot for more than a day.
  - alert: Boot_MachineFailedToBoot
    expr: epoxy_last_success < epoxy_last_boot
      unless on(machine) (
        gmx_machine_maintenance == 1 or
        label_replace(
          kube_node_spec_taint{cluster="platform", key="lame-duck"},
          "machine", "$1", "node", "(.*)"
        )
      )
    for: 24h
    labels:
      repo: dev-tracker
      severity: ticket
      cluster: prometheus-federation
    annotations:
      summary: A machine booted recently, but has not reported success yet.
      description: Failing to report success may be due to configuration or
        runtime failures. Check the machine console messages via the DRAC.
        Check the k8s setup logs on the machine (/tmp/setup_k8s.log). Check the
        epoxy-boot-api VM logs in Stackdriver.
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/JaSyFC9mk/boot-nodes

# Boot_EpoxyServerOfflineOrMissing fires when the epoxy boot api server is
# offline or misconfigured.
  - alert: Boot_EpoxyServerOfflineOrMissing
    expr: up{job="epoxy-boot-api"} == 0 or absent(up{job="epoxy-boot-api"})
    for: 30m
    labels:
      repo: dev-tracker
      severity: ticket
      cluster: prometheus-federation
    annotations:
      summary: The ePoxy boot api server is down or missing. Nodes cannot boot!
      description: Has the configuration been removed from prometheus? Is the
        service running? Check for the epoxy-boot-api-* VM. Check for recent
        failed Cloud Builder builds caused by recent commits; check those logs.
        Check the VM service logs from Stackdriver.
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/t_juk39ik/boot-epoxy-server

# FederationScrapeJobFailing_PlatformCluster indicates that scraping for the
# k8s platform cluster is failing.  The threshold for being down is 4m, such
# that this alert will fire slightly before the alerts for missing individual
# DaemonSet metrics (and can inhibit those).  Also, ~4 scrape cycles is enough
# time for this to be down before someone knows about it.
  - alert: FederationScrapeJobFailing_PlatformCluster
    expr: up{job="platform-cluster"} == 0
    for: 10m
    labels:
      repo: ops-tracker
      severity: ticket
      cluster: prometheus-federation
    annotations:
      summary: Federation scraping of the k8s platform cluster is down.
      description: Scraping of Prometheus on the platform cluster is down.
        This could be that Prometheus in the platform cluster is down, or it
        could be that the prometheus-federation scrape job is failing for some
        reason unrelated to the platform clustser. Verify that the node where
        Prometheus should be running in the platform cluster is healthy
        (`kubectl get nodes --selector run=prometheus-server`). Verify the
        Prometheus deployment is running using - `kubectl get pods`.  Check pod
        logs.  Check the GCP console for the VM for messages or status. Check
        the status of the target in the prometheus-federation Web console.

# FederationScrapeJobMissing_PlatformCluster indicates that the job for
# scraping the k8s platform cluster does not exist.
  - alert: FederationScrapeJobMissing_PlatformCluster
    expr: absent(up{job="platform-cluster"})
    for: 10m
    labels:
      repo: ops-tracker
      severity: ticket
      cluster: prometheus-federation
    annotations:
      summary: Federation scraping of the k8s platform cluster is missing.
      description: A scrape job for the k8s platform cluster is missing. This
        is almost surely the result of a misconfiguration of Prometheus. Check
        the running prometheus configuration to be sure that there is actually
        a job for scraping the k8s platform cluster.

  # If any node is NotReady for too long, fire an alert, unless the node is in
  # lame-duck or GMX maintenance mode, or if the switch isn't pingable. Since
  # individual ICMP probes may be unreliable, we determine that a switch is
  # "down" if more than 75% of all ICMP probes over the last hour fail.
  #
  # NOTE: This alert should rightly exist in the platform cluster Prometheus
  # instances but it uses a metric from the blackbox-exporter, so for now it
  # remains here. TODO(kinkade): determine how to safely move this alert to the
  # platform cluster.
  - alert: PlatformCluster_NodeNotReady
    expr: |
      label_replace(
        kube_node_status_condition{cluster="platform", condition="Ready", status=~"(false|unknown)"} == 1,
        "site", "$1", "node", "mlab[1-4]-([a-z]{3}[0-9t]{2}).*"
      )
        unless on(node) (
          kube_node_spec_taint{cluster="platform", key="lame-duck"} or
          gmx_machine_maintenance == 1
        )
        unless on(site) (
          sum_over_time(probe_success{module="icmp"}[1h]) < 15
        )
    for: 1d
    labels:
      repo: ops-tracker
      severity: ticket
      cluster: prometheus-federation
    annotations:
      summary: A node has had a NotReady condition for too long.
      description: A node has had a NotReady condition for too long. Generally
        this is caused when the kubelet on the node is unable to communicate
        with the API. Is the machine booted? Does the machine have network
        connectivity? Check the status of the kubelet on the machine. Look at
        the kubelet logs (journalctl -u kubelet).
      dashboard: https://grafana.mlab-staging.measurementlab.net/d/rJ7z2Suik/k8s-site-overview

# BMC alerts
# The Reboot API performs an E2E connection test on a BMC every time its
# /v1/e2e endpoint is scraped, and reports failure via this metric.
  - alert: BMC_E2ETestDownOrMissing
    expr: up{job="bmc-targets"} == 0 or absent(up{job="bmc-targets"})
    for: 10m
    labels:
      repo: ops-tracker
      severity: ticket
      cluster: prometheus-federation
    annotations:
      summary: reboot-service is down on missing.
      description: >
        Metrics from the e2e test are missing. Please check the reboot-service
        deployment on GKE, and that the /v1/e2e endpoint is returning a status
        code = 200.

  - alert: BMC_E2eTestFailed
    expr: |
        reboot_e2e_success == 0
        unless on(machine) gmx_machine_maintenance == 1
    for: 1d
    labels:
      repo: ops-tracker
      severity: ticket
      cluster: prometheus-federation
    annotations:
      summary: E2E testing is failing to a DRAC
      description: https://github.com/m-lab/ops-tracker/wiki/Alerts-&-Troubleshooting#bmc_e2etestfailed

# switch-monitoring alerts.
# The switch-monitoring tool checks that the switch configurations match
# what is currently stored in a GCS bucket and warns us if that's not the case.
  - alert: SwitchMonitoring_DownOrMissing
    expr: up{job="switch-monitoring-targets"} == 0 or absent(up{job="switch-monitoring-targets"})
    for: 10m
    labels:
      repo: ops-tracker
      severity: ticket
      cluster: prometheus-federation
    annotations:
      summary: The switch-monitoring service is down or missing.
      description: Check the status of the switch-monitoring deployment on GKE.
        Make sure the switch-monitoring service is reachable and /v1/check is
        returning 200.

  - alert: SwitchMonitoring_ConfigMismatch
    expr: |
      switch_monitoring_config_match{status="config_mismatch"} == 1
      unless on(site) gmx_site_maintenance == 1
    for: 2m
    labels:
      repo: ops-tracker
      severity: ticket
      cluster: prometheus-federation
    annotations:
      summary: The configuration for {{ $labels.target }} does not match.
      description: >
        The configuration on this switch does not match the version that's
        currently stored in the switch-config GCS bucket. Please inspect
        switch-monitoring's logs for a full diff and re-apply the standard
        configuration by using genconfig.py and ansible

  - alert: SwitchMonitoring_ConfigNotFoundGCS
    expr: |
      switch_monitoring_config_match{status="config_not_found_gcs"} == 1
      unless on(site) gmx_site_maintenance == 1
    for: 2m
    labels:
      repo: ops-tracker
      severity: ticket
      cluster: prometheus-federation
    annotations:
      summary: The config file for {{ $labels.target }} does not exist on GCS.
      description: >
        There is no configuration file for this switch on GCS. It should be
        generated when deploying the switch-config repository, provided that
        this site existed on siteinfo at deployment time. Please check that the
        site has been added to siteinfo and trigger a switch-config deployment.

  # A switch can be offline for 24hrs before a corresponding SwitchDownAtSite
  # alert fires. This sets a lower bound for how long the switch can be unreachable
  # before we care about being notified. This alert's "for" value should always be
  # significantly longer than that.
  - alert: SwitchMonitoring_ConfigNotFoundSwitch
    expr: |
      switch_monitoring_config_match{status="config_not_found_switch"} == 1
      unless on(site) gmx_site_maintenance == 1
    for: 5d
    labels:
      repo: ops-tracker
      severity: ticket
      cluster: prometheus-federation
    annotations:
      summary: Could not retrieve the configuration for {{ $labels.target }}.
      description: >
        Connection to this switch failed, or the configuration could not be
        retrieved due to permission issues. Please check that the switch is
        currently reachable and the switch-monitoring user exists and has the
        right privileges.

# PrometheusPersistentDiskTooFull fires when the persistent
# disk mounted on the Prometheus VM gets too full (less than 5% free).
# TODO(cristinaleon): Add an alerts configuration to the data-processing instance
# that includes its own version of this alert.
  - alert: PrometheusPersistentDiskTooFull
    expr: |
      ((kubelet_volume_stats_available_bytes{cluster="data-processing", persistentvolumeclaim="auto-prometheus-ssd0"}
        / kubelet_volume_stats_capacity_bytes) < 0.05) OR
      ((kubelet_volume_stats_available_bytes{cluster="prometheus-federation", persistentvolumeclaim="auto-prometheus-disk0"}
        / kubelet_volume_stats_capacity_bytes) < 0.05)
    for: 1m
    labels:
      repo: ops-tracker
      severity: ticket
      cluster: "{{ $labels.cluster }}"
    annotations:
      summary: The Prometheus persistent disk has less than 5% free space.
      description: https://github.com/m-lab/ops-tracker/wiki/Alerts-&-Troubleshooting#prometheuspersistentdisktoofull
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/sVklmeHik/prometheus-self-monitoring?orgId=1&var-datasource=default


# Pipeline_DailyDiscussAccessZeroOrMissing fires when queries run with the same
# permissions as a member of the discuss@ mailing list return zero results
# or fail entirely.
  - alert: Pipeline_DailyDiscussAccessZeroOrMissing
    expr: |
      bq_daily_discuss_total == 0 OR absent(bq_daily_discuss_total)
    for: 10m
    labels:
      repo: dev-tracker
      severity: ticket
      cluster: prometheus-federation
    annotations:
      summary: Queries run by discuss@ users may be broken or return zero results.
      description: https://github.com/m-lab/ops-tracker/wiki/Alerts-&-Troubleshooting#pipeline_dailydiscussaccesszeroormissing
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/UTgnK-jMz/pipeline-overview?orgId=1&refresh=5m
