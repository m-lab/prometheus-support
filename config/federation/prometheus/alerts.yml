# M-Lab alert configuration.
#
# See https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules
# for more information on the alerting rules syntax.
#
## Notes about delay.
#
# There are inherent delays in the monitoring and alert pipeline. When
# designing alerts, remember that each step includes some delay:
#
#  * the time between reality changing and monitoring observing the change.
#  * the time between the observed change and the delay defined by the alert.
#  * the time between a firing alert and the "group_wait" time in alertmanager.
#  * the time between sending a notification and the notification arriving.
#
# For example, for a 60s collection period, a 2m alert delay, a 30s group_wait,
# and a 15s-3m SMS delivery delay, the time between reality changing and a
# human knowing could be over six minutes.
#
# Your job is to balance false-positives with responsiveness when necessary.

groups:
- name: alerts.yml
  rules:
# ClusterDown: when any of the federated prometheus k8s clusters is down for
# three sample periods, then raise an alert.
  - alert: ClusterDown
    expr: up{job="federation-targets"} == 0
    for: 10m
    labels:
      repo: dev-tracker
      severity: page
    annotations:
      summary: Instance {{ $labels.instance }} down
      description: '{{ $labels.instance }} of job {{ $labels.job }} has been down
        for more than 10 minutes.'

##
## SLOs
##
#
# SwitchSLO
#
# A switch at a site has been down for too long and we need to contact the site
# host or transit provider to investigate. If SNMP scraping *and* pings are both
# failing for a certain period, then this is probaby a reasonable stand-in as an
# "up"/"aliveness" check.
  - alert: SwitchDownAtSite
    expr: |
      up{job="snmp-targets",site!~".*t$"} == 0
        and on(site) probe_success{instance=~"s1.*",module="icmp"} == 0
          unless on(site) gmx_site_maintenance == 1
    for: 1d
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: The switch at {{ $labels.site }} has been unreachable for too long.
      description: >
        The SNMP exporter cannot scrape new metrics from the switch. The issue
        could be with the switch itself, or with the transit provider.
      dashboard: 'https://grafana.mlab-oti.measurementlab.net/d/SuqnZ6Hiz/?orgId=1&var-site_name={{$labels.site}}'

# DownloaderIsFailingToUpdate: The downloader hasn't successfully retrieved the files in
# at least 21 hours, meaning that at least the last two download attempts have failed.
  - alert: DownloaderIsFailingToUpdate
    expr: time() - downloader_last_success_time_seconds > (21 * 60 * 60)
    for: 5m
    labels:
      repo: dev-tracker
      severity: ticket
    annotations:
      summary: Neither of the last two attempts to download the maxmind or
        routeviews feeds were successful.
      description: Check for errors with the downloader service on grafana with
        the downloader_error_count metric, or check the stackdriver logs for
        the downloader cluster.
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/ZGuYht1mk/

# DownloaderNotRunning: The downloader cluster crashed and not running at all.
  - alert: DownloaderDownOrMissing
    expr: up{container="downloader"} == 0 or absent(up{container="downloader"})
    for: 10m
    labels:
      repo: dev-tracker
      severity: ticket
    annotations:
      summary: The downloader for maxmind/routeviews feeds is down or missing.
      description: Check the status of Kubernetes clusters on each M-Lab GCP
        project. Look at the travis deployment history for m-lab/downloader.

# Prometheus is unable to get data from the snmp_exporter service.
  - alert: SnmpExporterDownOrMissing
    expr: up{container="snmp-exporter"} == 0 or absent(up{container="snmp-exporter"})
    for: 10m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: The snmp_exporter service is down or missing.
      description: Check the status of Kubernetes clusters on each M-Lab GCP
        project. Look at the travis deployment history for
        m-lab/prometheus-support.

# Some SNMP metrics are missing from Prometheus. These should always be present.
# The wait period shouuld be longer than that for the SnmpExporterDownOrMissing
# alert.
  - alert: SnmpExporterMissingMetrics
    expr: absent(ifHCOutOctets)
    for: 30m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: Expected SNMP metrics are missing from Prometheus!
      description: >
        If the snmp_exporter service is running, then there may be a
        target configuration error. Check the target definitions in GCS[1] and
        the target status in Prometheus[2].

        [1]: https://console.cloud.google.com/storage/browser/operator-mlab-oti/prometheus/snmp-targets
        [2]: https://prometheus.mlab-oti.measurementlab.net/targets
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/SuqnZ6Hiz/

# Scraping SNMP metrics from a switch is failing.
  - alert: SnmpScrapingDownAtSite
    expr: |
      up{job="snmp-targets",site!~".*t$"} == 0
        and on(site) probe_success{instance=~"s1.*",module="icmp"} == 1
        unless on(site) gmx_site_maintenance == 1
    for: 2h
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: Prometheus is unable to scrape SNMP metrics from a switch.
      description: >
        Maybe the switch is down? Is the snmp_exporter using the right community
        string? Look in switch-details.json in the m-lab/switch-config repo. Is
        the IP of the snmp_exporter VM in GCE whitelisted on the switch?
      dashboard: 'https://grafana.mlab-oti.measurementlab.net/d/SuqnZ6Hiz/?orgId=1&var-site_name={{$labels.site}}'

# More than 20% of the reachable switches are up but not providing SNMP metrics.
  - alert: TooManySnmpMetricsMissing
    expr: |
      count(up{job="snmp-targets",site!~".*t$"} == 0 and on(site)
      probe_success{instance=~"s1.*",module="icmp"} == 1) /
      count(up{job="snmp-targets",site!~".*t$"}) > 0.2
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: More than 20% of reachable switches are not providing SNMP metrics.
      description: >
        The switches are pingable but SNMP scraping is not working.
        Is the snmp_exporter using the right community strings? Look in
        switch-details.json in the m-lab/switch-config repo. Is the IP of the
        snmp_exporter VM in GCE whitelisted on the switch?
      dashboard: 'https://grafana.mlab-oti.measurementlab.net/d/SuqnZ6Hiz'

# Prometheus is unable to get data from the script_exporter service.
  - alert: ScriptExporterDownOrMissing
    expr: up{job="script-exporter"} == 0 or absent(up{job="script-exporter"})
    for: 10m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: The script_exporter service is down on missing.
      description: >
        The script_exporter service runs in a Docker container on a GCE VM
        named 'script-exporter' in each M-Lab GCP project. For deployment
        details and troubleshooting, you can usually figure out the issue by
        looking through the Travis-CI build logs[1]. You can also look for
        hints in the GCP console for the GCE instance, or by SSHing to the
        instance itself.
        [1]: https://travis-ci.org/m-lab/script-exporter-support

# Some script_exporter metrics are missing from Prometheus. These should always
# be present. The wait period should be longer than that for the
# ScriptExporterDownOrMissing alert.
  - alert: ScriptExporterMissingMetrics
    expr: |
      absent(script_success{service="ndt_e2e"})
    for: 30m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: Expected script_exporter metrics are missing from Prometheus!
      description: >
        If the script_exporter service is running, then there may be a target
        configuration error. Check the target definitions in GCS[1] and the target
        status in Prometheus[2].

        [1]: https://console.cloud.google.com/storage/browser/operator-mlab-oti/prometheus/script-targets
        [2]: http://prometheus.mlab-oti.measurementlab.net:9090/targets
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/SuqnZ6Hiz/

# Prometheus is unable to get data from the blackbox_exporter service for IPv4
# probes. The service is down, or the metric is missing.
  - alert: BlackboxExporterIpv4DownOrMissing
    expr: |
      up{job="blackbox-exporter-ipv4"} == 0
        or absent(up{job="blackbox-exporter-ipv4"})
    for: 10m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: The blackbox_exporter service is down for IPv4 probes.
      description: Check the status of the blackbox-server pod in the
        prometheus-federation cluster on each M-Lab GCP project.

# Prometheus is unable to get data from the blackbox_exporter service for IPv6
# probes. The service is down, or the metric is missing.
  - alert: BlackboxExporterIpv6DownOrMissing
    expr: up{job="blackbox-exporter-ipv6"} == 0 or absent(up{job="blackbox-exporter-ipv6"})
    for: 10m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: The blackbox_exporter service is down or missing for IPv6 probes.
      description: The blackbox_exporter for IPv6 checks runs in a Linode VM.
        Make sure the VM is up and running. If it is, check the status of the
        BBE container running in the VM. Domains for VMs are like
        blackbox-exporter-ipv6.<project>.measurementlab.net.

# Unable to scrape the Github Maintenance exporter or the job is missing.
  - alert: GithubMaintenanceExporterDownOrMissing
    expr: |
      up{job="nginx-proxied-services", service="gmx"} == 0
        or absent(up{job="nginx-proxied-services", service="gmx"})
    for: 10m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: Scraping of the Github Maintenance Exporter is failing or missing.
      description: >
        Scraping of the Github Maintenance Exporter is failing or missing.
        Check that the gmx-server deployment is healthy and that a pod for it
        exists. Check the status of the pod for errors. Check the logs of the
        pod for errors.  Check the reason that the scrape failed[1].
        [1]: https://prometheus.mlab-oti.measurementlab.net/targets#job-nginx-proxied-services

  # Blackbox exporter probes to a machine are succeeding to a node that
  # kubernetes does not know about (not joined to the k8s cluster).
  - alert: MachineRunningWithoutK8sNode
    expr: probe_success{service="ndt_ssl"} == 1 unless on(machine) kube_node_status_condition
    for: 1h
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: A machine is running that is not part of the k8s cluster.
      description: >
        Blackbox exporter probes (for ndt_ssl) are succeeding to a node that
        kubernetes does not know about. This can happen when a machine gets
        segmented from the network, then gets manually deleted from kubernetes,
        then at some point the machine has its network connectivity restored.
        In this situation all of the containers on the machine are still
        running, but the node is no longer known to kubernetes. It becomes like
        a zombie node, possibly continuing to upload data to GCS and serve
        experiment tests. The simple fix is to reboot the zombie node.

## mlab-ns queries.
#
# The following alerts are based on the exact queries that mlab-ns runs to
# determine the state of NDT services.
# https://github.com/m-lab/mlab-ns/blob/master/server/mlabns/util/prometheus_status.py
#
# TODO(kinkade): The rewrite of mlab-ns should export these types of metrics
# such that we con't have to duplicate the queries here in the alerts.

  # "ndt" mlab-ns query
  - alert: TooManyNdtIpv4ServersDown
    expr: |
      (
        sum(
          min by (experiment, machine) (
            probe_success{service="ndt_raw"} OR
            script_success{service="ndt_e2e"} OR
            label_replace(((node_filesystem_size_bytes{cluster="platform-cluster", mountpoint="/cache/data"} -
              node_filesystem_free_bytes{cluster="platform-cluster", mountpoint="/cache/data"}) /
                node_filesystem_size_bytes{cluster="platform-cluster", mountpoint="/cache/data"}),
                "experiment", "ndt.iupui", "", "") < bool 0.95 OR
            kube_node_spec_taint{cluster="platform-cluster", key="lame-duck"} != bool 1 OR
            label_replace(gmx_machine_maintenance, "experiment", "ndt.iupui", "", "") != bool 1
          )
        ) / count(probe_success{service="ndt_raw"})
      ) < 0.90
    for: 10m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: Less than 90% of ndt experiments are online according to mlab-ns.
      description: Make sure that the kubernetes ndt DaemonSet is healthy. Also
        consider that this could be a false positive because of bad or broken
        monitoring.
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/tZHLFQRZk/k8s-workload-overview

  # "ndt_ipv6" mlab-ns query
  - alert: TooManyNdtIpv6ServersDown
    expr: |
      (
        sum(
          min by (experiment, machine) (
            probe_success{service="ndt_raw_ipv6"} OR
            script_success{service="ndt_e2e"} OR
            label_replace(((node_filesystem_size_bytes{cluster="platform-cluster", mountpoint="/cache/data"} -
              node_filesystem_free_bytes{cluster="platform-cluster", mountpoint="/cache/data"}) /
                node_filesystem_size_bytes{cluster="platform-cluster", mountpoint="/cache/data"}),
                "experiment", "ndt.iupui", "", "") < bool 0.95 OR
            kube_node_spec_taint{cluster="platform-cluster", key="lame-duck"} != bool 1 OR
            label_replace(gmx_machine_maintenance, "experiment", "ndt.iupui", "", "") != bool 1
          )
        ) / count(probe_success{service="ndt_raw_ipv6"})
      ) < 0.75
    for: 10m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: Less than 75% of ndt_ipv6 experiments are online according to mlab-ns.
      description: Make sure that the kubernetes ndt DaemonSet is healthy. Also
        consider that this could be a false positive because of bad or broken
        monitoring.
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/tZHLFQRZk/k8s-workload-overview

  # "ndt_ssl" mlab-ns query
  - alert: TooManyNdtSslIpv4ServersDown
    expr: |
      (
        sum(
          min by (experiment, machine) (
            probe_success{service="ndt_ssl"} OR
            script_success{service="ndt_e2e"} OR
            label_replace(((node_filesystem_size_bytes{cluster="platform-cluster", mountpoint="/cache/data"} -
              node_filesystem_free_bytes{cluster="platform-cluster", mountpoint="/cache/data"}) /
                node_filesystem_size_bytes{cluster="platform-cluster", mountpoint="/cache/data"}),
                "experiment", "ndt.iupui", "", "") < bool 0.95 OR
            kube_node_spec_taint{cluster="platform-cluster", key="lame-duck"} != bool 1 OR
            label_replace(gmx_machine_maintenance, "experiment", "ndt.iupui", "", "") != bool 1
          )
        ) / count(probe_success{service="ndt_ssl"})
      ) < 0.50
    for: 10m
    labels:
      repo: ops-tracker
      severity: page
    annotations:
      summary: Less than 90% of ndt_ssl experiments are online according to mlab-ns.
      description: Make sure that the kubernetes ndt DaemonSet is healthy. Also
        consider that this could be a false positive because of bad or broken
        monitoring.
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/tZHLFQRZk/k8s-workload-overview

  - alert: TooManyNdtSslIpv6ServersDown
    expr: |
      (
        sum(
          min by (experiment, machine) (
            probe_success{service="ndt_ssl_ipv6"} OR
            script_success{service="ndt_e2e"} OR
            label_replace(((node_filesystem_size_bytes{cluster="platform-cluster", mountpoint="/cache/data"} -
              node_filesystem_free_bytes{cluster="platform-cluster", mountpoint="/cache/data"}) /
                node_filesystem_size_bytes{cluster="platform-cluster", mountpoint="/cache/data"}),
                "experiment", "ndt.iupui", "", "") < bool 0.95 OR
            kube_node_spec_taint{cluster="platform-cluster", key="lame-duck"} != bool 1 OR
            label_replace(gmx_machine_maintenance, "experiment", "ndt.iupui", "", "") != bool 1
          )
        ) / count(probe_success{service="ndt_ssl_ipv6"})
      ) < 0.75
    for: 10m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: Less than 75% of ndt_ssl_ipv6 experiments are online according to mlab-ns.
      description: Make sure that the kubernetes ndt DaemonSet is healthy. Also
        consider that this could be a false positive because of bad or broken
        monitoring.
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/tZHLFQRZk/k8s-workload-overview

# HTTP per-status response count is exported by stackdriver. By querying
# stackdriver metrics, we can alert if the server-side errors rate for mlab-ns
# is too high or if the service is unavailable.
  - alert: MlabNS_TooManyServerSideErrors
    expr: |
      sum(stackdriver_gae_app_appengine_googleapis_com_http_server_response_count{
        deployment="mlabns-stackdriver", module_id="default", loading="false",
        response_code=~"5.."}) /
      sum(stackdriver_gae_app_appengine_googleapis_com_http_server_response_count{
        deployment="mlabns-stackdriver", module_id="default", loading="false"}) > 0.01
    for: 2m
    labels:
      repo: dev-tracker
      severity: ticket
    annotations:
      summary: Server-side errors rate for mlab-ns is over 1%.
      description: Stackdriver reports more than 1% of the HTTP requests to mlab-ns are
        returning a 5xx status code. Please check the mlab-ns logs to verify what is
        causing them.

# Check 5xx errors for the rate-limiter deployment, too.
  - alert: RateLimiterTooManyServerSideErrors
    expr: |
      sum_over_time(stackdriver_gae_app_appengine_googleapis_com_http_server_response_count{
        deployment="mlabns-stackdriver", module_id="rate-limiter", loading="false",
        response_code=~"5.."}[24h]) > 5
    for: 1m
    labels:
      repo: dev-tracker
      severity: ticket
    annotations:
      summary: Server-side errors rate for rate-limiter is not zero.
      description: Stackdriver reports some of the HTTP requests to
        rate-limiter are returning a 5xx status code. Please check the
        rate-limiter logs to determine the cause.

# If any of the deployments in the mlab-ns GAE project is unavailable, an alert
# should fire immediately.
  - alert: MlabNS_ServiceUnavailable
    expr: sum(stackdriver_gae_app_appengine_googleapis_com_http_server_response_count{
      deployment="mlabns-stackdriver", response_code=~"503"}) > 0
    labels:
      repo: dev-tracker
      severity: page
    annotations:
      summary: Service {{ $labels.module }} in the mlab-ns project is unavailable.
      description: Stackdriver reports some requests to the {{ $labels.module }}
        service are returning error 503. This is likely generated by GAE and
        means there is a persistent failure in the underlying service. Please
        check the GAE dashboard and logs to determine the cause.

# If no ndt_ssl traffic has been redirected to the staging instance in the past
# hour, an alert should fire.
  - alert: MlabNS_NdtSslReverseProxyNotWorking
    expr: |
      sum(sum_over_time(stackdriver_gae_app_logging_googleapis_com_user_reverse_proxy_counter{
        resource="ndt_ssl"}[1h])) == 0
    for: 1m
    labels:
      repo: dev-tracker
      severity: ticket
    annotations:
      summary: mlab-ns' reverse proxy is not redirecting any ndt_ssl traffic.
      description: >
        mlab-ns is supposed to send some of the ndt_ssl traffic to the
        staging instance, and this has not been happening for the past hour.
        Please check that the probability set on Google Cloud Datastore is
        not zero for the ndt_ssl experiment and that there are no errors
        related to reverse proxying in the mlab-ns logs.

# One or more generic (non-experiment specific) mlab-ns metrics is missing.
# These are metrics that mlab-ns relies on to determine whether an experiment
# should receive production traffic, so we need to make sure that the metrics
# are always present.
  - alert: MlabNS_GenericMetricsMissing
    expr: |
      absent(node_filesystem_size_bytes{cluster="platform-cluster", mountpoint="/cache/data"})
        or absent(node_filesystem_avail_bytes{cluster="platform-cluster", mountpoint="/cache/data"})
        or absent(lame_duck_experiment{cluster="platform-cluster"})
    for: 30m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: A generic (non-experiment specific) mlab-ns metric is missing.
      description: >
        One of the metrics that mlab-ns relies on to determine if an experiment
        should receive production traffic is missing. Most of these metrics are
        scraped from the platform-cluster. Be sure that the fedrated scrape job
        for the platform-cluster is healthy[1]. If the GMX metric is missing,
        then make sure that gmx-server deployment healthy in the
        prometheus-federation cluster.
        [1]: https://prometheus.mlab-oti.measurementlab.net/targets#job-platform-cluster

# One or more NDT-specific metrics is missing. These are the NDT metrics that
# mlab-ns relies on to determine whether NDT is up and running, so we need to
# make sure that the metrics are always present. NOTE: mlab-ns additionally
# relies on the script_exporter metric 'script_success{service="ndt_e2e"}', but
# alerting for that metric is already handled by the
# ScriptExporterMissingMetrics alert.
  - alert: MlabNS_NdtMetricsMissing
    expr: |
      absent(probe_success{service="ndt_raw"})
        or absent(probe_success{service="ndt_raw_ipv6"})
        or absent(probe_success{service="ndt_ssl"})
        or absent(probe_success{service="ndt_ssl_ipv6"})
    for: 30m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: A metric for an NDT service is missing.
      description: >
        If the blackbox_exporter service is running, then there may be a target
        configuration error. Check the target definitions in GCS and the target
        status in Prometheus[1].
        [1]: https://prometheus.mlab-oti.measurementlab.net/targets#job-blackbox-targets

# One or more Neubot-specific metrics is missing. These are the Neubot metrics that
# mlab-ns relies on to determine whether Neubot is up and running, so we need to
# make sure that the metrics are always present.
  - alert: MlabNS_NeubotMetricsMissing
    expr: |
      absent(probe_success{service="neubot"})
        or absent(probe_success{service="neubot_ipv6"})
        or absent(probe_success{service="neubot_tls"})
        or absent(probe_success{service="neubot_tls_ipv6"})
    for: 30m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: A metric for a Neubot service is missing.
      description: >
        If the blackbox_exporter service is running, then there may be a target
        configuration error. Check the target definitions in GCS and the target
        status in Prometheus[1].
        [1]: https://prometheus.mlab-oti.measurementlab.net/targets#job-blackbox-targets

# The collectd container should be present on the platform-cluster.
  - alert: PlatformCluster_CoreServices_CollectdMlabMissing
    expr: |
      absent(collectd_mlab_success{cluster="platform-cluster"})
    for: 1h
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: A collectd-mlab k8s pod is missing.
      description: The collectd-mlab service runs in the 'utilization'
        daemonset on the platform-cluster. Is it deployed?

# A collectd service should always be running.
  - alert: PlatformCluster_CoreServices_CollectdMlabDown
    expr: |
      collectd_mlab_success{cluster="platform-cluster"} == 0
        unless on(node) gmx_machine_maintenance == 1
        unless on(node) kube_node_spec_taint{cluster="platform-cluster", key="lame-duck"}
    for: 1h
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: A collectd-mlab k8s pod is down.
      description: The collectd-mlab service in the 'utilization'
        daemonset on the platform-cluster is failing. Use `kubectl exec` to
        investigate the container environment on a failing pod.  Run the check
        script manually to see what the specific error is
        (/usr/lib/nagios/plugins/check_collectd_mlab.py). If it works, check
        cron settings, environment, etc.

# One or more of the backend services handled by the nginx proxy is down.
  - alert: Prometheus_NginxProxiedServiceDown
    expr: probe_success{job="nginx-proxied-services"} == 0
    for: 30m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: Backend services handled by the nginx proxy are down.
      description: Did the nginx k8s deployment or nginx-lb k8s service fail?
        Did the backend service or deployment fail in some way?

# TODO:
#   Replace this with two other alerts:
#    1.  Alert if hourly test volume on servers drops relative to same hour on recent days.
#    2.  E2E alert that compares rows in tables to test volume on servers.
#
# ParserDailyVolumeTooLow: 24 hour test volume has dropped over 30% compared to
# the average of the 2 smallest test volumes of 4 days out of the last week.  Two vectors
# of conservative constant value avoid false alarms when there is little valid history.
# On occasion, processing may fall behind a bit.  The "FOR 2h" waits 2h before triggering
# an actual alert, so the pipeline may fall behind for up to 2 hours without alerting.
# However, if the pipeline falls several hours behind, and stays behind for more than
# 2 hours, the alert will fire.
#
# In normal operation, we expect the 50th quantile to split mid-way between the two smallest
# volume days of the 4 sample days.  The 4 sample always include one weekend day, so one of
# the two smallest days will generally be a weekend day.
# For example, for a Tuesday, the prior data might be ordered (decreasing):
# M, F, W, Sa, C1, C2,
# and the 50th quantile will be midway between previous Sat and previous Wed.
#
# The alert condition ignores batch processing.
#
# Implementation notes:
# This alert uses label_replace to merge multiple vectors.  We tried simpler queries using
# AND, OR or +, but these do not do what we need.  We use label_replace to add a new "delay"
# label, which then allows us to compute quantile across multiple vectors.  (Or we could compute
# sums, averages, topk, min, etc.)
# The constant vectors require adding two labels, "service" which associates with a pipeline, and
# "delay" which differentiates from the actual delayed metrics for 1d, 3d, etc.
# For each pipeline service, the quantile computation then aggregates across the 6 vectors in
# the delay dimension.
  - alert: ParserDailyVolumeTooLow
    expr: |
      candidate_service:etl_test_count:increase24h
        < (0.7 * quantile by(service)(0.5,
          label_replace(candidate_service:etl_test_count:increase24h offset 1d, "delay", "1d", "", ".*") or
          label_replace(candidate_service:etl_test_count:increase24h offset 3d, "delay", "3d", "", ".*") or
          label_replace(candidate_service:etl_test_count:increase24h offset 5d, "delay", "5d", "", ".*") or
            label_replace(candidate_service:etl_test_count:increase24h offset 1w, "delay", "7d", "", ".*") or
            label_replace(label_replace(vector(0), "delay", "c1", "", ".*"), "service", "etl-disco-parser", "", ".*") or
            label_replace(label_replace(vector(0), "delay", "c2", "", ".*"), "service", "etl-disco-parser", "", ".*") or
            label_replace(label_replace(vector(0), "delay", "c1", "", ".*"), "service", "etl-ndt-parser", "", ".*") or
            label_replace(label_replace(vector(0), "delay", "c2", "", ".*"), "service", "etl-ndt-parser", "", ".*") or
            label_replace(label_replace(vector(0), "delay", "c1", "", ".*"), "service", "etl-sidestream-parser", "", ".*") or
            label_replace(label_replace(vector(0), "delay", "c2", "", ".*"), "service", "etl-sidestream-parser", "", ".*") or
            label_replace(label_replace(vector(0), "delay", "c1", "", ".*"), "service", "etl-traceroute-parser", "", ".*") or
            label_replace(label_replace(vector(0), "delay", "c2", "", ".*"), "service", "etl-traceroute-parser", "", ".*")
            ))
    for: 2h
    labels:
      repo: dev-tracker
      severity: ticket
    annotations:
      summary: Today's test volume is less than 70% of nominal daily volume.
      description: Are machines online? Is data being collected? Is the parser working?
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/PKqnWeNmz/

  - alert: PlatformCluster_PusherDailyDataVolumeTooLow
    expr: |
      datatype:pusher_bytes_per_tarfile:increase24h
        < (0.7 * quantile by(datatype)(0.5,
          label_replace(datatype:pusher_bytes_per_tarfile:increase24h offset 1d, "delay", "1d", "", ".*") or
          label_replace(datatype:pusher_bytes_per_tarfile:increase24h offset 3d, "delay", "3d", "", ".*") or
          label_replace(datatype:pusher_bytes_per_tarfile:increase24h offset 5d, "delay", "5d", "", ".*") or
            label_replace(datatype:pusher_bytes_per_tarfile:increase24h offset 1w, "delay", "7d", "", ".*") or
            label_replace(label_replace(vector(0), "delay", "c1", "", ".*"), "datatype", "ndt5", "", ".*") or
            label_replace(label_replace(vector(0), "delay", "c2", "", ".*"), "datatype", "ndt5", "", ".*") or
            label_replace(label_replace(vector(0), "delay", "c1", "", ".*"), "datatype", "pcap", "", ".*") or
            label_replace(label_replace(vector(0), "delay", "c2", "", ".*"), "datatype", "pcap", "", ".*") or
            label_replace(label_replace(vector(0), "delay", "c1", "", ".*"), "datatype", "switch", "", ".*") or
            label_replace(label_replace(vector(0), "delay", "c2", "", ".*"), "datatype", "switch", "", ".*") or
            label_replace(label_replace(vector(0), "delay", "c1", "", ".*"), "datatype", "tcpinfo", "", ".*") or
            label_replace(label_replace(vector(0), "delay", "c2", "", ".*"), "datatype", "tcpinfo", "", ".*") or
            label_replace(label_replace(vector(0), "delay", "c1", "", ".*"), "datatype", "traceroute", "", ".*") or
            label_replace(label_replace(vector(0), "delay", "c2", "", ".*"), "datatype", "traceroute", "", ".*")
            ))
    for: 2h
    labels:
      repo: dev-tracker
      severity: ticket
    annotations:
      summary: Test data volume today is less than 70% of nominal daily volume.
      description: Are machines online? Is data being collected? Is pusher working?
        Is mlab-ns working? A new rollout?
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/WnaxPZJZz

# PusherSLO
#
# Pusher uploads archives for a machine every few hours. After every boot, a
# machine starts with a "lower bound" mtime equal to the current time. As data
# is written to disk, we expect the "lower bound" to gradually move forward in
# time. If it does not, then data is not being successfully uploaded and
# removed. That is a problem.
#
# The alert excludes nodes in maintenance or lame-duck.
  - alert: PusherFinderMtimeLowerBoundIsTooOld
    expr: |
      (time() - pusher_finder_mtime_lower_bound) > (16 * 60 * 60)
        unless on(machine) gmx_machine_maintenance == 1
        unless on(machine) kube_node_spec_taint{cluster="platform-cluster", key="lame-duck"}
    for: 8h
    labels:
      repo: dev-tracker
      severity: ticket
    annotations:
      summary: Data on disk are too old and should have been uploaded already
      description: The min file mtime seen by pusher has been older than 16
        hours for at least 8 hours. If uploads are failing then data will be lost
        if the node reboots.
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/WnaxPZJZz

# GCS Transfer SLO
#
# We run daily GCS transfers between project buckets and to the public archive.
# See: https://github.com/m-lab/gcp-config/blob/master/daily-archive-transfers.yaml
#
# This alert enforces that daily transfers are working for all datatypes.
  - alert: GCSTransfers_ArchiveFilesDoNotMatchOrMissing
    expr: |
      increase(gcs_archive_files_total{bucket="archive-mlab-oti"}[1d]) - ignoring(bucket)
         increase(gcs_archive_files_total{bucket="archive-measurement-lab"}[1d]) != 0
      OR
      absent(gcs_archive_files_total)
    for: 1d
    labels:
      repo: dev-tracker
      severity: ticket
    annotations:
      summary: GCS Transfers may not include all files.
      description: Daily transfers should include all archive files for each day.
        This alert is firing because over the last two days, the archive file
        counts did not match.

# Too many NDT S2C tests on a node are being impacted by switch discards.
  - alert: DataQuality_TooManyNdtS2cTestsWithDiscards
    expr: (bq_ndt_s2c_with_discards / bq_ndt_s2c_total) > 0.05
    for: 10m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: More than 5% of NDT tests in a 24h period were affected by discards.
      description: Too many NDT S2C tests in a 24h period (BQ partition_date)
        were affected by switch discards on {{ $labels.node }}. The switch may
        be overloaded, or TCP pacing may be misconfigured on the node.
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/SuqnZ6Hiz/ops-switch-overview

# Too many ifInErrors are occuring each day on a switch uplink for too may days in a row.
  - alert: DataQuality_TooManySwitchIfInErrors
    expr: increase(ifInErrors{ifAlias="uplink"}[1d]) > 100
    for: 7d
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: There have been more than 100 ifInErrors per day for more than 7d.
      description: ifInErrors are generally very low level, physical layer
        errors. Some amount of errors is normal (e.g., even solar activity can
        cause them), but over a certain threshold they should be investigated.
        In the past, we have found that eleveated levels of errors is resolved
        by having a tech visit the rack and clean and reseat the uplink optic
        and fiber patch cable.
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/SuqnZ6Hiz/ops-switch-overview

# The node_exporter running on eb.measurementlab.net is down.
  - alert: NodeExporterOnEbDownOrMissing
    expr: |
      up{job="eb-node-exporter"} == 0
        or absent(up{job="eb-node-exporter"})
    for: 10m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: The node_exporter running on eb.measurementlab.net is down.
      description: Login to EB to see if it is in fact crashed. If so, look
        through the logs.

# The node_exporter running on dns.measurementlab.net is down.
  - alert: NodeExporterOnDnsDownOrMissing
    expr: up{job="dns-node-exporter"} == 0 or absent(up{job="dns-node-exporter"})
    for: 10m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: The node_exporter running on dns.measurementlab.net is down.
      description: Login to to see if it is in fact crashed. If so, look
        through the logs.

# GardenerDownOrMissing fires when the etl-gardener is down or absent.
# TODO: enable annotations to ignore some container ports, and simplify this query.
# https://github.com/m-lab/prometheus-support/issues/48
  - alert: GardenerDownOrMissing
    expr: |
      up{container="etl-gardener",instance=~".*:9090"} == 0
        or absent(up{container="etl-gardener",instance=~".*:9090"})
    for: 10m
    labels:
      repo: dev-tracker
      severity: ticket
    annotations:
      summary: The ETL Gardener instance is down or missing.
      description: Gardener runs in the data-processing-cluster.
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/eBbUW6oik

# ETL_ParserPanicNonZero fires when an ETL parser panics. The number of panics
# may increase due to a transient issue or a short-lived trigger for a parser
# bug. The alert will fire as long as the rate is above zero for more than 5
# minutes. So, it's possible for false-negatives in response to isolated events,
# but it also allows the alert to stop firing without a redeploy when the event
# is short-lived.
  - alert: ETL_ParserPanicNonZero
    expr: irate(etl_panic_count[4m]) > 0
    for: 5m
    labels:
      repo: dev-tracker
      severity: ticket
    annotations:
      summary: An ETL parser panicked {{ $labels.instance }}
      description: Bugs cause panics. This bug should be fixed. Parsers run in
        AppEngine. Check logs to see the panic stack trace. Identify the archive
        that led to the panic (logs or TaskQueue tasks with many retries). Fix
        the bug or create a new issue describing the failure and linking to the
        triggering archive.

# ETL_AnnotationDownOrMissing fires when the annotator AppEngine service is down
# (prometheus scrape attempts fail) or prometheus does not know about the
# annotator service at all.
  - alert: ETL_AnnotationDownOrMissing
    expr: up{service="annotator"} == 0 or absent(up{service="annotator"})
    for: 30m
    labels:
      repo: dev-tracker
      severity: ticket
    annotations:
      summary: An ETL Annotation Server is offline or missing!
      description: The annotator runs in AppEngine. Check logs and recent
        deployments. The daily and batch parsers may also be affected.

# NDT_AsnAnnotationRatioTooLowOrMissing fires when the client annotations on NDT
# tests appears to have too many failures or the bq_ndt_annotation_* metrics
# disappear.
  - alert: NDT_AsnAnnotationRatioTooLowOrMissing
    expr: |
      bq_annotation_asn_success / bq_annotation_total < 0.98
        or absent(bq_annotation_asn_success / bq_annotation_total)
    for: 30m
    labels:
      repo: dev-tracker
      severity: ticket
    annotations:
      summary: Too many NDT tests are missing asn annotations!
      description: The annotator runs in AppEngine. Check logs and recent
        deployments. The daily and batch parsers may also be affected.

# NDT_GeoAnnotationRatioTooLowOrMissing fires when the client annotations on NDT
# tests appears to have too many failures or the bq_annotation_* metrics
# disappear.
  - alert: NDT_GeoAnnotationRatioTooLowOrMissing
    expr: |
      bq_annotation_geo_success / bq_annotation_total < 0.98
        or absent(bq_annotation_geo_success / bq_annotation_total)
    for: 30m
    labels:
      repo: dev-tracker
      severity: ticket
    annotations:
      summary: Too many NDT tests are missing geolocation annotations!
      description: The annotator runs in AppEngine. Check logs and recent
        deployments. The daily and batch parsers may also be affected.

# Gardener_ParseTimeDifferenceTooOldOrMissing fires when the maximum and minimum
# parse_time values for each data set are greater than 80 days or the
# bq_gardener_parse_time_age_days metrics disappear.
  - alert: Gardener_ParseTimeDifferenceTooOldOrMissing
    expr: |
      bq_gardener_parse_time_age_days > 80
        or absent(bq_gardener_parse_time_age_days)
    for: 10m
    labels:
      repo: dev-tracker
      severity: ticket
    annotations:
      summary: 'Gardener progress is too slow for dataset: {{ $labels.dataset }}'
      description: Gardener throughput is dependent on the etl-batch-parser and
        associated queues.yaml.
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/eBbUW6oik/

# Rebot_MissingMetrics fires when no metrics can be collected for rebot for
# the past 10 minutes.
  - alert: Rebot_MissingMetrics
    expr: absent(up{run='rebot'})
    for: 10m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: The rebot instance running on eb.measurementlab.net is down.
      description: Metrics for rebot cannot be collected. The instance is down
        or not reachable. Check that the rebot daemon on eb.measurementlab.net
        is running and port 9999 is reachable.

# Cloud_TooManyAppEngineVersions fires when there are too many AE versions to deploy
# full ETL deployment successfully.
  - alert: Cloud_TooManyAppEngineVersions
    expr: sum(gcp_aeflex_versions{container="service-discovery"}) / 210 > 0.95
    for: 10m
    labels:
      repo: dev-tracker
      severity: ticket
    annotations:
      summary: Too many AppEngine versions for the project.
      description: AppEngine has a project maximum of 210 versions across all
        services. Until old versions are deleted, future deployments will begin
        to fail. Please delete them or create a tool that does it automatically.
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/Yor77urmk/cloud-appengine

# Cloud_TooManyInactiveInstances checks that all running instances are active
# rather than wasting resources.
  - alert: Cloud_TooManyInactiveInstances
    expr: |
      sum (gcp_aeflex_instances{container="service-discovery", active="false"})
        / sum (gcp_aeflex_instances{container="service-discovery"}) > 0.25
    for: 30m
    labels:
      repo: dev-tracker
      severity: ticket
    annotations:
      summary: Too many AppEngine instances are running but not serving.
      description: Failed or interrupted deployments do not shutdown instances
        completely. Over time, a project accumulates inactive instances that
        waste resources. Please delete them or create a tool that does it
        automatically.
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/Yor77urmk/cloud-appengine

# Boot_MachineFailedToBoot a machine has failed to boot for more than a day.
  - alert: Boot_MachineFailedToBoot
    expr: epoxy_last_success < epoxy_last_boot
      unless on(machine) (
        gmx_machine_maintenance == 1 or
        label_replace(
          kube_node_spec_taint{cluster="platform-cluster", key="lame-duck"},
          "machine", "$1", "node", "(.*)"
        )
      )
    for: 24h
    labels:
      repo: dev-tracker
      severity: ticket
    annotations:
      summary: A machine booted recently, but has not reported success yet.
      description: Failing to report success may be due to configuration or
        runtime failures. Check the machine console messages via the DRAC.
        Check the k8s setup logs on the machine (/tmp/setup_k8s.log). Check the
        epoxy-boot-api VM logs in Stackdriver.
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/JaSyFC9mk/boot-nodes

# Boot_EpoxyServerOfflineOrMissing fires when the epoxy boot api server is
# offline or misconfigured.
  - alert: Boot_EpoxyServerOfflineOrMissing
    expr: up{job="epoxy-boot-api"} == 0 or absent(up{job="epoxy-boot-api"})
    for: 30m
    labels:
      repo: dev-tracker
      severity: ticket
    annotations:
      summary: The ePoxy boot api server is down or missing. Nodes cannot boot!
      description: Has the configuration been removed from prometheus? Is the
        service running? Check for the epoxy-boot-api-* VM. Check for recent
        failed Cloud Builder builds caused by recent commits; check those logs.
        Check the VM service logs from Stackdriver.
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/t_juk39ik/boot-epoxy-server

# PlatformCluster_FederationScrapeJobFailing indicates that scraping for the
# k8s platform cluster is failing.  The threshold for being down is 4m, such
# that this alert will fire slightly before the alerts for missing individual
# DaemonSet metrics (and can inhibit those).  Also, ~4 scrape cycles is enough
# time for this to be down before someone knows about it.
  - alert: PlatformCluster_FederationScrapeJobFailing
    expr: up{job="platform-cluster"} == 0
    for: 4m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: Federation scraping of the k8s platform cluster is down.
      description: Scraping of Prometheus on the platform cluster is down.
        This could be that Prometheus in the platform cluster is down, or it
        could be that the prometheus-federation scrape job is failing for some
        reason unrelated to the platform clustser. Verify that the node where
        Prometheus should be running in the platform cluster is healthy
        (`kubectl get nodes --selector run=prometheus-server`). Verify the
        Prometheus deployment is running using - `kubectl get pods`.  Check pod
        logs.  Check the GCP console for the VM for messages or status. Check
        the status of the target in the prometheus-federation Web console.

# PlatformCluster_FederationScrapeJobMissing indicates that the job for
# scraping the k8s platform cluster does not exist.
  - alert: PlatformCluster_FederationScrapeJobMissing
    expr: absent(up{job="platform-cluster"})
    for: 4m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: Federation scraping of the k8s platform cluster is missing.
      description: A scrape job for the k8s platform cluster is missing. This
        is almost surely the result of a misconfiguration of Prometheus. Check
        the running prometheus configuration to be sure that there is actually
        a job for scraping the k8s platform cluster.

# PlatformCluster_PrometheusPersistentDiskTooFull fires when the persistent
# disk mounted on the Prometheus VM gets too full (less than 5% free).
  - alert: PlatformCluster_PrometheusPersistentDiskTooFull
    expr: |
      node_filesystem_avail_bytes{cluster="platform-cluster", node="prometheus-platform-cluster", mountpoint="/mnt/local"}
        / node_filesystem_size_bytes < 0.05
    for: 1m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: The Prometheus persistent disk has less than 5% free space.
      description: The Prometheus persistent disk has less than 5% free space.
        Investigate filesystem usage on the VM, but most likely if this alert
        fires it means that the size of the persistent disk is too small and
        may need to be increased. GCE persistent disks can be resized, even on
        a running VM. Increase the size of the disk as necessary, then unmount
        the disk on the machine and run `resize2fs`, then mount it again. Don't
        forget to increase the value in the bootstrap_prometheus.sh script in
        the k8s-support repository.
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/sVklmeHik/prometheus-self-monitoring?orgId=1&var-datasource=Platform%20Cluster%20(mlab-oti)

# Check for missing workloads.

  - alert: PlatformCluster_CadvisorMissing
    expr: absent(up{deployment="cadvisor", cluster="platform-cluster"})
    for: 5m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: The CAdvisor DaemonSet is missing or has no metrics.
      description: The CAdvisor DaemonSet is missing or has no metrics. Verify that
        the DaemonSet is healthy (`kubectl describe ds cadvisor`).
      dashboard: https://grafana.mlab-staging.measurementlab.net/d/tZHLFQRZk/k8s-workload-overview

  - alert: PlatformCluster_FluentdMissing
    expr: absent(up{deployment="fluentd", cluster="platform-cluster"})
    for: 5m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: The Fluentd DaemonSet is missing or has no metrics.
      description: The Fluentd DaemonSet is missing or has no metrics. Verify that
        the DaemonSet is healthy (`kubectl describe ds fluentd`).
      dashboard: https://grafana.mlab-staging.measurementlab.net/d/tZHLFQRZk/k8s-workload-overview

  - alert: PlatformCluster_NdtMissing
    expr: absent(up{deployment="ndt", cluster="platform-cluster"})
    for: 5m
    labels:
      repo: ops-tracker
      severity: page
    annotations:
      summary: The NDT DaemonSet is missing or has no metrics.
      description: The NDT DaemonSet is missing or has no metrics. Verify that
        the DaemonSet is healthy (`kubectl describe ds ndt`).
      dashboard: https://grafana.mlab-staging.measurementlab.net/d/tZHLFQRZk/k8s-workload-overview

  - alert: PlatformCluster_NeubotMissing
    expr: absent(up{deployment="neubot", cluster="platform-cluster"})
    for: 5m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: The Neubot DaemonSet is missing or has no metrics.
      description: The Neubot DaemonSet is missing or has no metrics. Verify that
        the DaemonSet is healthy (`kubectl describe ds neubot`).
      dashboard: https://grafana.mlab-staging.measurementlab.net/d/tZHLFQRZk/k8s-workload-overview

  - alert: PlatformCluster_NodeExporterMissing
    expr: absent(up{deployment="node-exporter", cluster="platform-cluster"})
    for: 5m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: The node_exporter DaemonSet is missing or has no metrics.
      description: The node_exporter DaemonSet is missing or has no metrics.
        Verify that the DaemonSet is healthy (`kubectl describe ds
        node-exporter`).
      dashboard: https://grafana.mlab-staging.measurementlab.net/d/tZHLFQRZk/k8s-workload-overview

  - alert: PlatformCluster_HostExperimentMissing
    expr: absent(up{deployment="host", cluster="platform-cluster"})
    for: 5m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: The host DaemonSet is missing or has no metrics.
      description: The host DaemonSet is missing or has no metrics.
        Verify that the DaemonSet is healthy (`kubectl describe ds
        host`).
      dashboard: https://grafana.mlab-staging.measurementlab.net/d/tZHLFQRZk/k8s-workload-overview

  # If any node is NotReady for too long, fire an alert, unless the node is in
  # lame-duck mode, GMX maintenance mode, or the scrape job for the entire node
  # is down.
  - alert: PlatformCluster_NodeNotReady
    expr: |
      kube_node_status_condition{cluster="platform-cluster", condition="Ready", status="false"} == 1
        unless on(node) (
          kube_node_spec_taint{cluster="platform-cluster", key="lame-duck"} or
          gmx_machine_maintenance == 1 or
          up{job="kubernetes-nodes", cluster="platform-cluster"} == 0
        )
    for: 1h
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: A node has had a NotReady condition for too long.
      description: A node has had a NotReady condition for too long. Generally
        this is caused when the kubelet on the node is unable to communicate
        with the API. Is the machine booted? Does the machine have network
        connectivity? Check the status of the kubelet on the machine. Look at
        the kubelet logs (journalctl -u kubelet).
      dashboard: https://grafana.mlab-staging.measurementlab.net/d/rJ7z2Suik/k8s-site-overview

  # If any pod is down or otherwise broken, fire an alert, unless the node is
  # in lame-duck mode, the node is NotReady, GMX maintenance mode, or the
  # scrape job for the entire node is down.
  - alert: PlatformCluster_PodDown
    expr: |
      kube_pod_info == 1 and on(exported_pod) kube_pod_status_ready{condition="true"} == 0 unless on(node) (
          kube_node_spec_taint{cluster="platform-cluster", key="lame-duck"} or
          kube_node_status_condition{condition="Ready", status="false"} == 1 or
          gmx_machine_maintenance == 1 or
          up{job="kubernetes-nodes", cluster="platform-cluster"} == 0
        )
    for: 1h
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: A {{ $labels.deployment }} pod is down or broken.
      description: A {{ $labels.deployment }} pod is down or broken. Verify that the
        DaemonSet or Deployment is healthy. Check the status of the node that the
        pod is scheduled on. Check the status of the pod itself, if it exists.
      dashboard: https://grafana.mlab-staging.measurementlab.net/d/rJ7z2Suik/k8s-site-overview

  # A DaemonSet rollout is progressing too slowly. Unless 95% of a DaemonSet's
  # pods are updated, then fire an alert if the rate of increase over the last
  # hour is less than or equal to 2. This is pretty conservative since NDT pods
  # should update at something more like ~24 per hour. The somewhat unusual
  # toleration of 70m is because once a rollout starts, comparing the current
  # value of updated_scheduled to the value from an hour ago (all pods) will
  # yield a negative number for roughly the period of the offset (1h in this
  # case). 70m just gives us a safe window to cross that inversion.
  - alert: PlatformCluster_RolloutTooSlowOrStuck
    expr: |
      kube_daemonset_updated_number_scheduled - (kube_daemonset_updated_number_scheduled offset 1h) <= 2
        unless (
          kube_daemonset_updated_number_scheduled / kube_daemonset_status_desired_number_scheduled > 0.95 or
          kube_daemonset_status_desired_number_scheduled == 0
        )
    for: 70m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: A {{ $labels.daemonset }} DaemonSet rollout is going too slowly.
      description: Not enough pods were updated in the past hour for a
        {{ $labels.daemonset }} DaemonSet rollout, which indicates that the
        rollout is stuck in some way. Usually this happens when errors occur
        updating a pod on a node, and the number of nodes on which this error
        happens exceeds the maxUnavailable setting for a RollingUpdate. Look
        for {{ $labels.daemonset }} pods with a status other than Running and
        inspect them to figure out why they are in that state.
      dashboard: https://grafana.mlab-staging.measurementlab.net/d/tZHLFQRZk/k8s-workload-overview

  # Too many kubelet or kernel versions on the platform for too long. This is
  # an indication that a rolling reboot has stalled.
  - alert: PlatformCluster_TooManyKubeletOrKernelVersions
    expr: |
      count(
        count by (kubelet_version) (
          kube_node_info and on(node) kube_node_labels{label_mlab_type="platform"}
        )
      ) > 1
      or
      count(
        count by (kernel_version) (
          kube_node_info and on(node) kube_node_labels{label_mlab_type="platform"}
        )
      ) > 1
    for: 7d
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: Too many kubelet or kernel versions in the cluster for too long.
      description: >
        There has been more than one kubelet or kernel version running in the
        platform cluster for too long. Probably the only reason there should be
        more than one version in the cluster is when kubernetes and/or CoreOS
        has been upgraded in epoxy-images and a rolling reboot has not
        completed to apply those changes. Since CLUO will only reboot a single
        node at a time, stalls of rolling reboots can be easily caused by a
        node in a NotReady state.  Check for nodes in a NotReady state, and if
        necessary delete them from the cluster so the rolling reboot can
        continue. Looks at the log of the update-operator pod in the
        reboot-coordinator namespace to see which node it may be stuck on.
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/tZHLFQRZk/k8s-workload-overview

  # The desired number of pods for a DaemonSet are not equal to the current
  # number scheduled.
  - alert: PlatformCluster_DaemonSetHasTooFewPods
    expr: |
      kube_daemonset_status_desired_number_scheduled{cluster="platform-cluster"} !=
        kube_daemonset_status_current_number_scheduled{cluster="platform-cluster"}
    for: 1h
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: DaemonSet {{ $labels.daemonset }} has fewer pods scheduled than desired.
      description: DaemonSet {{ $labels.daemonset }} has fewer pods scheduled than desired.
        Check the status of the DaemonSet for clues with
        `kubectl describe daemonset {{ $labels.daemonset }} -n {{ $labels.namespace }}`
      dashboard: https://grafana.mlab-staging.measurementlab.net/d/tZHLFQRZk/k8s-workload-overview

  # The desired number of replicas for a Deployment are not equal to the
  # current number scheduled.
  - alert: PlatformCluster_DeploymentHasTooFewReplicas
    expr: |
      kube_deployment_spec_replicas{cluster="platform-cluster"} !=
        kube_deployment_status_replicas{cluster="platform-cluster"}
    for: 1h
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: Deployment {{ $labels.exported_deployment }} has less replicas than desired.
      description: Deployment {{ $labels.exported_deployment }} has less replicas than desired.
        Check the status of the deployment for clues with
        `kubectl describe deployment {{ $labels.exported_deployment }}`
      dashboard: https://grafana.mlab-staging.measurementlab.net/d/tZHLFQRZk/k8s-workload-overview

  # The /cache/data mount point on a node has exceeded 95% of its capacity.
  # This is where all pods write all experiment and core service data (shared
  # pool of space). If this mount point fills up, all experiments and core
  # services will fail in some way.
  - alert: PlatformCluster_DataPartitionTooFull
    expr: |
      ((node_filesystem_size_bytes{cluster="platform-cluster", mountpoint="/cache/data"} -
          node_filesystem_free_bytes{cluster="platform-cluster", mountpoint="/cache/data"})
          / node_filesystem_size_bytes{cluster="platform-cluster", mountpoint="/cache/data"})
        > 0.95
    for: 10m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: The /cache/data mount point on a node is more than 95% full.
      description: All experiment and core service data is written to a shared
        pool of disk space on a partition mounted at /cache/data. The mount
        point has exceed 95% usage. Check that the pusher sidecar container in
        all pods is working. See which pod is using all the space with `df -sh
        /cache/data/*`.
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/rJ7z2Suik/k8s-site-overview

# PlatformCluster Etcd alerts.
# Mostly gleaned from:
# https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/etcd3_alert.rules.yml

  - alert: PlatformCluster_EtcdMetricsMissing
    expr: absent(etcd_server_has_leader)
    for: 5m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: Metrics are missing for etcd.
      description: Metrics are missing for etcd. Scraping of etcd
        is probably failing, or Prometheus is having trouble scraping the
        federated platform cluster instance. Check to be sure that the platform
        cluster instance is running. Is there is a TLS certificate error
        causing scraping to fail, or a network issue? Look at the "Error"
        column of the targets page on the Prometheus Web interface for clues
        http://prometheus-platform-cluster.mlab-oti.measurementlab.net:9090/targets#job-kubernetes-etcd

  - alert: PlatformCluster_EtcdHasNoLeader
    expr: etcd_server_has_leader == 0
    for: 5m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: At least one etcd cluster member has no leader.
      description: An etcd cluster member is reporting that it has no leader.
        This should never happen. Find out which master server is hosting the
        ectd instance(s) and make sure that node is healthy and has network
        connectivity to the other masters.
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/milv1PgZz/k8s-etcd-overview

  - alert: PlatformCluster_EtcdTooManyLeaderChanges
    expr: rate(etcd_server_leader_changes_seen_total{job=~".*etcd.*"}[30m]) > 3
    for: 30m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: Too many leader changes in the etcd cluster.
      description: etcd cluster members are changing leaders too frequently.
        Leader changes are normal (e.g., a master node is rebooted), but they
        should not happen too often. Look into networking, resource or other
        issues on the master nodes.
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/K8-zAIuik/k8s-master-cluster

  - alert: PlatformCluster_EtcdTooManyProposalFailures
    expr: rate(etcd_server_proposals_failed_total[30m]) > 5
    for: 30m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: Too many raft protocol proposal failures in the etcd cluster.
      description: There are too many raft protocol proposal failures happening
        in the etcd cluster. These type of errors are normally related to two
        issues - temporary failures related to a leader election or longer
        downtime caused by a loss of quorum in the cluster.
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/milv1PgZz/k8s-etcd-overview

  - alert: PlatformCluster_EtcdMemberCommunicationTooSlow
    expr: |
      histogram_quantile(0.99,
        rate(etcd_network_peer_round_trip_time_seconds_bucket{job=~".*etcd.*"}[30m])
      ) > 0.15
    for: 1h
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: Network communication between etcd cluster members is too slow.
      description: The members of the etcd cluster are on different master
        nodes, and each node is in a different power zone. Communication
        between the members is taking too long. Make sure that the VPC network
        is working as intended, and that it isn't overloaded.
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/milv1PgZz/k8s-etcd-overview

  - alert: PlatformCluster_EtcdWalFsyncsTooSlow
    expr: histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket[30m])) > 0.5
    for: 1h
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: etcd write-ahead-log fsync operations are taking too long.
      description: etcd uses a WAL (Write Ahead Log), and fsync operations to
        it are taking too long. Check that there are no problems with the disk
        on the master node, and that disk I/O throughput is not becoming a
        bottleneck.
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/K8-zAIuik/k8s-master-cluster

  - alert: PlatformCluster_EtcdBackedCommitsTooSlow
    expr: |
      histogram_quantile(0.99,
        rate(etcd_disk_backend_commit_duration_seconds_bucket{job=~".*etcd.*"}[30m])
      ) > 0.25
    for: 1h
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: etcd backend commits are taking too long to complete.
      description: etcd writes incremental snapshots to disk. These writes are
        taking too long to complete.  Check that there are no problems with the
        disk on the master node, and that disk I/O throughput is not becoming a
        bottleneck.
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/K8-zAIuik/k8s-master-cluster

# Platform Hardware alerts

  - alert: PlatformHardware_RamBelowExpected
    expr: |
      node_memory_MemTotal_bytes{machine=~"^mlab[1-4].[a-z]{3}[0-9]{2}.*"} / 2^20 < 16000
    for: 1d
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: System RAM is below the expected minimum value.
      description: All M-Lab machines have at least 16GB of RAM. The quantity
        of RAM on one or more machines has gone below 16GB, which may indicate
        a failed RAM module. Login to the machine and double check
        the hardware and/or system messages.
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/rJ7z2Suik/k8s-site-overview

  - alert: PlatformHardware_EdacUncorrectableErrors
    expr: |
      node_edac_uncorrectable_errors_total{machine=~"^mlab[1-4].[a-z]{3}[0-9tc]{2}.*"} > 0
    for: 1d
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: Uncorrectable errors detected in RAM.
      description: EDAC metrics are reporting uncorrectable memory errors.
        This may indicate a DIMM module beginning to go bad or an issue with
        the mainboard. Login to the machine and double check the hardware
        and/or system messages.
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/rJ7z2Suik/k8s-site-overview

  - alert: PlatformHardware_EdacCorrectableErrors
    expr: |
      node_edac_correctable_errors_total{machine=~"^mlab[1-4].[a-z]{3}[0-9tc]{2}.*"} > 0
    for: 1d
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: Correctable errors detected in RAM.
      description: EDAC metrics are reporting correctable memory errors.
        While correctable, this may indicate some issue with a DIMM module or
        the mainboard. Login to the machine and double check the hardware
        and/or system messages.
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/rJ7z2Suik/k8s-site-overview

# BMC alerts
# The Reboot API performs an E2E connection test on a BMC every time its
# /v1/e2e endpoint is scraped, and reports failure via this metric.
  - alert: BMC_CredentialsNotFound
    expr: |
        reboot_e2e_result{status="credentials_not_found"}
        unless on(site) gmx_site_maintenance == 1
        unless on(machine) gmx_machine_maintenance == 1
    for: 5m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: Credentials for {{ $labels.target }} not found on GCD.
      description: >
        The E2E test cannot run because credentials for this BMC are not
        present on Google Cloud Datastore. Please add them.

  - alert: BMC_ConnectionFailed
    expr: |
      reboot_e2e_result{status="connection_failed"}
      unless on(site) gmx_site_maintenance == 1
      unless on(machine) gmx_machine_maintenance == 1
    for: 1d
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: Connection to {{ $labels.target }} has failed.
      description: >
        The E2E test is failing because the Reboot API could't connect to this
        BMC. Please check that the BMC is enabled on this machine and the
        credentials in Datastore are correct.
