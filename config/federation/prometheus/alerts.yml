# M-Lab alert configuration.
#
# See https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules
# for more information on the alerting rules syntax.
#
## Notes about delay.
#
# There are inherent delays in the monitoring and alert pipeline. When
# designing alerts, remember that each step includes some delay:
#
#  * the time between reality changing and monitoring observing the change.
#  * the time between the observed change and the delay defined by the alert.
#  * the time between a firing alert and the "group_wait" time in alertmanager.
#  * the time between sending a notification and the notification arriving.
#
# For example, for a 60s collection period, a 2m alert delay, a 30s group_wait,
# and a 15s-3m SMS delivery delay, the time between reality changing and a
# human knowing could be over six minutes.
#
# Your job is to balance false-positives with responsiveness when necessary.

groups:
- name: alerts.yml
  rules:
# ClusterDown: when any of the federated prometheus k8s clusters is down for
# three sample periods, then raise an alert.
#
# All scraper metrics come from federated targets, so this is critical.
  - alert: ClusterDown
    expr: up{job="federation-targets"} == 0
    for: 10m
    labels:
      repo: dev-tracker
      severity: page
    annotations:
      summary: Instance {{ $labels.instance }} down
      description: '{{ $labels.instance }} of job {{ $labels.job }} has been down
        for more than 10 minutes.'

##
## SLOs
##
#
# MachineSLO
#
# CoreServices_SidestreamIsNotRunning: an M-Lab server is online, but the
# sidestream exporter is not. Since sidestream is a core service, this must be
# fixed.
  - alert: CoreServices_SidestreamIsNotRunning
    expr: |
      sum_over_time(up{service="sidestream"}[10m]) == 0
        and on(machine)
      sum_over_time(probe_success{service="ssh806"}[20m]) / 20 >= 0.9
        unless on(machine) (lame_duck_node == 1 or gmx_machine_maintenance
      == 1)
    for: 10m
    labels:
      repo: ops-tracker
      severity: page
    annotations:
      summary: "An M-Lab machine is online, but the sidestream exporter is not."
      description: "Since sidestream is a core service, this must be fixed."

# ScraperSLO
#
# ScraperMostRecentArchivedFileTimeIsTooOld: scraper uploads archives for a
# machine once a day. If the machine is online (for at least 30 hours), but
# scraper has not uploaded an archive for that machine for more than two days
# plus 8 hours, there is a problem.
#
# Note: we can wait two days because we expect that either a) few machines are
# affected by this at once, or b) many machines are affected and the
# ParserDailyVolumeTooLow will trigger first.
#
# Note: the delay threshold is set to 2h to prevent false positives. For
# example, if a machine remains running while it is not network accessible,
# then the machine will need time for scraper to catch up once it is network
# accessible again.
#
# TODO(soltesz): remove the != 0 check when legacy records are removed.
  - alert: ScraperMostRecentArchivedFileTimeIsTooOld
    expr: |
      (time() - (scraper_maxrawfiletimearchived{container="scraper-sync"} != 0)) > (56 * 60 * 60)
        and on(machine)
      (time() - process_start_time_seconds{service="sidestream"})> (30 * 60 * 60)
        unless on(machine) (lame_duck_node == 1 or gmx_machine_maintenance == 1)
    for: 2h
    labels:
      repo: dev-tracker
      severity: page
    annotations:
      summary: Scraper max file mtime is too old {{ $labels.rsync_url }}
      description: Max file mtime for {{ $labels.rsync_url }} is older than 56 hours.

# Scraper internal consistency.
#
# Verify that for every running scraper there is a corresponding metric from
# scraper-sync indicating that a collection was attempted. These should always
# be in sync with one another.
#
# We use scraper_lastcollectionattempt because scraper_maxrawfiletimearchived
# is not updated until the first successful upload. This is not possible before
# a machine comes online.
  - alert: ScraperSyncPresentWithoutScraperCollector
    expr: |
      (scraper_lastcollectionattempt{container="scraper-sync"}
        unless on(machine, experiment, rsync_module)
          up{container="scraper"})
    for: 3h
    labels:
      repo: dev-tracker
      severity: page
    annotations:
      summary: "Scraper deployent is out of sync with scraper-sync"
      description: ""

  - alert: ScraperCollectorMissingFromScraperSync
    expr: |
      (up{container="scraper"}
        unless on(machine, experiment, rsync_module)
          scraper_lastcollectionattempt{container="scraper-sync"})
    for: 3h
    labels:
      repo: dev-tracker
      severity: page
    annotations:
      summary: "Scraper deployent is out of sync with scraper-sync"
      description: >
        Scraper-sync should only report on the last collection attempt of every
        active Scraper deployment.

# Scraper_InconsistentDeployment checks whether each machine has the same
# number of deployments as all other machines. This is achived by counting the
# number of deployments per machine, and then comparing that number to the
# total number of rsync_module. Each machine should have one of each.
  - alert: Scraper_InconsistentDeployment
    expr: |
      count by(machine) (up{container="scraper"})
        != scalar(count(count by(rsync_module) (up{container="scraper"})))
    for: 2h
    labels:
      repo: dev-tracker
      severity: page
    annotations:
      summary: Scraper deployent is missing or inconsistent
      description: >
        The number of scraper deployments should be the same for every machine.
        When this alert fires, some machine has more or less deployments than
        expected. Check whether there have been recent changes to the machine
        or to scraper.

# SwitchSLO
#
# A switch at a site has been down for too long and we need to contact the site
# host or transit provider to investigate. If SNMP scraping *and* pings are both
# failing for a certain period, then this is probaby a reasonable stand-in as an
# "up"/"aliveness" check.
  - alert: SwitchDownAtSite
    expr: |
      up{job="snmp-targets",site!~".*t$"} == 0
        and on(site) probe_success{instance=~"s1.*",module="icmp"} == 0
          unless on(site) gmx_site_maintenance == 1
    for: 1d
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: The switch at {{ $labels.site }} has been unreachable for too long.
      description: >
        The SNMP exporter cannot scrape new metrics from the switch. The issue
        could be with the switch itself, or with the transit provider.
      dashboard: 'https://grafana.mlab-oti.measurementlab.net/d/SuqnZ6Hiz/?orgId=1&var-site_name={{$labels.site}}'

##
## Inventory.
##
  - alert: InventoryConfigurationIsMissing
    expr: absent(up{service="ssh806"}) or absent(up{service="rsyncd"})
    for: 30m
    labels:
      repo: dev-tracker
      severity: ticket
    annotations:
      summary: Inventory configuration is missing.
      description: Machine or rsyncd service configuration has been missing for
        too long. Check the behavior of the m-lab/operator/.travis.yml
        deployment, the GCS buckets, and the gcp-service-discovery component of
        prometheus-support.

  - alert: InventoryMachinesWithoutRsyncd
    expr: up{service="ssh806"} unless on(machine) up{service="rsyncd"}
    for: 30m
    labels:
      repo: dev-tracker
      severity: ticket
    annotations:
      summary: Rsyncd configuration is missing from some machines.
      description: ""

  - alert: InventoryRsyncdWithoutMachines
    expr: up{service="rsyncd"} unless on(machine) up{service="ssh806"}
    for: 30m
    labels:
      repo: dev-tracker
      severity: ticket
    annotations:
      summary: Machine configuration is missing for some rsyncd services.
      description: ""

##
## Services.
##
  - alert: SidestreamServicesAreMissing
    expr: absent(up{service="sidestream"})
    for: 30m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: "All sidestream targets are missing."
      description: ""

  - alert: SidestreamRunningWithoutMachine
    expr: up{service="sidestream"} unless on(machine) up{service="ssh806"}
    for: 30m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: "Sidestream is monitored on an unknown machine."
      description: ""

  - alert: MachineWithoutSidestreamRunning
    expr: up{service="ssh806"} unless on(machine) up{service="sidestream"}
    for: 30m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: "Machines are missing sidestream monitoring."
      description: ""

# Scrapers are configured on machine "c", but machine "c" is not in the rsyncd inventory.
  - alert: ScraperRunningWithoutRsyncd
    expr: up{container="scraper"} unless on(machine, experiment) up{service="rsyncd"}
    for: 30m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: "Scraper deployments are running without rsyncd monitoring"
      description: ""

# Rsync inventory includes machine "b" and rsyncd is actually running, but
# machine "b" does not have a configured scraper.
  - alert: RsyncRunningWithoutScraper
    expr: |
      up{service="rsyncd"} and on(machine, experiment) probe_success{service="rsyncd"} == 1
        unless on(machine, experiment) up{container="scraper"}
    for: 30m
    labels:
      repo: dev-tracker
      severity: ticket
    annotations:
      summary: "Rsync is monitored without a corresponding scraper deployment"
      description: ""

# DownloaderIsFailingToUpdate: The downloader hasn't successfully retrieved the files in
# at least 21 hours, meaning that at least the last two download attempts have failed.
  - alert: DownloaderIsFailingToUpdate
    expr: time() - downloader_last_success_time_seconds > (21 * 60 * 60)
    for: 5m
    labels:
      repo: dev-tracker
      severity: page
    annotations:
      summary: Neither of the last two attempts to download the maxmind or
        routeviews feeds were successful.
      description: Check for errors with the downloader service on grafana with
        the downloader_error_count metric, or check the stackdriver logs for
        the downloader cluster.
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/ZGuYht1mk/

# DownloaderNotRunning: The downloader cluster crashed and not running at all.
  - alert: DownloaderDownOrMissing
    expr: up{container="downloader"} == 0 or absent(up{container="downloader"})
    for: 10m
    labels:
      repo: dev-tracker
      severity: ticket
    annotations:
      summary: The downloader for maxmind/routeviews feeds is down or missing.
      description: Check the status of Kubernetes clusters on each M-Lab GCP
        project. Look at the travis deployment history for m-lab/downloader.

# Prometheus is unable to get data from the snmp_exporter service.
  - alert: SnmpExporterDownOrMissing
    expr: up{container="snmp-exporter"} == 0 or absent(up{container="snmp-exporter"})
    for: 10m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: The snmp_exporter service is down or missing.
      description: Check the status of Kubernetes clusters on each M-Lab GCP
        project. Look at the travis deployment history for
        m-lab/prometheus-support.

# Some SNMP metrics are missing from Prometheus. These should always be present.
# The wait period shouuld be longer than that for the SnmpExporterDownOrMissing
# alert.
  - alert: SnmpExporterMissingMetrics
    expr: absent(ifHCOutOctets)
    for: 30m
    labels:
      repo: ops-tracker
      severity: page
    annotations:
      summary: Expected SNMP metrics are missing from Prometheus!
      description: >
        If the snmp_exporter service is running, then there may be a
        target configuration error. Check the target definitions in GCS[1] and
        the target status in Prometheus[2].

        [1]: https://console.cloud.google.com/storage/browser/operator-mlab-oti/prometheus/snmp-targets
        [2]: https://prometheus.mlab-oti.measurementlab.net/targets
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/SuqnZ6Hiz/

# Scraping SNMP metrics from a switch is failing.
  - alert: SnmpScrapingDownAtSite
    expr: |
      up{job="snmp-targets",site!~".*t$"} == 0
        and on(site) probe_success{instance=~"s1.*",module="icmp"} == 1
        unless on(site) gmx_site_maintenance == 1
    for: 2h
    labels:
      repo: ops-tracker
      severity: page
    annotations:
      summary: Prometheus is unable to scrape SNMP metrics from a switch.
      description: >
        Maybe the switch is down? Is the snmp_exporter using the right community
        string? Look in switch-details.json in the m-lab/switch-config repo. Is
        the IP of the snmp_exporter VM in GCE whitelisted on the switch?
      dashboard: 'https://grafana.mlab-oti.measurementlab.net/d/SuqnZ6Hiz/?orgId=1&var-site_name={{$labels.site}}'

# Prometheus is unable to get data from the script_exporter service.
  - alert: ScriptExporterDownOrMissing
    expr: up{job="script-exporter"} == 0 or absent(up{job="script-exporter"})
    for: 10m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: The script_exporter service is down on missing.
      description: >
        The script_exporter service runs in a Docker container on a GCE VM
        named 'script-exporter' in each M-Lab GCP project. For deployment
        details and troubleshooting, you can usually figure out the issue by
        looking through the Travis-CI build logs[1]. You can also look for
        hints in the GCP console for the GCE instance, or by SSHing to the
        instance itself.
        [1]: https://travis-ci.org/m-lab/script-exporter-support

# Some script_exporter metrics are missing from Prometheus. These should always
# be present. The wait period should be longer than that for the
# ScriptExporterDownOrMissing alert.
  - alert: ScriptExporterMissingMetrics
    expr: |
      absent(script_success{service="ndt_e2e"})
        or absent(script_success{service="ndt_queue"})
    for: 30m
    labels:
      repo: ops-tracker
      severity: page
    annotations:
      summary: Expected script_exporter metrics are missing from Prometheus!
      description: >
        If the script_exporter service is running, then there may be a target
        configuration error. Check the target definitions in GCS[1] and the target
        status in Prometheus[2].

        [1]: https://console.cloud.google.com/storage/browser/operator-mlab-oti/prometheus/script-targets
        [2]: http://prometheus.mlab-oti.measurementlab.net:9090/targets
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/SuqnZ6Hiz/

# Prometheus is unable to get data from the blackbox_exporter service for IPv4
# probes. The service is down, or the metric is missing.
  - alert: BlackboxExporterIpv4DownOrMissing
    expr: |
      up{job="blackbox-exporter-ipv4"} == 0
        or absent(up{job="blackbox-exporter-ipv4"})
    for: 10m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: The blackbox_exporter service is down for IPv4 probes.
      description: Check the status of the blackbox-server pod in the
        prometheus-federation cluster on each M-Lab GCP project.

# Prometheus is unable to get data from the blackbox_exporter service for IPv6
# probes. The service is down, or the metric is missing.
  - alert: BlackboxExporterIpv6DownOrMissing
    expr: up{job="blackbox-exporter-ipv6"} == 0 or absent(up{job="blackbox-exporter-ipv6"})
    for: 10m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: The blackbox_exporter service is down or missing for IPv6 probes.
      description: The blackbox_exporter for IPv6 checks runs in a Linode VM.
        Make sure the VM is up and running. If it is, check the status of the
        BBE container running in the VM. Domains for VMs are like
        blackbox-exporter-ipv6.<project>.measurementlab.net.

# More than a certain percentage of NDT servers meet the criteria for being
# down.
  - alert: TooManyNdtServersDown
    expr: |
      scalar(
        count(
          probe_success{service="ndt_raw"} and on(machine)
            up{service="nodeexporter"} == 1
              unless on(machine) (lame_duck_node == 1 or gmx_machine_maintenance == 1)
          unless on(machine) (
            probe_success{service="ndt_raw"} == 1 and on(machine)
            probe_success{service="ndt_ssl"} == 1 and on(machine)
            script_success{service="ndt_e2e"} == 1 and on(machine)
            vdlimit_used{experiment="ndt.iupui"} /
              vdlimit_total{experiment="ndt.iupui"} < 0.95
          )
        )
      )
      /
      count(
        probe_success{service="ndt_raw"} and on(machine)
        up{service="nodeexporter"} == 1
          unless on(machine) (lame_duck_node == 1 or gmx_machine_maintenance == 1)
      ) > 0.25
    for: 30m
    labels:
      repo: ops-tracker
      severity: page
    annotations:
      summary: Too large a percentage of NDT servers are down.
      description: Make sure that the blackbox_exporter, script_exporter and
        node_exporters are all working as expected. Was any update to the
        platform just released?
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/JAq7W6Nmk/

# One or more NDT-specific metrics is missing. These are the NDT metrics that
# mlab-ns relies on to determine whether NDT is up and running, so we need to
# make sure that the metrics are always present. NOTE: mlab-ns additionally
# relies on the script_exporter metric 'script_success{service="ndt_e2e"}', but
# alerting for that metric is already handled by the
# ScriptExporterMissingMetrics alert.
  - alert: NdtMetricsMissing
    expr: |
      absent(probe_success{service="ndt_raw"})
        or absent(probe_success{service="ndt_raw_ipv6"})
        or absent(probe_success{service="ndt_ssl"})
        or absent(probe_success{service="ndt_ssl_ipv6"})
        or absent(vdlimit_used{experiment="ndt.iupui"})
        or absent(vdlimit_total{experiment="ndt.iupui"})
    for: 30m
    labels:
      repo: ops-tracker
      severity: page
    annotations:
      summary: A metric for an NDT service is missing.
      description: >
        If the blackbox_exporter service is running, then there may be a target
        configuration error. Check the target definitions in GCS and the target
        status in Prometheus[1]. vdlimit_* metrics are provided by node_exporter
        on each node.
        [1]: https://prometheus.mlab-oti.measurementlab.net/targets#job-blackbox-targets

# One or more Neubot-specific metrics is missing. These are the Neubot metrics that
# mlab-ns relies on to determine whether Neubot is up and running, so we need to
# make sure that the metrics are always present.
  - alert: NeubotMetricsMissing
    expr: |
      absent(probe_success{service="neubot"})
        or absent(probe_success{service="neubot_ipv6"})
    for: 30m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: A metric for a Neubot service is missing.
      description: >
        If the blackbox_exporter service is running, then there may be a target
        configuration error. Check the target definitions in GCS and the target
        status in Prometheus[1].
        [1]: https://prometheus.mlab-oti.measurementlab.net/targets#job-blackbox-targets

# One or more Mobiperf-specific metrics is missing. These are the Mobiperf
# metrics that mlab-ns relies on to determine whether Mobiperf is up and
# running, so we need to make sure that the metrics are always present.
  - alert: MobiperfMetricsMissing
    expr: |
      absent(probe_success{service="mobiperf"})
        or absent(probe_success{service="mobiperf_ipv6"})
    for: 30m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: A metric for a Mobiperf service is missing.
      description: >
        If the blackbox_exporter service is running, then there may be a target
        configuration error. Check the target definitions in GCS and the target
        status in Prometheus[1].
        [1]: https://prometheus.mlab-oti.measurementlab.net/targets#job-blackbox-targets

# Some number of nodes don't have a lame-duck status.
  - alert: LameDuckMetricMissingForNode
    expr: |
      up{service="nodeexporter"} == 1
        unless on(machine) lame_duck_node
        unless on(machine) gmx_machine_maintenance == 1
    for: 30m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: Some number of nodes are missing lame-duck status metrics.
      description: Check /var/spool/node_exporter/ on the node to see if the file
        lame_duck.prom is missing. If it is, use the mlabops Ansible
        lame-duck.yaml playbook to restore it.

# vdlimit_used and/or vdlimit_free metrics are completely missing for a node.
# There are other vdlimit_* metrics, but we care especially about these because
# mlab-ns uses them to query Prometheus for node status.
  - alert: VdlimitMetricsMissingForNode
    expr: |
      up{service="nodeexporter"} == 1
        unless on(machine) (vdlimit_used and vdlimit_total)
        unless on(machine) gmx_machine_maintenance == 1
    for: 30m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: Some vdlimit_* metrics are missing.
      description: Check /var/spool/node_exporter/ on the node to see if the file
        vdlimit.prom is missing. The file is created by
        /etc/cron.d/prom_vdlimit_metrics.cron.
      dashboard: https://grafana.mlab-sandbox.measurementlab.net/d/JAq7W6Nmk/

# NPAD_VdlimitTooMuchUsedDisk checks whether the disk usage for the NPAD vserver
# is higher than 9GB (90%, as NPAD is set up with a 10GB disk).
  - alert: NPAD_VdlimitTooMuchUsedDisk
    expr: vdlimit_used{experiment="npad.iupui"} > 9*1024*1024
    for: 30m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: NPAD disk usage is much higher than normal.
      description: Check /var/spool/iupui_npad/ on the node to see if data is
        being collected. If not, then check the health of scraper for this node
        and slice. If so, then check or other sources of disk usage, like
        /var/logs and /home/iupui_npad/VAR/logs/paris-traceroute.log.

# A collectd-mlab service has a problem and is down.
  - alert: CoreServices_CollectdMlabDown
    expr: |
      collectd_mlab_success == 0
        unless on(machine) gmx_machine_maintenance == 1
    for: 10m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: A collectd-mlab service is down.
      description: The collectd-mlab service runs in the mlab_utility slice.
        Try running the ansible/disco/update-mlab-utility.yaml Ansible playbook
        in the mlabops repository to configure collectd-mlab. Login to the node
        and run the check script manually to see what the specific error is
        (/usr/lib/nagios/plugins/check_collectd_mlab.py).

# A collectd-mlab service metric is missing on some node.
  - alert: CoreServices_CollectdMlabMissing
    expr: |
      up{service="nodeexporter"} == 1
        unless on(machine) collectd_mlab_success
        unless on(machine) gmx_machine_maintenance == 1
    for: 10m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: A collectd-mlab service metric is missing.
      description: The collectd-mlab service runs in the mlab_utility slice.
        Try running the ansible/disco/update-mlab-utility.yaml Ansible playbook
        in the mlabops repository to configure collectd-mlab. Login to the node
        and run the check script manually to see what the specific error is
        (/usr/lib/nagios/plugins/check_collectd_mlab.py).

# One or more of the backend services handled by the nginx proxy is down.
  - alert: Prometheus_NginxProxiedServiceDown
    expr: probe_success{job="nginx-proxied-services"} == 0
    for: 30m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: Backend services handled by the nginx proxy are down.
      description: Did the nginx k8s deployment or nginx-lb k8s service fail?
        Did the backend service or deployment fail in some way?

# TODO:
#   Replace this with two other alerts:
#    1.  Alert if hourly test volume on servers drops relative to same hour on recent days.
#    2.  E2E alert that compares rows in tables to test volume on servers.
#
# ParserDailyVolumeTooLow: 24 hour test volume has dropped over 30% compared to
# the average of the 2 smallest test volumes of 4 days out of the last week.  Two vectors
# of conservative constant value avoid false alarms when there is little valid history.
# On occasion, processing may fall behind a bit.  The "FOR 2h" waits 2h before triggering
# an actual alert, so the pipeline may fall behind for up to 2 hours without alerting.
# However, if the pipeline falls several hours behind, and stays behind for more than
# 2 hours, the alert will fire.
#
# In normal operation, we expect the 50th quantile to split mid-way between the two smallest
# volume days of the 4 sample days.  The 4 sample always include one weekend day, so one of
# the two smallest days will generally be a weekend day.
# For example, for a Tuesday, the prior data might be ordered (decreasing):
# M, F, W, Sa, C1, C2,
# and the 50th quantile will be midway between previous Sat and previous Wed.
#
# The alert condition ignores batch processing.
#
# Implementation notes:
# This alert uses label_replace to merge multiple vectors.  We tried simpler queries using
# AND, OR or +, but these do not do what we need.  We use label_replace to add a new "delay"
# label, which then allows us to compute quantile across multiple vectors.  (Or we could compute
# sums, averages, topk, min, etc.)
# The constant vectors require adding two labels, "service" which associates with a pipeline, and
# "delay" which differentiates from the actual delayed metrics for 1d, 3d, etc.
# For each pipeline service, the quantile computation then aggregates across the 6 vectors in
# the delay dimension.
  - alert: ParserDailyVolumeTooLow
    expr: |
      candidate_service:etl_test_count:increase24h
        < (0.7 * quantile by(service)(0.5,
          label_replace(candidate_service:etl_test_count:increase24h offset 1d, "delay", "1d", "", ".*") or
          label_replace(candidate_service:etl_test_count:increase24h offset 3d, "delay", "3d", "", ".*") or
          label_replace(candidate_service:etl_test_count:increase24h offset 5d, "delay", "5d", "", ".*") or
            label_replace(candidate_service:etl_test_count:increase24h offset 1w, "delay", "7d", "", ".*") or
            label_replace(label_replace(vector(0), "delay", "c1", "", ".*"), "service", "etl-disco-parser", "", ".*") or
            label_replace(label_replace(vector(0), "delay", "c2", "", ".*"), "service", "etl-disco-parser", "", ".*") or
            label_replace(label_replace(vector(0), "delay", "c1", "", ".*"), "service", "etl-ndt-parser", "", ".*") or
            label_replace(label_replace(vector(0), "delay", "c2", "", ".*"), "service", "etl-ndt-parser", "", ".*") or
            label_replace(label_replace(vector(0), "delay", "c1", "", ".*"), "service", "etl-sidestream-parser", "", ".*") or
            label_replace(label_replace(vector(0), "delay", "c2", "", ".*"), "service", "etl-sidestream-parser", "", ".*") or
            label_replace(label_replace(vector(0), "delay", "c1", "", ".*"), "service", "etl-traceroute-parser", "", ".*") or
            label_replace(label_replace(vector(0), "delay", "c2", "", ".*"), "service", "etl-traceroute-parser", "", ".*")
            ))
    for: 2h
    labels:
      repo: dev-tracker
      severity: page
    annotations:
      summary: Today's test volume is less than 70% of nominal daily volume.
      description: Are machines online? Is data being collected? Is the parser working?
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/PKqnWeNmz/

# The node_exporter running on eb.measurementlab.net is down.
  - alert: NodeExporterOnEbDownOrMissing
    expr: |
      up{job="eb-node-exporter"} == 0
        or absent(up{job="eb-node-exporter"})
    for: 10m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: The node_exporter running on eb.measurementlab.net is down.
      description: Login to EB to see if it is in fact crashed. If so, look
        through the logs.

# The node_exporter running on mirror.measurementlab.net is down.
  - alert: NodeExporterOnMirrorDownOrMissing
    expr: up{job="mirror-node-exporter"} == 0 or absent(up{job="mirror-node-exporter"})
    for: 10m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: The node_exporter running on mirror.measurementlab.net is down.
      description: Login to to see if it is in fact crashed. If so, look
        through the logs.

# The node_exporter running on dns.measurementlab.net is down.
  - alert: NodeExporterOnDnsDownOrMissing
    expr: up{job="dns-node-exporter"} == 0 or absent(up{job="dns-node-exporter"})
    for: 10m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: The node_exporter running on dns.measurementlab.net is down.
      description: Login to to see if it is in fact crashed. If so, look
        through the logs.

# GardenerDownOrMissing fires when the etl-gardener is down or absent.
# TODO: enable annotations to ignore some container ports, and simplify this query.
# https://github.com/m-lab/prometheus-support/issues/48
  - alert: GardenerDownOrMissing
    expr: |
      up{container="etl-gardener",instance=~".*:9090"} == 0
        or absent(up{container="etl-gardener",instance=~".*:9090"})
    for: 10m
    labels:
      repo: dev-tracker
      severity: ticket
    annotations:
      summary: The ETL Gardener instance is down or missing.
      description: Gardener runs in the data-processing-cluster.
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/eBbUW6oik

# ETL_ParserPanicNonZero fires when an ETL parser panics. The number of panics
# may increase due to a transient issue or a short-lived trigger for a parser
# bug. The alert will fire as long as the rate is above zero for more than 5
# minutes. So, it's possible for false-negatives in response to isolated events,
# but it also allows the alert to stop firing without a redeploy when the event
# is short-lived.
  - alert: ETL_ParserPanicNonZero
    expr: irate(etl_panic_count[4m]) > 0
    for: 5m
    labels:
      repo: dev-tracker
      severity: ticket
    annotations:
      summary: An ETL parser panicked {{ $labels.instance }}
      description: Bugs cause panics. This bug should be fixed. Parsers run in
        AppEngine. Check logs to see the panic stack trace. Identify the archive
        that led to the panic (logs or TaskQueue tasks with many retries). Fix
        the bug or create a new issue describing the failure and linking to the
        triggering archive.

# ETL_AnnotationDownOrMissing fires when the annotator AppEngine service is down
# (prometheus scrape attempts fail) or prometheus does not know about the
# annotator service at all.
  - alert: ETL_AnnotationDownOrMissing
    expr: up{service="annotator"} == 0 or absent(up{service="annotator"})
    for: 30m
    labels:
      repo: dev-tracker
      severity: ticket
    annotations:
      summary: An ETL Annotation Server is offline or missing!
      description: The annotator runs in AppEngine. Check logs and recent
        deployments. The daily and batch parsers may also be affected.

# NDT_AnnotationRatioTooLow fires when the client annotations on NDT
# tests appears to have too many failures or the bq_ndt_annotation_* metrics
# disappear.
  - alert: NDT_AnnotationRatioTooLow
    expr: |
      bq_ndt_annotation_success / bq_ndt_annotation_total < 0.98
        or absent(bq_ndt_annotation_success / bq_ndt_annotation_total)
    for: 30m
    labels:
      repo: dev-tracker
      severity: ticket
    annotations:
      summary: Too many NDT tests are missing annotations!
      description: The annotator runs in AppEngine. Check logs and recent
        deployments. The daily and batch parsers may also be affected.

# Gardener_ParseTimeDifferenceTooOld fires when the maximum and minimum
# parse_time values for each data set are greater than 80 days.
  - alert: Gardener_ParseTimeDifferenceTooOld
    expr: bq_gardener_parse_time_difference_days > 80
    for: 10m
    labels:
      repo: dev-tracker
      severity: ticket
    annotations:
      summary: 'Gardener progress is too slow for dataset: {{ $labels.dataset }}'
      description: Gardener throughput is dependent on the etl-batch-parser and
        associated queues.yaml.
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/eBbUW6oik/

# Rebot_MissingMetrics fires when no metrics can be collected for rebot for
# the past 10 minutes.
  - alert: Rebot_MissingMetrics
    expr: absent(up{job='rebot'})
    for: 10m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: The rebot instance running on eb.measurementlab.net is down.
      description: Metrics for rebot cannot be collected. The instance is down
        or not reachable. Check that the rebot daemon on eb.measurementlab.net
        is running and port 9999 is reachable.

# Cloud_TooManyAppEngineVersions fires when there are too many AE versions to deploy
# full ETL deployment successfully.
  - alert: Cloud_TooManyAppEngineVersions
    expr: sum(gcp_aeflex_versions{container="service-discovery"}) / 210 > 0.95
    for: 10m
    labels:
      repo: dev-tracker
      severity: ticket
    annotations:
      summary: Too many AppEngine versions for the project.
      description: AppEngine has a project maximum of 210 versions across all
        services. Until old versions are deleted, future deployments will begin
        to fail. Please delete them or create a tool that does it automatically.
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/Yor77urmk/cloud-appengine

# Cloud_TooManyInactiveInstances checks that all running instances are active
# rather than wasting resources.
  - alert: Cloud_TooManyInactiveInstances
    expr: |
      sum (gcp_aeflex_instances{container="service-discovery", active="false"})
        / sum (gcp_aeflex_instances{container="service-discovery"}) > 0.25
    for: 30m
    labels:
      repo: dev-tracker
      severity: ticket
    annotations:
      summary: Too many AppEngine instances are running but not serving.
      description: Failed or interrupted deployments do not shutdown instances
        completely. Over time, a project accumulates inactive instances that
        waste resources. Please delete them or create a tool that does it
        automatically.
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/Yor77urmk/cloud-appengine

# Boot_MachineFailedToBoot a machine has failed to boot for more than a day.
  - alert: Boot_MachineFailedToBoot
    expr: epoxy_last_success < epoxy_last_boot
      unless on(machine) (lame_duck_node == 1 or gmx_machine_maintenance == 1)
    for: 24h
    labels:
      repo: dev-tracker
      severity: ticket
    annotations:
      summary: A machine booted recently, but has not reported success yet.
      description: Failing to report success may be due to configuration or
        runtime failures. Check the machine console messages via the DRAC.
        Check the k8s setup logs on the machine (/tmp/setup_k8s.log). Check the
        epoxy-boot-api VM logs in Stackdriver.
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/JaSyFC9mk/boot-nodes

# Boot_EpoxyServerOfflineOrMissing fires when the epoxy boot api server is
# offline or misconfigured.
  - alert: Boot_EpoxyServerOfflineOrMissing
    expr: up{job="epoxy-boot-api"} == 0 or absent(up{job="epoxy-boot-api"})
    for: 30m
    labels:
      repo: dev-tracker
      severity: ticket
    annotations:
      summary: The ePoxy boot api server is down or missing. Nodes cannot boot!
      description: Has the configuration been removed from prometheus? Is the
        service running? Check for the epoxy-boot-api-* VM. Check for recent
        failed Cloud Builder builds caused by recent commits; check those logs.
        Check the VM service logs from Stackdriver.
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/t_juk39ik/boot-epoxy-server

# PlatformCluster_DownOrMissing extends the ClusterDown alert to apply only to
# the platform cluster instance of Prometheus.
# TODO: retire this alert in favor of ClusterDown when possible.
  - alert: PlatformCluster_DownOrMissing
    expr: up{job="platform-cluster"} == 0 or absent(up{job="platform-cluster"})
    for: 1h
    labels:
      repo: ops-tracker
      severity: page
    annotations:
      summary: Prometheus is down on the k8s platform cluster.
      description: The Prometheus instance on the platform cluster appears to
        be down. This makes investigation using Prometheus impossible. Verify
        ithat the node where Prometheus should be running is healthy (`kubectl
        get nodes --selector run=prometheus-server`). Verify that the node is
        connected to the cluster. Verify the Prometheus deployment is running
        using - `kubectl get pods`. Check pod logs.
      dashboard: https://grafana.mlab-staging.measurementlab.net/d/K8-zAIuik/k8s-master-cluster

  - alert: PlatformCluster_NodeExporterMissing
    expr: absent(up{deployment="node-exporter", cluster="platform-cluster"})
    for: 5m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: The node_exporter DaemonSet is missing or has no metrics.
      description: The node_exporter DaemonSet is missing or has no metrics.
        Verify that the DaemonSet is healthy (`kubectl describe ds
        node-exporter`).
      dashboard: https://grafana.mlab-staging.measurementlab.net/d/rJ7z2Suik/k8s-site-overview

  # If a node_exporter pod is down or otherwise broken, fire an alert, unless
  # the node is in lame-duck mode, GMX maintenance mode, or the scrape job for
  # the entire node is down.
  - alert: PlatformCluster_NodeExporterDown
    expr: |
      up{deployment="node-exporter", cluster="platform-cluster"} == 0 unless on(machine) (
          lame_duck_node == 1 or
          gmx_machine_maintenance == 1 or
          up{job="kubernetes-nodes", cluster="platform-cluster"} == 0
        )
    for: 1h
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: At least one node has a broken or no node_exporter pod.
      description: At least one node has a broken or no node_exporter pod.
        Verify that the DaemonSet is healthy. Check the status of the node that
        the pod is supposed to be running on. Check the status of the pod
        itself, if it exists.
      dashboard: https://grafana.mlab-staging.measurementlab.net/d/rJ7z2Suik/k8s-site-overview

  - alert: PlatformCluster_CadvisorMissing
    expr: absent(up{deployment="cadvisor", cluster="platform-cluster"})
    for: 5m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: The CAdvisor DaemonSet is missing or has no metrics.
      description: The CAdvisor DaemonSet is missing or has no metrics. Verify that
        the DaemonSet is healthy (`kubectl describe ds cadvisor`).
      dashboard: https://grafana.mlab-staging.measurementlab.net/d/rJ7z2Suik/k8s-site-overview

  # If a CAdvisor pod is down or otherwise broken, fire an alert, unless the
  # node is in lame-duck mode, GMX maintenance mode, or the scrape job for the
  # entire node is down.
  - alert: PlatformCluster_CadvisorDown
    expr: |
      up{deployment="cadvisor", cluster="platform-cluster"} == 0 unless on(machine) (
          lame_duck_node == 1 or
          gmx_machine_maintenance == 1 or
          up{job="kubernetes-nodes", cluster="platform-cluster"} == 0
        )
    for: 1h
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: At least one node has a broken or no CAdvisor pod.
      description: At least one node has a broken or no CAdvisor pod. Verify that
        the DaemonSet is healthy. Check the status of the node that the pod is
        supposed to be running on. Check the status of the pod itself, if it
        exists.
      dashboard: https://grafana.mlab-staging.measurementlab.net/d/rJ7z2Suik/k8s-site-overview

  - alert: PlatformCluster_NdtMissing
    expr: absent(up{deployment="ndt", cluster="platform-cluster"})
    for: 5m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: The NDT DaemonSet is missing or has no metrics.
      description: The NDT DaemonSet is missing or has no metrics. Verify that
        the DaemonSet is healthy (`kubectl describe ds ndt`).
      dashboard: https://grafana.mlab-staging.measurementlab.net/d/rJ7z2Suik/k8s-site-overview

  # If a NDT pod is down or otherwise broken, fire an alert, unless the node is
  # in lame-duck mode, GMX maintenance mode, or the scrape job for the entire
  # node is down.
  - alert: PlatformCluster_NdtDown
    expr: |
      up{deployment="ndt", cluster="platform-cluster"} == 0 unless on(machine) (
          lame_duck_node == 1 or
          gmx_machine_maintenance == 1 or
          up{job="kubernetes-nodes", cluster="platform-cluster"} == 0
        )
    for: 1h
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: At least one node has a broken or no NDT pod.
      description: At least one node has a broken or no NDT pod. Verify that
        the DaemonSet is healthy. Check the status of the node that the pod
        is supposed to be running on. Check the status of the pod itself, if it
        exists.
      dashboard: https://grafana.mlab-staging.measurementlab.net/d/rJ7z2Suik/k8s-site-overview

  # The /cache/data mount point on a node has exceeded 95% of its capacity.
  # This is where all pods write all experiment and core service data (shared
  # pool of space). If this mount point fill up, all experiments and core
  # services will fail in some way.
  - alert: PlatformCluster_DataPartitionTooFull
    expr: |
      ((node_filesystem_size_bytes{cluster="platform-cluster", mountpoint="/cache/data"} -
          node_filesystem_free_bytes{cluster="platform-cluster", mountpoint="/cache/data"})
          / node_filesystem_size_bytes{cluster="platform-cluster", mountpoint="/cache/data"})
        > 0.95
    for: 10m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: The /cache/data mount point on a node is more than 95% full.
      description: All experiment and core service data is written to a shared
        pool of disk space on a partition mounted at /cache/data. The mount
        point has exceed 95% usage. Check that the pusher sidecar container in
        all pods is working. See which pod is using all the space with `df -sh
        /cache/data/*`.
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/rJ7z2Suik/k8s-site-overview

# Etcd alerts mostly gleaned from:
# https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/etcd3_alert.rules.yml

  - alert: PlatformCluster_EtcdHasNoLeader
    expr: etcd_server_has_leader == 0
    for: 1m
    labels:
      repo: ops-tracker
      severity: page
    annotations:
      summary: At least one etcd cluster member has no leader.
      description: An etcd cluster member is reporting that it has no leader.
        This should never happen. Find out which master server is hosting the
        ectd instance(s) and make sure that node is healthy and has network
        connectivity to the other masters.
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/milv1PgZz/k8s-etcd-overview

  - alert: PlatformCluster_EtcdTooManyLeaderChanges
    expr: rate(etcd_server_leader_changes_seen_total{job=~".*etcd.*"}[15m]) > 3
    for: 15m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: Too many leader changes in the etcd cluster.
      description: etcd cluster members are changing leaders too frequently.
        Leader changes are normal (e.g., a master node is rebooted), but they
        should not happen too often. Look into networking, resource or other
        issues on the master nodes.
      dashboard: https://grafana.mlab-staging.measurementlab.net/d/K8-zAIuik/k8s-master-cluster

  - alert: PlatformCluster_EtcdTooManyProposalFailures
    expr: rate(etcd_server_proposals_failed_total[15m]) > 5
    for: 15m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: Too many raft protocol proposal failures in the etcd cluster.
      description: There are too many raft protocol proposal failures happening
        in the etcd cluster. These type of errors are normally related to two
        issues - temporary failures related to a leader election or longer
        downtime caused by a loss of quorum in the cluster.
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/milv1PgZz/k8s-etcd-overview

  - alert: PlatformCluster_EtcdMemberCommunicationTooSlow
    expr: |
      histogram_quantile(0.99,
        rate(etcd_network_peer_round_trip_time_seconds_bucket{job=~".*etcd.*"}[5m])
      ) > 0.15
    for: 10m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: Network communication between etcd cluster members is too slow.
      description: The members of the etcd cluster are on different master
        nodes, and each node is in a different power zone. Communication
        between the members is taking too long. Make sure that the VPC network
        is working as intended, and that it isn't overloaded.
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/milv1PgZz/k8s-etcd-overview

  - alert: PlatformCluster_EtcdWalFsyncsTooSlow
    expr: histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket[5m])) > 0.5
    for: 10m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: etcd write-ahead-log fsync operations are taking too long.
      description: etcd uses a WAL (Write Ahead Log), and fsync operations to
        it are taking too long. Check that there are no problems with the disk
        on the master node, and that disk I/O throughput is not becoming a
        bottleneck.
      dashboard: https://grafana.mlab-staging.measurementlab.net/d/K8-zAIuik/k8s-master-cluster

  - alert: PlatformCluster_EtcdBackedCommitsTooSlow
    expr: |
      histogram_quantile(0.99,
        rate(etcd_disk_backend_commit_duration_seconds_bucket{job=~".*etcd.*"}[5m])
      ) > 0.25
    for: 10m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      summary: etcd backend commits are taking too long to complete.
      description: etcd writes incremental snapshots to disk. These writes are
        taking too long to complete.  Check that there are no problems with the
        disk on the master node, and that disk I/O throughput is not becoming a
        bottleneck.
      dashboard: https://grafana.mlab-staging.measurementlab.net/d/K8-zAIuik/k8s-master-cluster

