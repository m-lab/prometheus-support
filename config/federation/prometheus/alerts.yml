# M-Lab alert configuration.
#
# ALERT <alert name>
#   IF <expression>
#   [ FOR <duration> ]
#   [ LABELS <label set> ]
#   [ ANNOTATIONS <label set> ]
#
## Notes about delay.
#
# There are inherent delays in the monitoring and alert pipeline. When
# designing alerts, remember that each step includes some delay:
#
#  * the time between reality changing and monitoring observing the change.
#  * the time between the observed change and the delay defined by the alert.
#  * the time between a firing alert and the "group_wait" time in alertmanager.
#  * the time between sending a notification and the notification arriving.
#
# For example, for a 60s collection period, a 2m alert delay, a 30s group_wait,
# and a 15s-3m SMS delivery delay, the time between reality changing and a
# human knowing could be over six minutes.
#
# Your job is to balance false-positives with responsiveness when necessary.

# ClusterDown: when any of the federated prometheus k8s clusters is down for
# three sample periods, then raise an alert.
#
# All scraper metrics come from federated targets, so this is critical.
ALERT ClusterDown
  IF up{job="federation-targets"} == 0
  FOR 3m
  LABELS {
    severity = "page"
  }
  ANNOTATIONS {
    summary = "Instance {{ $labels.instance }} down",
    description = "{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 2 minutes.",
  }

##
## SLOs
##

# MachineSLO
#
# SidestreamIsNotRunning: an M-Lab server is online, but the sidestream exporter
# is not. Since sidestream is a core service, this must be fixed.
ALERT SidestreamIsNotRunning
  IF sum_over_time(up{service="sidestream"}[10m]) == 0
        AND ON(machine)
     sum_over_time(probe_success{service="ssh806"}[20m]) / 20 >= 0.90
        UNLESS ON(machine)
     lame_duck_node == 1
  FOR 10m
  LABELS {
    severity = "page"
  }
  ANNOTATIONS {
    summary = "",
    description = "",
  }


# ScraperSLO
#
# ScraperMostRecentArchivedFileTimeIsTooOld: scraper uploads archives for a
# machine once a day. If the machine is online (for at least 30 hours), but
# scraper has not uploaded an archive for that machine for more than two days
# plus 8 hours, there is a problem.
#
# Note: we can wait two days because we expect that either a) few machines are
# affected by this at once, or b) many machines are affected and the
# ParserDailyVolumeTooLow will trigger first.
#
# Note: the delay threshold is set to 2h to prevent false positives. For
# example, if a machine remains running while it is not network accessible,
# then the machine will need time for scraper to catch up once it is network
# accessible again.
#
# TODO(soltesz): remove the != 0 check when legacy records are removed.
ALERT ScraperMostRecentArchivedFileTimeIsTooOld
  IF (time() - (scraper_maxrawfiletimearchived{container="scraper-sync"} != 0)) > (56 * 60 * 60)
        AND ON(machine)
     (time() - process_start_time_seconds{service="sidestream"}) > (30 * 60 * 60)
        UNLESS ON(machine)
     lame_duck_node == 1
  FOR 2h
  LABELS {
    severity = "page"
  }
  ANNOTATIONS {
    summary = "Scraper max file mtime is too old {{ $labels.rsync_url }}",
    description = "Max file mtime for {{ $labels.rsync_url }} is older than 56 hours.",
  }

# Scraper internal consistency.
#
# Verify that for every running scraper there is a corresponding metric from
# scraper-sync indicating that a collection was attempted. These should always
# be in sync with one another.
#
# We use scraper_lastcollectionattempt because scraper_maxrawfiletimearchived
# is not updated until the first successful upload. This is not possible before
# a machine comes online.
ALERT ScraperSyncPresentWithoutScraperCollector
  IF (scraper_lastcollectionattempt{container="scraper-sync"}
        UNLESS ON(machine, experiment, rsync_module)
           up{container="scraper"})
  FOR 3h
  LABELS {
    severity = "page"
  }
  ANNOTATIONS {
    summary = "",
    description = "",
  }

ALERT ScraperCollectorMissingFromScraperSync
  IF (up{container="scraper"}
        UNLESS ON(machine, experiment, rsync_module)
           scraper_lastcollectionattempt{container="scraper-sync"})
  FOR 3h
  LABELS {
    severity = "page"
  }
  ANNOTATIONS {
    summary = "",
    description = "",
  }

##
## Inventory.
##

ALERT InventoryConfigurationIsMissing
  IF absent(up{service="ssh806"}) OR absent(up{service="rsyncd"})
  FOR 30m
  LABELS {
    severity = "ticket"
  }
  ANNOTATIONS {
    summary = "Inventory configuration {{ $labels.service }} is missing.",
    description = "Machine or rsyncd service configuration has been missing for too long.",
    hints = "Check the behavior of the m-lab/operator/.travis.yml deployment, the GCS buckets, and the gcp-service-discovery component of prometheus-support.",
  }

ALERT InventoryMachinesWithoutRsyncd
  IF up{service="ssh806"} UNLESS ON(machine) up{service="rsyncd"}
  FOR 30m
  LABELS {
    severity = "ticket"
  }
  ANNOTATIONS {
    summary = "Rsyncd configuration is missing from some machines.",
    hints = "",
  }

ALERT InventoryRsyncdWithoutMachines
  IF up{service="rsyncd"} UNLESS ON(machine) up{service="ssh806"}
  FOR 30m
  LABELS {
    severity = "ticket"
  }
  ANNOTATIONS {
    summary = "Machine configuration is missing for some rsyncd services.",
    hints = "",
  }


##
## Services.
##

ALERT SidestreamServicesAreMissing
  IF absent(up{service="sidestream"})
  FOR 30m
  LABELS {
    severity = "ticket"
  }
  ANNOTATIONS {
    summary = "",
    hints = "",
  }

ALERT SidestreamRunningWithoutMachine
  IF up{service="sidestream"} UNLESS ON(machine) up{service="ssh806"}
  FOR 30m
  LABELS {
    severity = "ticket"
  }
  ANNOTATIONS {
    summary = "",
    hints = "",
  }

ALERT MachineWithoutSidestreamRunning
  IF up{service="ssh806"} UNLESS ON(machine) up{service="sidestream"}
  FOR 30m
  LABELS {
    severity = "ticket"
  }
  ANNOTATIONS {
    summary = "",
    hints = "",
  }

# Scrapers are configured on machine "c", but machine "c" is not in the rsyncd inventory.
ALERT ScraperRunningWithoutRsyncd
  IF up{container="scraper"} UNLESS ON(machine, experiment) up{service="rsyncd"}
  FOR 30m
  LABELS {
    severity = "ticket"
  }
  ANNOTATIONS {
    summary = "",
    hints = "",
  }

# Rsync inventory includes machine "b", but machine "b" does not have a configured scraper.
ALERT RsyncRunningWithoutScraper
  IF up{service="rsyncd"} UNLESS ON(machine, experiment) up{container="scraper"}
  FOR 30m
  LABELS {
    severity = "ticket"
  }
  ANNOTATIONS {
    summary = "",
    hints = "",
  }


# DownloaderIsFailingToUpdate: The downloader hasn't successfully retrieved the files in at least 21 hours, meaning that at least the last two download attempts have failed.
ALERT DownloaderIsFailingToUpdate
  IF time()-downloader_last_success_time_seconds > (21 * 60 * 60)
  FOR 5m
  LABELS {
    severity = "page"
  }
  ANNOTATIONS {
    summary = "Neither of the last two attempts to download the maxmind/routeviews feeds were successful.",
    hints = "Check for errors with the downloader service on grafana with the downloader_Error_Count metric, or check the stackdriver logs for the downloader cluster."
  }

# DownloaderNotRunning: The downloader cluster crashed and not running at all.
ALERT DownloaderDownOrMissing
  IF up{container="downloader"} == 0 OR absent(up{container="downloader"})
  FOR 10m
  LABELS {
    severity = "ticket"
  }
  ANNOTATIONS {
    summary = "The downloader for maxmind/routeviews feeds is down on {{ $labels.instance }}.",
    hints = "Check the status of Kubernetes clusters on each M-Lab GCP project. Look at the travis deployment history for m-lab/downloader."
  }

# Prometheus is unable to get data from the snmp_exporter service.
ALERT SnmpExporterDownOrMissing
  IF up{job="snmp-exporter"} == 0 OR absent(up{job="snmp-exporter"})
  FOR 10m
  LABELS {
    severity = "ticket"
  }
  ANNOTATIONS {
    summary = "The snmp_exporter service is down on {{ $labels.instance }}.",
    hints = "The snmp_exporter service runs in a Docker container on a GCE VM named 'snmp-exporter' in each M-Lab GCP project. Look at the Travis-CI builds/deploys for m-lab/prometheus-snmp-exporter, or SSH to the VM and poke around."
  }

# Some SNMP metrics are missing from Prometheus. These should always be present.
# The wait period shouuld be longer than that for the SnmpExporterDownOrMissing
# alert.
ALERT SnmpExporterMissingMetrics
  IF absent(ifHCOutOctets)
  FOR 30m
  LABELS {
    severity = "page"
  }
  ANNOTATIONS {
    summary = "Expected SNMP metrics are missing from Prometheus!",
    hints = "If the snmp_exporter service is running, then there may be a target configuration error. Check the target definitions in GCS and and the target status in Prometheus.",
    prometheus_targets = "http://status.mlab-oti.measurementlab.net:9090/targets",
    gcsbucket = "https://console.cloud.google.com/storage/browser/operator-mlab-oti/prometheus/snmp-targets",
    dashboard = "http://status.mlab-oti.measurementlab.net:3000/dashboard/db/switch-metrics"
  }

# TODO:
#   Replace this with two other alerts:
#    1.  Alert if hourly test volume on servers drops relative to same hour on recent days.
#    2.  E2E alert that compares rows in tables to test volume on servers.
#
# ParserDailyVolumeTooLow: 24 hour test volume has dropped over 30% compared to
# the average of the 2 smallest test volumes of 4 days out of the last week.  Two vectors
# of conservative constant value avoid false alarms when there is little valid history.
# On occasion, processing may fall behind a bit.  The "FOR 2h" waits 2h before triggering
# an actual alert, so the pipeline may fall behind for up to 2 hours without alerting.
# However, if the pipeline falls several hours behind, and stays behind for more than 
# 2 hours, the alert will fire.
#
# In normal operation, we expect the 50th quantile to split mid-way between the two smallest
# volume days of the 4 sample days.  The 4 sample always include one weekend day, so one of
# the two smallest days will generally be a weekend day.
# For example, for a Tuesday, the prior data might be ordered (decreasing):
# M, F, W, Sa, C1, C2,
# and the 50th quantile will be midway between previous Sat and previous Wed.
#
# The alert condition ignores batch processing.
#
# Implementation notes:
# This alert uses label_replace to merge multiple vectors.  We tried simpler queries using
# AND, OR or +, but these do not do what we need.  We use label_replace to add a new "delay"
# label, which then allows us to compute quantile across multiple vectors.  (Or we could compute
# sums, averages, topk, min, etc.)
# The constant vectors require adding two labels, "service" which associates with a pipeline, and
# "delay" which differentiates from the actual delayed metrics for 1d, 3d, etc.
# For each pipeline service, the quantile computation then aggregates across the 6 vectors in
# the delay dimension.
#ALERT ParserDailyVolumeTooLow
#  IF sum by(service) (increase(etl_test_count{service!~".*batch.*"}[24h]))
#      < (0.7 * quantile by(service)(0.50,
#         label_replace(sum by(service) (increase(etl_test_count{service!~".*batch.*"}[24h] offset 1d)),"delay","1d","",".*" ) OR
#         label_replace(sum by(service) (increase(etl_test_count{service!~".*batch.*"}[24h] offset 3d)),"delay","3d","",".*" ) OR
#         label_replace(sum by(service) (increase(etl_test_count{service!~".*batch.*"}[24h] offset 5d)),"delay","5d","",".*" ) OR
#         label_replace(sum by(service) (increase(etl_test_count{service!~".*batch.*"}[24h] offset 7d)),"delay","7d","",".*" ) OR
#         label_replace(label_replace(vector(0),"delay","c1","",".*" ), "service", "etl-ndt-parser",        "", ".*") OR
#         label_replace(label_replace(vector(0),"delay","c2","",".*" ), "service", "etl-ndt-parser",        "", ".*") OR
#         label_replace(label_replace(vector(0),"delay","c1","",".*" ), "service", "etl-sidestream-parser", "", ".*") OR
#         label_replace(label_replace(vector(0),"delay","c2","",".*" ), "service", "etl-sidestream-parser", "", ".*") OR
#         label_replace(label_replace(vector(0),"delay","c1","",".*" ), "service", "etl-traceroute-parser", "", ".*") OR
#         label_replace(label_replace(vector(0),"delay","c2","",".*" ), "service", "etl-traceroute-parser", "", ".*")
#         ))
#  FOR 2h
#  LABELS {
#    severity = "page"
#  }
#  ANNOTATIONS {
#    summary = "Today's test volume is less than 70% of nominal daily volume.",
#    hints = "Are machines online? Is data being collected? Is the parser working?",
#    url = "http://status.mlab-oti.measurementlab.net:3000/dashboard/db/parserdailyvolumetoolow"
#  }

# Nagios is not running (or otherwise broken) on nagios-oam.measurementlab.net,
# or the Prometheus job for scraping the nagios_exporter on this instance is
# missing.
ALERT NagiosOamDownOrMissing
  IF nagios_livestatus_available{job="nagios-oam-exporter"} == 0 OR absent(up{job="nagios-oam-exporter"})
  FOR 10m
  LABELS {
    severity = "ticket"
  }
  ANNOTATIONS {
    summary = "The Nagios OA&M instance is down on {{ $labels.instance }}.",
    hints = "The Nagios OA&M instance runs in a Linode VM."
  }
